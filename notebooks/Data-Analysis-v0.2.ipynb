{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Python-Framework\" data-toc-modified-id=\"Python-Framework-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Python Framework</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Libraries-and-dependencies\" data-toc-modified-id=\"Libraries-and-dependencies-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Libraries and dependencies</a></span></li><li><span><a href=\"#Import-packages-and-create-modules\" data-toc-modified-id=\"Import-packages-and-create-modules-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Import packages and create modules</a></span></li></ul></li><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Import</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Calibration-Data\" data-toc-modified-id=\"Load-Calibration-Data-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Load Calibration Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#AlphaSense-sensors\" data-toc-modified-id=\"AlphaSense-sensors-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>AlphaSense sensors</a></span></li></ul></li><li><span><a href=\"#Import-Local-CSV\" data-toc-modified-id=\"Import-Local-CSV-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Import Local CSV</a></span></li><li><span><a href=\"#Import-Test\" data-toc-modified-id=\"Import-Test-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Import Test</a></span></li></ul></li><li><span><a href=\"#Formulas\" data-toc-modified-id=\"Formulas-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Formulas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-formulas\" data-toc-modified-id=\"Useful-formulas-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Useful formulas</a></span></li><li><span><a href=\"#Calculator\" data-toc-modified-id=\"Calculator-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Calculator</a></span></li></ul></li><li><span><a href=\"#AlphaSense-Baseline-Calibration\" data-toc-modified-id=\"AlphaSense-Baseline-Calibration-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>AlphaSense Baseline Calibration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Functions-and-Model-Parameters\" data-toc-modified-id=\"Functions-and-Model-Parameters-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Functions and Model Parameters</a></span></li><li><span><a href=\"#Test-Correction\" data-toc-modified-id=\"Test-Correction-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Test Correction</a></span></li><li><span><a href=\"#TODO:-Correction-Checks\" data-toc-modified-id=\"TODO:-Correction-Checks-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>TODO: Correction Checks</a></span></li></ul></li><li><span><a href=\"#Data-Export\" data-toc-modified-id=\"Data-Export-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Data Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Local-Data-Export\" data-toc-modified-id=\"Local-Data-Export-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Local Data Export</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-Plots\" data-toc-modified-id=\"Time-Series-Plots-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Time Series Plots</a></span></li><li><span><a href=\"#TODO:-Basic-Sensor-Correlations\" data-toc-modified-id=\"TODO:-Basic-Sensor-Correlations-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>TODO: Basic Sensor Correlations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Full-Seaborn-Correlogram\" data-toc-modified-id=\"Full-Seaborn-Correlogram-1.5.2.1\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>Full Seaborn Correlogram</a></span></li><li><span><a href=\"#Basic-Seaborn-XYPlot\" data-toc-modified-id=\"Basic-Seaborn-XYPlot-1.5.2.2\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>Basic Seaborn XYPlot</a></span></li></ul></li><li><span><a href=\"#TODO:-Anomaly-Detection\" data-toc-modified-id=\"TODO:-Anomaly-Detection-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>TODO: Anomaly Detection</a></span></li></ul></li><li><span><a href=\"#Data-Model\" data-toc-modified-id=\"Data-Model-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Data Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Functionality\" data-toc-modified-id=\"Basic-Functionality-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Basic Functionality</a></span></li><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Feature selection and data training split</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO-Select-variables-here\" data-toc-modified-id=\"TODO-Select-variables-here-1.6.2.1\"><span class=\"toc-item-num\">1.6.2.1&nbsp;&nbsp;</span>TODO Select variables here</a></span></li><li><span><a href=\"#TODO-Preliminary-Checks\" data-toc-modified-id=\"TODO-Preliminary-Checks-1.6.2.2\"><span class=\"toc-item-num\">1.6.2.2&nbsp;&nbsp;</span>TODO Preliminary Checks</a></span></li></ul></li><li><span><a href=\"#TODO-Naive-Linear-Regression\" data-toc-modified-id=\"TODO-Naive-Linear-Regression-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>TODO Naive Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ordinary-Linear-Regression\" data-toc-modified-id=\"Ordinary-Linear-Regression-1.6.3.1\"><span class=\"toc-item-num\">1.6.3.1&nbsp;&nbsp;</span>Ordinary Linear Regression</a></span></li><li><span><a href=\"#Ordinary-Linear-Regression-with-differentiation\" data-toc-modified-id=\"Ordinary-Linear-Regression-with-differentiation-1.6.3.2\"><span class=\"toc-item-num\">1.6.3.2&nbsp;&nbsp;</span>Ordinary Linear Regression with differentiation</a></span></li></ul></li><li><span><a href=\"#TODO-ARIMA(X)-model\" data-toc-modified-id=\"TODO-ARIMA(X)-model-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>TODO ARIMA(X) model</a></span></li></ul></li></ul></li><li><span><a href=\"#R-Framework\" data-toc-modified-id=\"R-Framework-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>R Framework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-environment\" data-toc-modified-id=\"Initialise-environment-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initialise environment</a></span></li><li><span><a href=\"#Install-dependencies\" data-toc-modified-id=\"Install-dependencies-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Install dependencies</a></span></li><li><span><a href=\"#Load-in-R-libraries\" data-toc-modified-id=\"Load-in-R-libraries-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Load in R libraries</a></span></li><li><span><a href=\"#Export-Data-to-R-Dataframe\" data-toc-modified-id=\"Export-Data-to-R-Dataframe-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Export Data to R Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-and-timestamp-reading\" data-toc-modified-id=\"Renaming-and-timestamp-reading-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Renaming and timestamp reading</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pairs-Plot\" data-toc-modified-id=\"Pairs-Plot-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Pairs Plot</a></span></li><li><span><a href=\"#Coplot\" data-toc-modified-id=\"Coplot-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Coplot</a></span></li></ul></li><li><span><a href=\"#Model-Iterations\" data-toc-modified-id=\"Model-Iterations-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Model Iterations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Diagnostics\" data-toc-modified-id=\"Model-Diagnostics-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Model Diagnostics</a></span></li><li><span><a href=\"#Model-Fit-Plot\" data-toc-modified-id=\"Model-Fit-Plot-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Model Fit Plot</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Traditional-R\" data-toc-modified-id=\"Using-Traditional-R-2.6.2.1\"><span class=\"toc-item-num\">2.6.2.1&nbsp;&nbsp;</span>Using Traditional R</a></span></li><li><span><a href=\"#Using-interactive-Plot\" data-toc-modified-id=\"Using-interactive-Plot-2.6.2.2\"><span class=\"toc-item-num\">2.6.2.2&nbsp;&nbsp;</span>Using interactive Plot</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '''\n",
    "    <script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "        if (code_show){\n",
    "            $('div.input').hide();\n",
    "        } else {\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    \n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Libraries and dependencies\n",
    "Run this cell to install all necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install pytz==2017.2 fileupload==0.1.2 ipywidgets==6.0.0 pandas==0.20.1 numpy==1.12.1 matplotlib==2.0.2 seaborn==0.8.0\n",
    "! jupyter nbextension install --py fileupload \n",
    "! jupyter nbextension enable --py fileupload\n",
    "! jupyter nbextension install --py widgetsnbextension \n",
    "! jupyter nbextension enable --py widgetsnbextension\n",
    "! jupyter nbextension install https://rawgit.com/jfbercher/small_nbextensions/master/toc2.zip  --user\n",
    "#! jupyter nbextension install --py toc2/main \n",
    "! jupyter nbextension enable toc2/main\n",
    "! pip install cufflinks --upgrade\n",
    "! pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import packages and create modules\n",
    "Run this cell to load in all necessary Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter stuff\n",
    "from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# System\n",
    "import io, pytz, os, time, datetime, fileupload\n",
    "from shutil import copyfile\n",
    "from os.path import dirname, join\n",
    "import yaml\n",
    "import markdown\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plots\n",
    "import cufflinks as cf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.figure_factory import create_2d_density\n",
    "import plotly.tools as tls\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "# init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Calibration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlphaSense sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "alpha_calData = pd.read_json('https://raw.githubusercontent.com/fablabbcn/smartcitizen-iscape-data/internal_dev/calData/AlphaSense.json', orient='columns', lines = True)\n",
    "alpha_calData.index = alpha_calData['Serial No']\n",
    "\n",
    "print display(alpha_calData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Local CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = 0\n",
    "end_date = 0\n",
    "openData = list()\n",
    "\n",
    "def _upload():\n",
    "    \n",
    "    _upload_widget = fileupload.FileUploadWidget()\n",
    "    _tz_widget = widgets.Dropdown(options=pytz.common_timezones, value='UTC', description='Timezone: ')\n",
    "    \n",
    "    def _cb(change):\n",
    "        # get file\n",
    "        decoded = io.StringIO(change['owner'].data.decode('utf-8'))\n",
    "        filename = change['owner'].filename \n",
    "        fileData = io.StringIO(change['new'].decode('utf-8'))\n",
    "        df = pd.read_csv(fileData, verbose=True, skiprows=[1]).set_index('Time')\n",
    "          \n",
    "        # prepare dataframe\n",
    "        print df.index\n",
    "        df.index = pd.to_datetime(df.index).tz_localize('UTC').tz_convert(_tz_widget.value)\n",
    "        df.sort_index(inplace=True)\n",
    "        df = df.groupby(pd.TimeGrouper(freq='10Min')).aggregate(np.mean)\n",
    "        df.drop([i for i in df.columns if 'Unnamed' in i], axis=1, inplace=True)\n",
    "        \n",
    "        readings[filename] = df[df.index > '2001-01-01T00:00:01Z']\n",
    "        if start_date > 0: readings[filename] = df[df.index > start_date]\n",
    "        if end_date > 0: readings[filename] = df[df.index < end_date]\n",
    "        listFiles(filename)\n",
    "    \n",
    "    # widgets\n",
    "    _upload_widget.observe(_cb, names='data')\n",
    "    _hb = widgets.HBox([_upload_widget, _tz_widget, widgets.HTML(' ')])\n",
    "    display(_hb)\n",
    "\n",
    "def delFile(b):\n",
    "    clear_output()\n",
    "    for d in list(b.hbl.children): d.close()\n",
    "    readings.pop(b.f)\n",
    "\n",
    "def describeFile(b):\n",
    "    clear_output()\n",
    "    display(readings[b.f].describe())\n",
    "    \n",
    "def exportFile(b):\n",
    "    export_dir = 'exports'\n",
    "    if not os.path.exists(export_dir): os.mkdir(export_dir)\n",
    "    savePath = os.path.join(export_dir, b.f+'_clean_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%dT%H%M%S')\n",
    "+'.csv')\n",
    "    if not os.path.exists(savePath):\n",
    "        readings[b.f].to_csv(savePath, sep=\",\")\n",
    "        display(FileLink(savePath))\n",
    "    else:\n",
    "        display(widgets.HTML(' File Already exists!'))\n",
    "    \n",
    "def listFiles(filename):\n",
    "#     clear_output()\n",
    "    temp = list(fileList.children)\n",
    "    cb = widgets.Button(icon='close',layout=widgets.Layout(width='30px'))\n",
    "    cb.on_click(delFile)\n",
    "    cb.f = filename\n",
    "    eb = widgets.Button(description='Export processed CSV', layout=widgets.Layout(width='180px'))\n",
    "    eb.on_click(exportFile)\n",
    "    eb.f = filename\n",
    "    sb = widgets.Button(description='describe', layout=widgets.Layout(width='80px'))\n",
    "    sb.on_click(describeFile)\n",
    "    sb.f = filename  \n",
    "    hbl = widgets.HBox([cb, widgets.HTML(' <b>'+filename+'</b> \\t'), sb, eb])\n",
    "    cb.hbl = hbl\n",
    "    temp.append(hbl)\n",
    "    fileList.children = temp\n",
    "\n",
    "readings = {}\n",
    "display(widgets.HTML('<hr><h3>Select CSV files (remember to change the timezone!)</h3>'))\n",
    "_upload()\n",
    "fileList = widgets.VBox([widgets.HTML('<hr>')])\n",
    "display(fileList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Test\n",
    "\n",
    "Import test from local test database. \n",
    "\n",
    "**Requirement**:\n",
    "\n",
    "- Include where the directory of your tests is (GIT LFS directory)\n",
    "- Make sure that the desired test is available and has been created with the yaml tool\n",
    "\n",
    "**The cell below will**:\n",
    "\n",
    "- Load all the kits within the test\n",
    "- Check if there were alphasense sensors and retrieve their calibration data and order\n",
    "- Check if there was a reference and convert it units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "\n",
    "readings = {}\n",
    "#testsDir = os.getcwd()\n",
    "testsDir = '/Users/macoscar/Documents/04_Projects/02_FabLab/02_SmartCitizen/04_iScape/03_Development/03_TestResults/TestStructure'\n",
    "                \n",
    "pollutantLUT = (['CO', 28, 'ppm'],\n",
    "                ['NO', 30, 'ppb'],\n",
    "                ['NO2', 46, 'ppb'],\n",
    "                ['O3', 48, 'ppb'])\n",
    "\n",
    "def getSensorNames(_sensorsh):\n",
    "    # read only 20 000 chars\n",
    "    data = urllib2.urlopen(_sensorsh).read(20000)\n",
    "    # split it into lines\n",
    "    data = data.split(\"\\n\") \n",
    "    sensorNames = dict()\n",
    "    lineSensors = len(data)\n",
    "    for line in data:\n",
    "        \n",
    "        if 'class AllSensors' in line:\n",
    "            lineSensors = data.index(line)\n",
    "            \n",
    "        if data.index(line) > lineSensors:\n",
    "                \n",
    "                if 'OneSensor' in line:\n",
    "                    try:\n",
    "                        # Split commas\n",
    "                        lineTokenized =  line.strip('').split(',')\n",
    "                        \n",
    "                        # Elimminate unnecessary elements\n",
    "                        lineTokenizedSub = list()\n",
    "                        for item in lineTokenized:\n",
    "                            item = re.sub('\\t', '', item)\n",
    "                            item = re.sub('OneSensor', '', item)\n",
    "                            item = re.sub('{', '', item)\n",
    "                            item = re.sub('}', '', item)\n",
    "                            #item = re.sub(' ', '', item)\n",
    "                            item = re.sub('\"', '', item)\n",
    "                            lineTokenizedSub.append(item)\n",
    "                        \n",
    "                            # Add them to the dict\n",
    "                            if len(lineTokenizedSub) >2:\n",
    "                                sensorID = re.sub(' ','', lineTokenizedSub[1])\n",
    "                                sensorNames[sensorID] = dict()\n",
    "                                sensorNames[sensorID]['SensorLocation'] = re.sub(' ', '', lineTokenizedSub[0])\n",
    "                                if lineTokenizedSub[2][0] == ' ':\n",
    "                                    sensorNames[sensorID]['shortTitle'] = lineTokenizedSub[2][1:len(lineTokenizedSub[2])]\n",
    "                                else:\n",
    "                                    sensorNames[sensorID]['shortTitle'] = lineTokenizedSub[2]\n",
    "                                sensorNames[sensorID]['title'] = lineTokenizedSub[3]\n",
    "                                sensorNames[sensorID]['id'] = lineTokenizedSub[4]\n",
    "                                if len(lineTokenizedSub) > 5:\n",
    "                                    sensorNames[lineTokenizedSub[1]]['unit'] = lineTokenizedSub[7]\n",
    "                    except:\n",
    "                        pass\n",
    "    return sensorNames\n",
    "\n",
    "## Alphasense Name\n",
    "def CHANNEL_NAME(_sensorNames, _measurement, _slot, electrode, _SensorLocation):\n",
    "    sensor_name = ''\n",
    "    electrode_eq = electrode[0]\n",
    "    \n",
    "    for name in _sensorNames:\n",
    "\n",
    "        if _sensorNames[name]['SensorLocation'] == _SensorLocation and '{}{}'.format(_slot, electrode_eq) in name and '{}'.format(_measurement) in name:\n",
    "            \n",
    "            sensor_name = str(_sensorNames[name]['shortTitle'])\n",
    "            return sensor_name\n",
    "        elif _sensorNames[name]['SensorLocation'] == _SensorLocation and not '{}{}'.format(_slot, electrode_eq) in name and '{}'.format(_measurement) in name:\n",
    "            sensor_name = str(_sensorNames[name]['shortTitle'])\n",
    "\n",
    "\n",
    "    return sensor_name\n",
    "\n",
    "def getTests(directory):\n",
    "    tests = dict()\n",
    "    mydir = join(directory, 'data')\n",
    "    for root, dirs, files in os.walk(mydir):\n",
    "        for _file in files:\n",
    "            if _file.endswith(\".yaml\"):\n",
    "                filePath = join(root, _file)\n",
    "                stream = file(filePath)\n",
    "                yamlFile = yaml.load(stream)\n",
    "                tests[yamlFile['test']['id']] = root\n",
    "                #print [yamlFile['test']['id'], filePath]\n",
    "    return tests\n",
    "\n",
    "selectedTests = tuple()\n",
    "def selectTests(x):\n",
    "    global selectedTests\n",
    "    selectedTests = list(x)\n",
    "\n",
    "def loadTest(b):\n",
    "    # print selectedTests\n",
    "    clear_output()\n",
    "    for testPath in selectedTests:\n",
    "        # Find Yaml\n",
    "        filePath = join(testPath, 'test_description.yaml')\n",
    "        stream = file(filePath)\n",
    "        test = yaml.load(stream)\n",
    "        test_id = test['test']['id']\n",
    "        \n",
    "        readings[test_id] = dict()\n",
    "        readings[test_id]['commit_hash'] = test['commit_hash']\n",
    "        commitSensorsh = ('https://raw.githubusercontent.com/fablabbcn/smartcitizen-kit-20/' + readings[test_id]['commit_hash'] + '/lib/Sensors/Sensors.h')\n",
    "\n",
    "        commitSensorNames = getSensorNames(commitSensorsh)\n",
    "        \n",
    "        targetSensorNames = list()\n",
    "        for types in ('WORKING','AUXILIARY'):\n",
    "            for slot in (1,2,3):\n",
    "                targetSensorNames.append(CHANNEL_NAME(currentSensorNames, 'GASES', slot, types, 'BOARD_AUX'))\n",
    "        targetSensorNames.append(CHANNEL_NAME(currentSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX'))\n",
    "        targetSensorNames.append(CHANNEL_NAME(currentSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX'))\n",
    "        \n",
    "        testSensorNames = list()\n",
    "        for types in ('WORKING','AUXILIARY'):\n",
    "            for slot in (1,2,3):\n",
    "                testSensorNames.append(CHANNEL_NAME(commitSensorNames, 'ALPHA', slot, types, 'BOARD_AUX'))\n",
    "        testSensorNames.append(CHANNEL_NAME(commitSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX'))\n",
    "        testSensorNames.append(CHANNEL_NAME(commitSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX'))\n",
    "        \n",
    "        # Get test metadata\n",
    "        test_init_date = test['test']['init_date']\n",
    "        test_end_date = test['test']['end_date']\n",
    "        \n",
    "        print '------------------------------------------------------'\n",
    "        print '\\tLoading test {} for performed from {} to {}'.format(test_id, test_init_date, test_end_date)\n",
    "        print '\\tTest performed with commit {}'.format(readings[test_id]['commit_hash'])\n",
    "        print '\\t-----KIT-----'\n",
    "        # Open all kits\n",
    "        for kit in test['test']['devices']['kits']:\n",
    "            # Get fileName\n",
    "            fileNameProc = test['test']['devices']['kits'][kit]['fileNameProc']\n",
    "            fileData = join(testPath, fileNameProc)\n",
    "            \n",
    "            # Create pandas dataframe\n",
    "            df = pd.read_csv(fileData, verbose=False, skiprows=[1]).set_index('Time')\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "            df.sort_index(inplace=True)\n",
    "            df = df.groupby(pd.TimeGrouper(freq='10Min')).aggregate(np.mean)\n",
    "            df.drop([i for i in df.columns if 'Unnamed' in i], axis=1, inplace=True)\n",
    "            # Create dictionary and add it to the readings key\n",
    "            \n",
    "            if len(targetSensorNames) == len(testSensorNames) and len(targetSensorNames) > 0:\n",
    "                for i in range(len(targetSensorNames)):\n",
    "                    df.rename(columns={testSensorNames[i]: targetSensorNames[i]}, inplace=True)\n",
    "                    print '\\t\\tRenaming column {} to {}'.format(testSensorNames[i], targetSensorNames[i])\n",
    "            \n",
    "            kitDict = dict()\n",
    "            kitDict['data'] = df\n",
    "            readings[test_id][kit] = kitDict\n",
    "            \n",
    "            print '\\t\\tKit {} has been loaded'.format(kit)\n",
    "            ## Check if it's a STATION and retrieve alphadelta codes\n",
    "            if test['test']['devices']['kits'][kit]['type'] == 'STATION':\n",
    "                # print 'AlphaSense the sensor is'\n",
    "                alphaDelta = dict()\n",
    "                alphaDelta['CO'] = test['test']['devices']['kits'][kit]['alphasense']['CO']\n",
    "                alphaDelta['NO2'] = test['test']['devices']['kits'][kit]['alphasense']['NO2']\n",
    "                alphaDelta['O3'] = test['test']['devices']['kits'][kit]['alphasense']['O3']\n",
    "                alphaDelta['SLOTS'] = test['test']['devices']['kits'][kit]['alphasense']['slots']\n",
    "                readings[test_id][kit]['alphasense'] = alphaDelta\n",
    "                print '\\t\\t-----ALPHASENSE-----'\n",
    "                print '\\t\\tAlphasense sensors used'\n",
    "                print '\\t\\t' + str(alphaDelta)\n",
    "\n",
    "            \n",
    "        ## Check if there's was a reference equipment during the test\n",
    "        if test['reference']['available']:\n",
    "            print '\\t-----REFERENCE-----'\n",
    "            for reference in test['reference']['files']:\n",
    "                print '\\t' + reference\n",
    "                # print 'Reference during the test was'\n",
    "                referenceDict =  dict()\n",
    "                \n",
    "                # Get the file name\n",
    "                fileNameProc = test['reference']['files'][reference]['fileNameProc']\n",
    "                \n",
    "                # Check the index name\n",
    "                timeIndex = test['reference']['files'][reference]['index']['name']\n",
    "                \n",
    "                # Open it with pandas    \n",
    "                fileData = join(testPath, fileNameProc)\n",
    "                df = pd.read_csv(fileData, verbose=False, skiprows=[1]).set_index(timeIndex)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                df.sort_index(inplace=True)\n",
    "                df = df.groupby(pd.TimeGrouper(freq='10Min')).aggregate(np.mean)\n",
    "                df.drop([i for i in df.columns if 'Unnamed' in i], axis=1, inplace=True)\n",
    "                \n",
    "                ## Convert units\n",
    "                # Get which pollutants are available in the reference\n",
    "                pollutants = test['reference']['files'][reference]['channels']['pollutants']\n",
    "                channels = test['reference']['files'][reference]['channels']['names']\n",
    "                units = test['reference']['files'][reference]['channels']['units']\n",
    "                \n",
    "                for index in range(len(channels)):\n",
    "                    \n",
    "                    pollutant = pollutants[index]\n",
    "                    channel = channels[index]\n",
    "                    unit = units[index]\n",
    "                    \n",
    "                    # Get molecular weight and target units for the pollutant in question\n",
    "                    for pollutantItem in pollutantLUT:\n",
    "                        if pollutantItem[0] == pollutant:\n",
    "                            molecularWeight = pollutantItem[1]\n",
    "                            targetUnit = pollutantItem[2]\n",
    "                            \n",
    "                    convertionLUT = (['ppm', 'ppb', 1000],\n",
    "                         ['mg/m3', 'ug/m3', 1000],\n",
    "                         ['mg/m3', 'ppm', 24.45/molecularWeight],\n",
    "                         ['ug/m3', 'ppb', 24.45/molecularWeight],\n",
    "                         ['mg/m3', 'ppb', 1000*24.45/molecularWeight],\n",
    "                         ['ug/m3', 'ppm', 1./1000*24.45/molecularWeight])\n",
    "                    \n",
    "                    # Get convertion factor\n",
    "                    if unit == targetUnit:\n",
    "                            convertionFactor = 1\n",
    "                            print '\\t\\tNo unit convertion needed for {}'.format(pollutant)\n",
    "                    else:\n",
    "                        for convertionItem in convertionLUT:\n",
    "                            if convertionItem[0] == unit and convertionItem[1] == targetUnit:\n",
    "                                convertionFactor = convertionItem[2]\n",
    "                            elif convertionItem[1] == unit and convertionItem[0] == targetUnit:\n",
    "                                convertionFactor = 1.0/convertionItem[2]\n",
    "                        print '\\t\\tConverting {} from {} to {}'.format(pollutant, unit, targetUnit)\n",
    "                            \n",
    "                    df.loc[:,pollutant] = df.loc[:,channel]*convertionFactor\n",
    "                    \n",
    "                referenceDict['data'] = df\n",
    "                readings[test_id][reference] = referenceDict\n",
    "                readings[test_id][reference]['is_reference'] = True\n",
    "                print '\\t\\t{} reference has been loaded'.format(reference)\n",
    "        print '------------------------------------------------------'\n",
    "\n",
    "currentSensorsh = ('https://raw.githubusercontent.com/fablabbcn/smartcitizen-kit-20/refurbish/lib/Sensors/Sensors.h')\n",
    "currentSensorNames = getSensorNames(currentSensorsh)\n",
    "\n",
    "tests = getTests(testsDir)\n",
    "interact(selectTests,\n",
    "         x = widgets.SelectMultiple(options=tests, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTests, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "loadB = widgets.Button(description='Load Tests')\n",
    "loadB.on_click(loadTest)\n",
    "\n",
    "display(loadB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "for test in readings:\n",
    "    print test\n",
    "    for reading in readings[test]:\n",
    "        print reading\n",
    "        if 'alphasense' in readings[test][reading]:\n",
    "            print 'The Kit with alphasense is {}'.format(reading)\n",
    "            display(readings[test][reading]['alphasense'])\n",
    "        if 'is_reference' in readings[test][reading]:\n",
    "            print 'The reference is {}'.format(reading)\n",
    "            display(readings[test][reading]['data'].head(4))\n",
    "            \n",
    "## Usage Example\n",
    "slot = 2\n",
    "name = CHANNEL_NAME(currentSensorNames, 'GASES', slot, 'WORKING', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "slot = 2\n",
    "name = CHANNEL_NAME(currentSensorNames, 'GASES', slot, 'AUXILIARY', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "slot = 3\n",
    "name = CHANNEL_NAME(commitSensorNames, 'ALPHA', slot, 'WORKING', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "slot = 2\n",
    "name = CHANNEL_NAME(commitSensorNames, 'ALPHA', slot, 'AUXILIARY', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "name = CHANNEL_NAME(currentSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "name = CHANNEL_NAME(currentSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "name = CHANNEL_NAME(commitSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "name = CHANNEL_NAME(commitSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX')\n",
    "print name\n",
    "\n",
    "typeSLOT = ('WORKING','AUXILIARY')\n",
    "\n",
    "namesNEW = list()\n",
    "for types in ('WORKING','AUXILIARY'):\n",
    "    for slot in (1,2,3):\n",
    "        namesNEW.append(CHANNEL_NAME(currentSensorNames, 'GASES', slot, types, 'BOARD_AUX'))\n",
    "namesNEW.append(CHANNEL_NAME(currentSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX'))\n",
    "namesNEW.append(CHANNEL_NAME(currentSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX'))\n",
    "\n",
    "namesOLD = list()\n",
    "for types in ('WORKING','AUXILIARY'):\n",
    "    for slot in (1,2,3):\n",
    "        namesOLD.append(CHANNEL_NAME(commitSensorNames, 'ALPHA', slot, types, 'BOARD_AUX'))\n",
    "namesOLD.append(CHANNEL_NAME(commitSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX'))\n",
    "namesOLD.append(CHANNEL_NAME(commitSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful formulas\n",
    "\n",
    "There are formulas for calculating:\n",
    "- *MICS* = Poly(R, H, T) - **MICS_FORMULA**\n",
    "- *Alphasense* = f(Curr, Sens, Zero) - **AD_FORMULA**\n",
    "- *Smoothing* = f(Signal, Window) - **SMOOTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMOOTH(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def AD_FORMULA(WE, AE, SensorType, I0WE, I0AE, SENSITIVITY1, SENSITIVITY2, AUX, UNIT1, UNIT2, X1W, X2W, X1A, X2A):\n",
    "    \n",
    "    \n",
    "    MW,BW = LINE_COEFF(X1W, X2W, -1400, 1399)\n",
    "    MA,BA = LINE_COEFF(X1A, X2A, -1400, 1399)\n",
    "    \n",
    "    FACTORWE = LINE(WE,MW,BW) / WE\n",
    "    FACTORAE = LINE(AE,MA,BA) / AE\n",
    "    \n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.plot(FACTORWE)\n",
    "    # plt.plot(FACTORAE)\n",
    "\n",
    "    if SensorType == 1 or 2:\n",
    "        # CO OR NO2\n",
    "        # CO: BOARD 5 AD_FORMULA(A, B, 1, -69.4, -18.6, 493.1, 0, C, \"ppm\", \"\", -220.7, 220.42, -220.45, 220.09)\n",
    "        # NO2: BOARD U AD_FORMULA(A, B, 1, 31.5, 17.7, -383.7, 0, C, \"ppb\", \"\", -220.45, 220.19, -219.98, 219.61)\n",
    "        # NO2: BOARD 5 AD_FORMULA(A, B, 1, 24.0, 14.2, -385.9, 0, C, \"ppb\", \"\", -220.69, 220.58, -220.39, 220.17)\n",
    "        result = (FACTORWE*WE - I0WE/I0AE*(FACTORAE*AE))/abs(SENSITIVITY1)\n",
    "        \n",
    "        if UNIT1 == \"ppb\":\n",
    "            result = result*1000\n",
    "    \n",
    "    if SensorType== 3:\n",
    "        # O3\n",
    "        # BOARD U AD_FORMULA(A, B, 3, 1, 23.65, 18.92, -421.58, -471.6497, C, \"ppb\", \"ppb\", -220.69, 220.58, -220.39, 220.17)\n",
    "        # BOARD 5 AD_FORMULA(A, B, 3, 1, 23.33, 19.86, -433.12, -506.96, C, \"ppb\", \"ppb\", -220.69, 220.58, -220.39, 220.17)\n",
    "    \n",
    "        if UNIT2 ==\"ppb\":\n",
    "            result = (FACTORWE*WE - I0WE/I0AE*(FACTORAE*AE) - AUX*SENSITIVITY2/1000)/SENSITIVITY1\n",
    "        if UNIT2 ==\"ppm\":\n",
    "            result = (FACTORWE*WE - I0WE/I0AE*(FACTORAE*AE) - AUX*SENSITIVITY2)/SENSITIVITY1\n",
    "        if UNIT1 == \"ppb\":\n",
    "            result = result*1000\n",
    "\n",
    "    return result\n",
    "\n",
    "def LINE_COEFF(X1,X2,Y1,Y2):\n",
    "    a = float(Y2-Y1)/(X2-X1)\n",
    "    b = Y1-a*X1\n",
    "    return a,b\n",
    "\n",
    "def LINE(x,a,b):\n",
    "    y = [i*a+b for i in x]\n",
    "    return y\n",
    "\n",
    "def MICS_FORMULA(Sensor1Type, Sensor1, Sensor2Type, Sensor2, Sensor3Type, Sensor3, Intercept, B, C, D, E, F, G):\n",
    "    SensorType = [Sensor1Type, Sensor2Type, Sensor3Type]\n",
    "    Sensor = [Sensor1, Sensor2, Sensor3]\n",
    "    Sens = Sensor\n",
    "    for i in range(3):\n",
    "        if SensorType[i] == \"Inverse\":\n",
    "            Sens[i] = 1/Sensor[i]\n",
    "    \n",
    "    result =  Intercept + B*Sens[0] +C*Sens[0]*Sens[0] + D*Sens[1] + E*Sens[1]*Sens[1] + F*Sens[2] + G*Sens[2]*Sens[2]\n",
    "\n",
    "    # MICS_Formula(\"Inverse\", A, \"Direct\", B, \"Direct\", C, -1.615e-01 , 210.08064516, -10236.73257, 0, 0, 0, 0)\n",
    "    return result\n",
    "\n",
    "def greater(y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]>val): result[i]=True\n",
    "        else: result [i]=False        \n",
    "    return result\n",
    "\n",
    "def greaterequal(y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]<val): result[i] = False\n",
    "        else: result [i] = True\n",
    "    return result\n",
    "\n",
    "def lower (y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]<val): result[i]=True\n",
    "        else: result [i]=False        \n",
    "    return result\n",
    "\n",
    "def lowerequal(y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]>val): result[i] = False\n",
    "        else: result [i] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator\n",
    "Input your formulas into this cell for analysis in the plots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = readings[selected[0]].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = readings[selected[0]].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(readings[s].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "        \n",
    "display(widgets.HTML('<hr><h4>Select the Files for your formulas to apply</h4>'))\n",
    "\n",
    "selected=tuple()\n",
    "def selectedFilesChannels(x):\n",
    "    global selected\n",
    "    selected = list(x)\n",
    "    commonChannels(selected)\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    clear_output()\n",
    "    A = _Aterm.value\n",
    "    B = _Bterm.value\n",
    "    C = _Cterm.value\n",
    "    D = _Dterm.value\n",
    "    Name = _formulaName.value\n",
    "    for s in list(selected):\n",
    "        result = functionFormula(s,A,B,C,D)\n",
    "        readings[s][Name] = result\n",
    "    print \"Formula Added!\"\n",
    "\n",
    "def functionFormula(s, Aname, Bname, Cname, Dname): \n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(readings[s].loc[:,(Aname,)],readings[s].loc[:,(Bname,)],left_index=True, right_index=True), readings[s].loc[:,(Cname,)], left_index=True, right_index=True),readings[s].loc[:,(Dname,)],left_index=True, right_index=True)\n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "            \n",
    "# Nice formula to check increasing or decreasing signals: greater(np.gradient(smooth(A,10)),0)\n",
    "\n",
    "layout = widgets.Layout(width='400px')\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "interact(selectedFilesChannels,x = widgets.SelectMultiple(options=readings.keys(), description=' ', selected_labels = selected,layout=widgets.Layout(width='700px')))\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "display(widgets.HTML('<h4>Select the terms you wish to use in your formula </h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaSense Baseline Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaDelta PCB factor\n",
    "factorPCB = 6.36\n",
    "\n",
    "# Background Concentration (model assumption) - (from Masbit et al.)\n",
    "backgroundConc_CO = 0 # NG\n",
    "backgroundConc_NO2 = 5 # NG\n",
    "backgroundConc_OX = 10 # NG\n",
    "\n",
    "# Overlap in hours for each day (index = [day(i)-overlapHours, day(i+1)+overlapHours])\n",
    "overlapHours = 2 \n",
    "\n",
    "# Filter Smoothing \n",
    "filterExpSmoothing = 0.2\n",
    "\n",
    "# Range of deltas\n",
    "deltas = np.arange(1,50,1)\n",
    "\n",
    "# Units Look Up Table - ['Pollutant', unit factor from ppm to target 1, unit factor from ppm to target 2]\n",
    "alphaUnitsFactorsLUT = (['CO', 1, 0],\n",
    "            ['NO2', 1000, 0],\n",
    "            ['O3', 1000, 1000])\n",
    "\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "import plotly as ply                             # even more plots\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from scipy.stats.stats import linregress   \n",
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "from dateutil import relativedelta\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def ExtractBaseline(_data, _delta):\n",
    "    '''\n",
    "        Input:\n",
    "            _data: dataframe containing signal to be baselined and index\n",
    "            _delta : float for delta time (N periods)\n",
    "        Output:\n",
    "            result: vector containing baselined values\n",
    "    ''' \n",
    "    \n",
    "    result = np.zeros(len(_data))\n",
    "    name = []\n",
    "    for n in range(0, len(_data)):\n",
    "        minIndex = max(n-_delta,0)\n",
    "        maxIndex = min(n+_delta, len(_data))\n",
    "        \n",
    "        chunk = _data.values[minIndex:maxIndex]\n",
    "        result[n] = min(chunk)\n",
    "\n",
    "    return result\n",
    "\n",
    "def findMax(_listF):\n",
    "    '''\n",
    "        Input: list to obtain maximum value\n",
    "        Output: value and index of maximum in the list\n",
    "    '''\n",
    "    \n",
    "    valMax=max(_listF)\n",
    "    indexMax = _listF.index(max(_listF))\n",
    "    # print 'Max Value found at index: {}, with value: {}'.format(indexMax, valMax)\n",
    "    \n",
    "    return valMax, indexMax\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def exponential_func(x, a, b, c):\n",
    "     return a * np.exp(b * x) + c\n",
    "\n",
    "def createBaselines(_dataBaseline, _dataCorr, _numberDeltas, _type_regress = 'linear', _plots = False, _verbose = False):\n",
    "    '''\n",
    "        Input:\n",
    "            _dataBaseline: dataframe containing signal and index to be baselined\n",
    "            _dataCorr: baseline data for regression\n",
    "            _type_regress= 'linear', 'exponential', 'best' (based on p_value of both)\n",
    "            _numberDeltas : vector of floats for deltas (N periods)\n",
    "            _plots:  display plots or not\n",
    "            _verbose: print info or not\n",
    "            _type_regress: regression type (linear, exp, ... )\n",
    "        Output:\n",
    "            baseline: pandas dataframe baseline\n",
    "        TODO:\n",
    "            implement other types of regression\n",
    "    '''\n",
    "    \n",
    "    resultData = _dataBaseline.copy()\n",
    "    vectorCorr = _dataCorr.values\n",
    "\n",
    "    name = resultData.name\n",
    "    pearsons =[]\n",
    "    \n",
    "    for delta in _numberDeltas:\n",
    "        resultData[(name +'_' +str(delta))] = ExtractBaseline(_dataBaseline, delta)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(np.transpose(resultData[(name +'_' +str(delta))]), np.transpose(vectorCorr))\n",
    "        pearsons.append(r_value)\n",
    "    \n",
    "    ## Find Max in the pearsons\n",
    "    valMax, indexMax = findMax(pearsons)\n",
    "        \n",
    "    ## Find regression between _dataCorr\n",
    "    baseline = pd.DataFrame(index = _dataBaseline.index)\n",
    "    if _type_regress == 'linear':\n",
    "        ## Fit with y = A + Bx\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(np.transpose(vectorCorr),resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        baseline[(name + '_' + 'baseline_' +  _type_regress)] = intercept + slope*vectorCorr\n",
    "    elif _type_regress == 'exponential':\n",
    "        ## Fit with y = Ae^(Bx) -> logy = logA + Bx\n",
    "        logy = np.log(resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(np.transpose(vectorCorr), logy)\n",
    "        baseline[(name + '_' + 'baseline_' +  _type_regress)] = exponential_func(np.transpose(vectorCorr), np.exp(intercept), slope, 0)\n",
    "    elif _type_regress == 'best':\n",
    "        ## Find linear r_value\n",
    "        slope_lin, intercept_lin, r_value_lin, p_value_lin, std_err_lin = linregress(np.transpose(vectorCorr),resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        \n",
    "        ## Find Exponential r_value\n",
    "        logy = np.log(resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        slope_exp, intercept_exp, r_value_exp, p_value_exp, std_err_exp = linregress(np.transpose(vectorCorr), logy)\n",
    "        \n",
    "        ## Pick which one is best\n",
    "        if r_value_lin > r_value_exp:\n",
    "            if _verbose:\n",
    "                print 'Using linear regression'\n",
    "            baseline[(name + '_' + 'baseline_' +  _type_regress)] = intercept_lin + slope_lin*vectorCorr\n",
    "        else:\n",
    "            if _verbose:\n",
    "                print 'Using exponential regression'\n",
    "            baseline[(name + '_' + 'baseline_' +  _type_regress)] = exponential_func(np.transpose(vectorCorr), np.exp(intercept_exp), slope_exp, 0)\n",
    "            \n",
    "    if _plots == True:\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "            \n",
    "            ax1.plot(_dataCorr.values, resultData[(name + '_'+str(_numberDeltas[indexMax]))], label = 'Baseline', linestyle='-', linewidth=0, marker='o')\n",
    "            ax1.plot(_dataCorr.values, baseline[(name + '_' + 'baseline_' +  _type_regress)] , label = 'Regressed value', linestyle='-', linewidth=1, marker=None)\n",
    "            legend = ax1.legend(loc='best')\n",
    "            ax1.set_xlabel(_dataCorr.name)\n",
    "            ax1.set_ylabel('Regression values')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            ax2.plot(_dataBaseline.index, _dataBaseline.values, label = \"Actual\", linestyle=':', linewidth=1, marker=None)\n",
    "            #[ax2.plot(resultData.index, resultData[(name +'_' +str(delta))].values, label=\"Delta {}\".format(delta), marker=None,  linestyle='-', linewidth=1) for delta in _numberDeltas]\n",
    "            ax2.plot(baseline.index, baseline.values, label='Baseline', marker = None)\n",
    "\n",
    "            ax2.axis('tight')\n",
    "            ax2.legend(loc='best')\n",
    "            ax2.set_title(\"Baseline Extraction\")\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            ax22 = ax2.twinx()\n",
    "            ax22.plot(_dataCorr.index, _dataCorr.values, color = 'red', label = _dataCorr.name, linestyle='-', linewidth=1, marker=None)\n",
    "            ax22.set_ylabel(_dataCorr.name, color = 'red')\n",
    "            ax22.tick_params(axis='y', labelcolor='red')\n",
    "            \n",
    "            fig2, ax3 = plt.subplots(figsize=(20,8)) # two axes on figure\n",
    "            ax3.plot(_numberDeltas, pearsons)\n",
    "            ax3.axis('tight')\n",
    "            ax3.set_title(\"R2 vs. Delta\")\n",
    "            ax3.set_xlabel('Delta')\n",
    "            ax3.set_ylabel('R2')\n",
    "            ax3.grid(True)\n",
    "\n",
    "    return baseline, indexMax\n",
    "\n",
    "def decompose(_data, plots = False):\n",
    "    '''\n",
    "            Function to decompose a signal into it's trend and normal variation\n",
    "            Input:\n",
    "                _data: signal to decompose\n",
    "                plots: print plots or not (default False)\n",
    "            Output:\n",
    "                DataDecomp = _data - slope*_data.index\n",
    "                slope, intercept = linear regression coefficients\n",
    "    '''\n",
    "    indexDecomp = np.arange(len(_data))\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(indexDecomp, np.transpose(_data.values))\n",
    "    dataDecomp=pd.DataFrame(index = _data.index)\n",
    "    name = _data.name\n",
    "    result = []\n",
    "    \n",
    "    for n in range(len(_data)):\n",
    "        result.append(float(_data.values[n]-slope*n))\n",
    "    dataDecomp[(name + '_' + '_flat')] = result\n",
    "    \n",
    "    trend = slope*indexDecomp + intercept\n",
    "    if plots == True:\n",
    "        \n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig, ax = plt.subplots(figsize=(20,10))\n",
    "            ax.plot(_data.index, _data.values, label = \"Actual\", marker = None)\n",
    "            ax.plot(_data.index, dataDecomp[(name + '_' +'_flat')], marker = None, label = 'Flattened')\n",
    "            ax.plot(_data.index, trend, label = 'Trend')\n",
    "            ax.legend(loc=\"best\")\n",
    "            ax.axis('tight')\n",
    "            ax.set_title(\"Signal Decomposition - \"+ name)\n",
    "            ax.set_xlabel('Index')\n",
    "            ax.set_ylabel('Signal')\n",
    "            ax.grid(True)\n",
    "            \n",
    "    return dataDecomp, slope, intercept\n",
    "\n",
    "def calculateBaselineDay(_dataFrame, _listNames, _baselined, _baseliner, _deltas, _type_regress, _trydecomp = False, _plots = False, _verbose = True):\n",
    "    '''\n",
    "        Function to calculate day-based baseline corrections\n",
    "        Input:\n",
    "            _dataFrame: pandas dataframe with datetime index containing 1 day of measurements\n",
    "            _listNames: list containing column names of WE, AE, Temp and Hum\n",
    "            _baselined: channel to calculate the baseline of\n",
    "            _baseliner: channel to use as input for the baselined (baselined = f(baseliner)) \n",
    "            _type_regress: type of regression to perform (linear, exponential, best)\n",
    "            _trydecomp: try trend decomposition (not needed . remove it)\n",
    "            _plot: plot analytics or not\n",
    "            _verbose: print analytics or not\n",
    "        Output:\n",
    "            _data_baseline: dataframe with baseline\n",
    "            _baseline_corr: metadata containing analytics for long term analysis\n",
    "    '''\n",
    "    \n",
    "    ## Un-pack list names\n",
    "    alphaW, alphaA, temp, hum = _listNames\n",
    "    \n",
    "    ## Verify anticorrelation between temperature and humidity\n",
    "    if _plots == True:\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig2, (ax3, ax4) = plt.subplots(nrows = 2, ncols = 1,figsize=(20,10))\n",
    "            ax3.scatter(_dataFrame[hum], _dataFrame[temp], marker = 'o', linewidth = 0)\n",
    "            ax3.set_xlabel(_dataFrame[hum].name)\n",
    "            ax3.set_ylabel(_dataFrame[temp].name)\n",
    "            ax3.grid(True)\n",
    "            \n",
    "            colorH = 'red'\n",
    "            colorT = 'blue'\n",
    "            ax4.plot(_dataFrame.index, _dataFrame[hum], c = colorH, label = _dataFrame[hum].name, marker = None)\n",
    "            ax5 = ax4.twinx()\n",
    "            ax5.plot(_dataFrame.index, _dataFrame[temp], c = colorT, label = _dataFrame[temp].name, marker = None)\n",
    "            ax4.tick_params(axis='y', labelcolor=colorH)\n",
    "            ax5.tick_params(axis='y', labelcolor=colorT)\n",
    "            ax4.set_xlabel('Time')\n",
    "            ax4.set_ylabel(_dataFrame[temp].name, color = colorH)\n",
    "            ax5.set_ylabel(_dataFrame[hum].name, color = colorT)\n",
    "            ax4.grid(True)\n",
    "\n",
    "    ## Correlation between temperature and working electrode raw\n",
    "    slopenDC, interceptnDC, r_valuenDC, p_valuenDC, std_errnDC = linregress(np.transpose(_dataFrame[_baseliner].values), np.transpose(_dataFrame[_baselined].values))\n",
    "   \n",
    "    ## Create Baselines\n",
    "    data_baseline, indexMax = createBaselines(_dataFrame[_baselined], _dataFrame[_baseliner], _deltas, _type_regress, _plots, _verbose)\n",
    "\n",
    "    if _trydecomp == True:\n",
    "        dataDecomp = pd.DataFrame(index = _dataFrame.index)\n",
    "        # Decompose Trend - Check if decomposition helps at all\n",
    "        dataDecomp[alphaW], dataWSlope, dataWIntercept = decompose(_dataFrame[alphaW], _plots)\n",
    "        dataDecomp[alphaA], dataASlope, dataAIntercept = decompose(_dataFrame[alphaA], _plots)\n",
    "        dataDecomp[temp], dataCorrSlope, dataCorrIntercept = decompose(_dataFrame[temp], _plots)\n",
    "\n",
    "        slopeDC, interceptDC, r_valueDC, p_valueDC, std_errDC = linregress(np.transpose(dataDecomp[_baseliner]), np.transpose(dataDecomp[_baselined]))\n",
    "        data_baselineDecomp, indexMaxDecomp = createBaselines(dataDecomp[_baselined], dataDecomp[_baseliner], _deltas, _type_regress, _plots)\n",
    "        slopeBADecomp, interceptBADecomp, r_valueBADecomp, p_valuenBADecomp, std_errnBADecomp = linregress(np.transpose(data_baseline.values), np.transpose(_dataFrame[alphaA].values))\n",
    "\n",
    "    ## Try to find a correlation with the auxiliary electrode\n",
    "    ## Correlation between Baseline and original auxiliary\n",
    "    slopeBA, interceptBA, r_valueBA, p_valueBA, std_errBA = linregress(np.transpose(data_baseline.values), np.transpose(_dataFrame[alphaA].values))\n",
    "    \n",
    "    deltaAuxBas = data_baseline.values-_dataFrame[alphaA].values\n",
    "    ratioAuxBas = data_baseline.values/_dataFrame[alphaA].values\n",
    "    \n",
    "    deltaAuxBas_avg = np.mean(deltaAuxBas)\n",
    "    ratioAuxBas_avg = np.mean(ratioAuxBas)\n",
    "    \n",
    "    # Pre filter the metadata\n",
    "    if slopeBA > 0 and r_valueBA > 0.3:\n",
    "        valid = True\n",
    "    else:\n",
    "        valid = False\n",
    "    baselineCorr = (slopeBA, interceptBA, r_valueBA, p_valueBA, std_errBA, deltaAuxBas_avg, ratioAuxBas_avg, indexMax, valid)\n",
    "    \n",
    "    if _verbose == True:\n",
    "        \n",
    "        print '-------------------'\n",
    "        print 'Correlation coeffs'\n",
    "        print '-------------------'\n",
    "        print 'Correlation coefficient without decomposed trend: {}'.format(r_valuenDC)\n",
    "        if _trydecomp == True:\n",
    "            print 'Correlation coefficient with decomposed trend: {}'.format(r_valueDC)\n",
    "        \n",
    "            print '-------------------'\n",
    "            print 'Slopes'\n",
    "            print '-------------------'\n",
    "            print 'Slope Working Electrode: {} \\n Slope Aux Electrode: {} \\n Slope Temperature : {} \\t Ratio WE/T: {}'.format(dataWSlope, dataASlope, dataCorrSlope, dataWSlope/dataCorrSlope)\n",
    "    \n",
    "        print '-------------------'\n",
    "        print 'Auxiliary Electrode'\n",
    "        print '-------------------'\n",
    "        print 'Correlation coefficient of Baseline and Original auxiliary: {}'.format(r_valueBA)\n",
    "        print 'Baseline Correlation Slope: {} \\t Intercept: {}'.format(slopeBA, interceptBA)\n",
    "        if _trydecomp == True:\n",
    "            print 'Correlation coefficient of Baseline and Original auxiliary with Decomp: {}'.format(r_valueBADecomp)\n",
    "            print 'Baseline Correlation Slope: {} \\t Intercept with Decomp: {}'.format(slopeBADecomp, interceptBADecomp)\n",
    "        \n",
    "        print 'Average Delta: {} \\t Average Ratio: {}'.format(deltaAuxBas_avg, ratioAuxBas_avg)\n",
    "\n",
    "    if _plots == True:\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            \n",
    "            if _trydecomp == False:\n",
    "                fig2, ax3 = plt.subplots(figsize=(20,8))\n",
    "            else: \n",
    "                fig2, (ax3, ax4) = plt.subplots(nrows = 1, ncols = 2, figsize=(20,8))\n",
    "                ax4.plot(data_baselineDecomp.index, data_baselineDecomp.values, label='Baseline', marker = None)\n",
    "                ax4.plot(dataDecomp.index, dataDecomp[alphaW], label = 'Working Decomp', marker = None)\n",
    "                ax4.plot(dataDecomp.index, dataDecomp[alphaA], label = 'Auxiliary Decomp', marker = None)\n",
    "                ax4.legend(loc=\"best\")\n",
    "                ax4.axis('tight')\n",
    "                ax4.set_title(\"Baseline Compensated\")\n",
    "                ax4.set(xlabel='Time', ylabel='Ouput-mV')\n",
    "                ax4.grid(True)\n",
    "                ax4.set_ylim(min(min(dataDecomp[temp]),min(data_baselineDecomp.values)) -5,max(max(dataDecomp[temp]),max(data_baselineDecomp.values))+5)\n",
    "                \n",
    "                ax6 = ax4.twinx()\n",
    "                ax6.plot(dataDecomp.index, dataDecomp[temp], label='Temperature Decomp', c = 'red', marker = None)\n",
    "                ax6.tick_params(axis='y', labelcolor ='red')\n",
    "                ax6.set_ylabel('Temperature (degC)', color = 'red')\n",
    "                ax6.set_ylim(min(min(dataDecomp[temp]),min(data_baselineDecomp.values)) -5,max(max(dataDecomp[temp]),max(data_baselineDecomp.values))+5)\n",
    "            \n",
    "            ax3.plot(data_baseline.index, data_baseline.values, label='Baseline', marker = None)\n",
    "            ax3.plot(_dataFrame.index, _dataFrame[alphaW], label='Original Working', marker = None)\n",
    "            ax3.plot(_dataFrame.index, _dataFrame[alphaA], label='Original Auxiliary', marker = None)\n",
    "\n",
    "            ax3.legend(loc=\"best\")\n",
    "            ax3.axis('tight')\n",
    "            ax3.set_title(\"Baseline Not Compensated\")\n",
    "            ax3.set(xlabel='Time', ylabel='Ouput-mV')\n",
    "            ax3.grid(True)\n",
    "            ax3.set_ylim(min(min(_dataFrame[temp]),min(data_baseline.values)) -5,max(max(_dataFrame[temp]),max(data_baseline.values))+5)\n",
    "            \n",
    "            if _trydecomp == True:\n",
    "                ax5 = ax3.twinx()\n",
    "                ax5.plot(dataDecomp.index, dataDecomp[temp], label='Temperature Decomp', c = 'red', marker = None)\n",
    "                ax5.tick_params(axis='y', labelcolor ='red')\n",
    "                ax5.set_ylabel(dataDecomp[temp].name, color = 'red')\n",
    "                ax5.set_ylim(min(min(dataDecomp[temp]),min(data_baselineDecomp.values)) -5,max(max(dataDecomp[temp]),max(data_baselineDecomp.values))+5)\n",
    "                \n",
    "            fig3, ax7 = plt.subplots(figsize=(20,8))\n",
    "            \n",
    "            ax7.plot(_dataFrame[temp], _dataFrame[alphaW], label='W - Raw', marker='o',  linestyle=None, linewidth = 0)\n",
    "            ax7.plot(_dataFrame[temp], _dataFrame[alphaA], label ='A - Raw', marker='v', linewidth=0)\n",
    "            \n",
    "            if _trydecomp == True:\n",
    "                ax7.plot(dataDecomp[temp], dataDecomp[alphaA], label ='A - Trend Decomposed', marker='v', linewidth=0)\n",
    "                ax7.plot(dataDecomp[temp], dataDecomp[alphaW], label = 'W - Trend Decomposed',marker='o', linestyle=None, linewidth = 0)\n",
    "            \n",
    "            ax7.legend(loc=\"best\")\n",
    "            ax7.axis('tight')\n",
    "            ax7.set_title(\"Output vs. Temperature\")\n",
    "            ax7.set(xlabel='Temperature', ylabel='Ouput-mV')\n",
    "            ax7.grid(True)\n",
    "    \n",
    "    return data_baseline, baselineCorr\n",
    "\n",
    "def findDates(_dataframe):\n",
    "    '''\n",
    "        Find minimum, maximum dates in the dataframe and the amount of days in between\n",
    "        Input: \n",
    "            _dataframe: pandas dataframe with datetime index\n",
    "        Output: \n",
    "            rounded up min day, floor max day and number of days between the min and max dates\n",
    "    '''\n",
    "    range_days = (_dataframe.index.max()-_dataframe.index.min()).days\n",
    "    min_date_df = _dataframe.index.min().ceil('D')\n",
    "    max_date_df = _dataframe.index.max().floor('D')\n",
    "    \n",
    "    return min_date_df, max_date_df, range_days\n",
    "\n",
    "def calculatePollutants(_dataframe, _pollutantTuples, _append, _refAvail, _dataframeRef, _deltas, _overlapHours = 0, _type_regress = 'best', _filterExpSmoothing = 0.2, _trydecomp = False, _plotsInter = False, _plotResult = True, _verbose = False, _printStats = False):\n",
    "    '''\n",
    "        Function to calculate alphasense pollutants with baseline technique\n",
    "        Input:\n",
    "            _dataframe: pandas dataframe from\n",
    "            _pollutantTuples: list of tuples containing: '[(_pollutant, _sensorID), (_pollutant, sensorID)...]\n",
    "        Output:\n",
    "            _dataframe with pollutants added\n",
    "            _metadata \n",
    "    '''\n",
    "    \n",
    "    dataframeResult = _dataframe.copy()\n",
    "    numberSensors = len(_pollutantTuples)\n",
    "    CorrParamsDict = dict()\n",
    "    \n",
    "    for sensor in range(numberSensors):\n",
    "        \n",
    "        # Get Sensor \n",
    "        pollutant = _pollutantTuples[sensor][0]\n",
    "        sensorID = _pollutantTuples[sensor][1]\n",
    "        method = _pollutantTuples[sensor][2]\n",
    "        slot = _pollutantTuples[sensor][4]\n",
    "        \n",
    "        if method == 'baseline':\n",
    "            baselineType = _pollutantTuples[sensor][3]\n",
    "        \n",
    "        # Get Sensor data\n",
    "        Sensitivity_1 = alpha_calData.loc[sensorID,'Sensitivity 1']\n",
    "        Sensitivity_2 = alpha_calData.loc[sensorID,'Sensitivity 2']\n",
    "        Target_1 = alpha_calData.loc[sensorID,'Target 1']\n",
    "        Target_2 = alpha_calData.loc[sensorID,'Target 2']\n",
    "        nWA = alpha_calData.loc[sensorID,'Zero Current']/alpha_calData.loc[sensorID,'Aux Zero Current']\n",
    "\n",
    "        if not Target_1 == pollutant:\n",
    "            print 'Sensor ID ({}) and pollutant type ({}) not matching'.format(Target_1, pollutant)\n",
    "            return\n",
    "\n",
    "        alphaW = CHANNEL_NAME(currentSensorNames, 'GASES', slot, 'WORKING', 'BOARD_AUX')\n",
    "        alphaA = CHANNEL_NAME(currentSensorNames, 'GASES', slot, 'AUXILIARY', 'BOARD_AUX')\n",
    "        temp = CHANNEL_NAME(currentSensorNames, 'TEMPERATURE', 0, '?ONE', 'BOARD_AUX')\n",
    "        hum = CHANNEL_NAME(currentSensorNames, 'HUMIDITY', 0, '?ONE', 'BOARD_AUX')\n",
    "        \n",
    "        _listNames = (alphaW, alphaA, temp, hum)\n",
    "        print _listNames\n",
    "\n",
    "        if pollutant == 'O3':\n",
    "            # Check if NO2 is already present in the dataset\n",
    "            if ('NO2'+ '_' + _append) in dataframeResult.columns:\n",
    "                pollutant_column_2 = ('NO2' + '_' + _append)\n",
    "            else:\n",
    "                print 'Change tuple order to [(CO,sensorID_CO, ...), (NO2, sensorID_NO2, ...), (O3, sensorID_O3, ...)]'\n",
    "                return\n",
    "        \n",
    "        # Get units for the pollutant in questions\n",
    "        for pollutantItem in alphaUnitsFactorsLUT:\n",
    "            \n",
    "            if pollutant == pollutantItem[0]:\n",
    "                factor_unit_1 = pollutantItem[1]\n",
    "                factor_unit_2 = pollutantItem[2]\n",
    "\n",
    "        ## Find min, max and range of days\n",
    "        min_date_df, max_date_df, range_days = findDates(_dataframe)\n",
    "        print '------------------------------------------------------------------'\n",
    "        print ('Calculation of ' + '\\033[1m{:10s}\\033[0m'.format(pollutant))\n",
    "        print 'Data Range from {} to {} with {} days'.format(min_date_df, max_date_df, range_days)\n",
    "        print '------------------------------------------------------------------'\n",
    "        \n",
    "        # Give name to pollutant column\n",
    "        pollutant_column = (pollutant + '_' + _append) \n",
    "        \n",
    "        if method == 'baseline':\n",
    "            # Select baselined - baseliner depending on the baseline type\n",
    "            if baselineType == 'single_temp':\n",
    "                baseliner = temp\n",
    "            elif baselineType == 'single_hum':\n",
    "                baseliner = hum\n",
    "            elif baselineType == 'single_aux':\n",
    "                baseliner = alphaA\n",
    "            baselined = alphaW\n",
    "            \n",
    "            # Iterate over days\n",
    "            for day in range(range_days):\n",
    "            \n",
    "                # Calculate overlap dates for that day\n",
    "                min_date_ovl = max(_dataframe.index.min(), (min_date_df + pd.DateOffset(days=day) - pd.DateOffset(hours = _overlapHours)))\n",
    "                max_date_ovl = min(_dataframe.index.max(), (min_date_ovl + pd.DateOffset(days=1) + pd.DateOffset(hours = _overlapHours + relativedelta.relativedelta(min_date_df + pd.DateOffset(days=day),min_date_ovl).hours)))\n",
    "                \n",
    "                # Calculate non overlap dates for that day\n",
    "                min_date_novl = max(min_date_df, (min_date_df + pd.DateOffset(days=day)))\n",
    "                max_date_novl = min(max_date_df, (min_date_novl + pd.DateOffset(days=1)))\n",
    "            \n",
    "                if _verbose:\n",
    "                    print '------------------------------------------------------------------'\n",
    "                    print 'Calculating day {}, with range: {} \\t to {}'.format(day, min_date_ovl, max_date_ovl)\n",
    "                    print '------------------------------------------------------------------'\n",
    "                \n",
    "                ## Trim dataframe to overlap dates\n",
    "                dataframeTrim = dataframeResult[dataframeResult.index > min_date_ovl]\n",
    "                dataframeTrim = dataframeTrim[dataframeTrim.index <= max_date_ovl]\n",
    "                \n",
    "                # Init stuff\n",
    "                if day == 0:\n",
    "                    # Init list for CorrParams\n",
    "                    CorrParams = list()\n",
    "                 \n",
    "                if dataframeTrim.empty:\n",
    "                    if _verbose:\n",
    "                        print 'No data between these dates'\n",
    "                    \n",
    "                    # Fill with nan if no data available (to avoid messing up the stats)\n",
    "                    nanV =np.ones(10)\n",
    "                    nanV.fill(np.nan)\n",
    "\n",
    "                    CorrParams.append(tuple(nanV))\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    # CALCULATE THE BASELINE PER DAY\n",
    "                    dataframeTrim[alphaW + '_baseline'], CorrParamsTrim = calculateBaselineDay(dataframeTrim, _listNames, baselined, baseliner, _deltas, _type_regress, _trydecomp, _plotsInter, _verbose)\n",
    "                    \n",
    "                    # TRIM IT BACK TO NO-OVERLAP\n",
    "                    dataframeTrim = dataframeTrim[dataframeTrim.index > min_date_novl]\n",
    "                    dataframeTrim = dataframeTrim[dataframeTrim.index <= max_date_novl]\n",
    "                    \n",
    "                    # CALCULATE ACTUAL POLLUTANT CONCENTRATION\n",
    "                    if pollutant == 'CO': \n",
    "                        # Not recommended for CO\n",
    "                        dataframeTrim[pollutant_column] = backgroundConc_CO + factor_unit_1*factorPCB*(dataframeTrim[alphaW] - dataframeTrim[alphaW + '_baseline'])/abs(Sensitivity_1)\n",
    "                    elif pollutant == 'NO2':\n",
    "                        dataframeTrim[pollutant_column] = backgroundConc_NO2 + factor_unit_1*factorPCB*(dataframeTrim[alphaW] - dataframeTrim[alphaW + '_baseline'])/abs(Sensitivity_1)\n",
    "                    elif pollutant == 'O3':\n",
    "                        dataframeTrim[pollutant_column] = backgroundConc_OX + factor_unit_1*(factorPCB*(dataframeTrim[alphaW] - dataframeTrim[alphaW + '_baseline']) - (dataframeTrim[pollutant_column_2])/factor_unit_2*abs(Sensitivity_2))/abs(Sensitivity_1)\n",
    "                    \n",
    "                    # ADD IT TO THE DATAFRAME\n",
    "                    dataframeResult = dataframeResult.combine_first(dataframeTrim)\n",
    "                    \n",
    "                    if _refAvail:\n",
    "                        ## Trim ref dataframe to no-overlap dates\n",
    "                        dataframeTrimRef = _dataframeRef[_dataframeRef.index >= dataframeTrim.index.min()]\n",
    "                        dataframeTrimRef = dataframeTrimRef[dataframeTrimRef.index <= dataframeTrim.index.max()]\n",
    "                        \n",
    "                        # Adapt dataframeTrim to be able to perform correlation\n",
    "                        if dataframeTrimRef.index.min() > dataframeTrim.index.min():\n",
    "                            dataframeTrim = dataframeTrim[dataframeTrim.index >= dataframeTrimRef.index.min()]\n",
    "                        if dataframeTrimRef.index.max() < dataframeTrim.index.max():\n",
    "                            dataframeTrim = dataframeTrim[dataframeTrim.index <= dataframeTrimRef.index.max()]\n",
    "                        \n",
    "                        if pollutant in dataframeTrimRef.columns and not dataframeTrimRef.empty:\n",
    "                            slopeRef, interceptRef, r_valueRef, p_valueRef, std_errRef = linregress(np.transpose(dataframeTrim[pollutant_column]), np.transpose(dataframeTrimRef[pollutant]))\n",
    "                        else:\n",
    "                            r_valueRef = np.nan\n",
    "                    else:\n",
    "                        r_valueRef = np.nan\n",
    "                    \n",
    "                    ## Get some metrics\n",
    "                    temp_avg = dataframeTrim[temp].mean(skipna = True)\n",
    "                    temp_stderr = dataframeTrim[temp].std(skipna = True)\n",
    "                    hum_avg = dataframeTrim[hum].mean(skipna = True)\n",
    "                    hum_stderr = dataframeTrim[hum].std(skipna = True)\n",
    "                    pollutant_avg = dataframeTrim[pollutant_column].mean(skipna = True)\n",
    "                    \n",
    "                    tempCorrParams = list(CorrParamsTrim)\n",
    "                    tempCorrParams.insert(0,r_valueRef**2)\n",
    "                    tempCorrParams.insert(1,pollutant_avg)\n",
    "                    tempCorrParams.insert(1,hum_stderr)\n",
    "                    tempCorrParams.insert(1,hum_avg)\n",
    "                    tempCorrParams.insert(1,temp_stderr)\n",
    "                    tempCorrParams.insert(1,temp_avg)                    \n",
    "                    CorrParamsTrim = tuple(tempCorrParams)\n",
    "                    CorrParams.append(CorrParamsTrim)\n",
    "            \n",
    "            ## Add relevant metadata for this method\n",
    "            labelsCP = ['r_valueRef',\n",
    "                        'avg_temp',\n",
    "                        'stderr_temp',\n",
    "                        'avg_hum',\n",
    "                        'stderr_hum',\n",
    "                        'avg_pollutant',\n",
    "                        'slopeBA', \n",
    "                        'interceptBA', \n",
    "                        'r_valueBA', \n",
    "                        'p_valueBA', \n",
    "                        'std_errBA', \n",
    "                        'deltaAuxBas_avg', \n",
    "                        'ratioAuxBas_avg', \n",
    "                        'indexMax', \n",
    "                        'valid']\n",
    "            \n",
    "            CorrParamsDF = pd.DataFrame(CorrParams, columns = labelsCP, index = [(min_date_df+ pd.DateOffset(days=days)).strftime('%Y-%m-%d') for days in range(range_days)])\n",
    "            \n",
    "            ## Find average ratio for hole dataset\n",
    "            deltaAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].mean(skipna = True)\n",
    "            deltaAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].std(skipna = True)\n",
    "            ratioAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].mean(skipna = True)\n",
    "            ratioAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].std(skipna = True)\n",
    "                    \n",
    "            # SHOW SOME METADATA FOR THE BASELINES FOUND\n",
    "            if _printStats:\n",
    "                        \n",
    "                print '------------------------'\n",
    "                print ' Meta Data'\n",
    "                print '------------------------'\n",
    "                display(CorrParamsDF)\n",
    "                        \n",
    "                print '------------------------'\n",
    "                print 'Average Delta between baseline and auxiliary electrode: {}, and ratio {}:'.format(deltaAuxBas_avg, ratioAuxBas_avg)\n",
    "                print 'Std Dev of Delta between baseline and auxiliary electrode: {}, and ratio {}:'.format(deltaAuxBas_std, ratioAuxBas_std)\n",
    "                print '------------------------'\n",
    "                    \n",
    "        elif method == 'classic':\n",
    "            \n",
    "            ## CorrParams\n",
    "            CorrParamsTrim = list()\n",
    "            \n",
    "            if pollutant == 'CO': \n",
    "                dataframeResult[pollutant_column] = factor_unit_1*factorPCB*(dataframeResult[alphaW] - nWA*dataframeResult[alphaA])/abs(Sensitivity_1)\n",
    "            elif pollutant == 'NO2':\n",
    "                dataframeResult[pollutant_column] = factor_unit_1*factorPCB*(dataframeResult[alphaW] - nWA*dataframeResult[alphaA])/abs(Sensitivity_1)\n",
    "            elif pollutant == 'O3':\n",
    "                dataframeResult[pollutant_column] = factor_unit_1*(factorPCB*(dataframeResult[alphaW] - nWA*dataframeResult[alphaA]) - (dataframeResult[pollutant_column_2])/factor_unit_2*abs(Sensitivity_2))/abs(Sensitivity_1)\n",
    "            \n",
    "            ## Calculate stats day by day to avoid stationarity\n",
    "            min_date_df, max_date_df, range_days = findDates(dataframeResult)\n",
    "            print 'Data Range from {} to {} with {} days'.format(min_date_df, max_date_df, range_days)\n",
    "            \n",
    "            for day in range(range_days):\n",
    "                ## CorrParams\n",
    "                CorrParamsTrim = list()\n",
    "                \n",
    "                # Calculate non overlap dates for that day\n",
    "                min_date_novl = max(min_date_df, (min_date_df + pd.DateOffset(days=day)))\n",
    "                max_date_novl = min(max_date_df, (min_date_novl + pd.DateOffset(days=1)))\n",
    "                \n",
    "                ## Trim dataframe to no-overlap dates\n",
    "                dataframeTrim = dataframeResult[dataframeResult.index > min_date_novl]\n",
    "                dataframeTrim = dataframeTrim[dataframeTrim.index <= max_date_novl]\n",
    "                \n",
    "                if _refAvail:\n",
    "                    ## Trim ref dataframe to no-overlap dates\n",
    "                    dataframeTrimRef = _dataframeRef[_dataframeRef.index >= dataframeTrim.index.min()]\n",
    "                    dataframeTrimRef = dataframeTrimRef[dataframeTrimRef.index <= dataframeTrim.index.max()]\n",
    "                    \n",
    "                    # Adapt dataframeTrim to be able to perform correlation\n",
    "                    if dataframeTrimRef.index.min() > dataframeTrim.index.min():\n",
    "                        dataframeTrim = dataframeTrim[dataframeTrim.index >= dataframeTrimRef.index.min()]\n",
    "                    if dataframeTrimRef.index.max() < dataframeTrim.index.max():\n",
    "                        dataframeTrim = dataframeTrim[dataframeTrim.index <= dataframeTrimRef.index.max()]                    \n",
    "\n",
    "                    if pollutant in dataframeTrimRef.columns and not dataframeTrimRef.empty:\n",
    "                        slopeRef, interceptRef, r_valueRef, p_valueRef, std_errRef = linregress(np.transpose(dataframeTrimRef[pollutant]),np.transpose(dataframeTrim[pollutant_column]))\n",
    "                        CorrParamsTrim.append(r_valueRef**2)\n",
    "                    else:\n",
    "                        CorrParamsTrim.append(np.nan)\n",
    "                else:\n",
    "                    CorrParamsTrim.append(np.nan)\n",
    "                \n",
    "                ## Get some metrics\n",
    "                CorrParamsTrim.append(dataframeTrim[temp].mean(skipna = True))\n",
    "                CorrParamsTrim.append(dataframeTrim[temp].std(skipna = True))\n",
    "                CorrParamsTrim.append(dataframeTrim[hum].mean(skipna = True))\n",
    "                CorrParamsTrim.append(dataframeTrim[hum].std(skipna = True))\n",
    "                CorrParamsTrim.append(dataframeTrim[pollutant_column].mean(skipna = True))\n",
    "                \n",
    "                if day == 0:\n",
    "                    CorrParams = list()\n",
    "                \n",
    "                CorrParams.append(CorrParamsTrim)\n",
    "            \n",
    "            ## TODO: Add relevant metadata for this method\n",
    "            labelsCP = ['r_valueRef',\n",
    "                        'avg_temp',\n",
    "                        'stderr_temp',\n",
    "                        'avg_hum',\n",
    "                        'stderr_hum',\n",
    "                        'avg_pollutant']\n",
    "            \n",
    "            CorrParamsDF = pd.DataFrame(CorrParams, columns = labelsCP, index = [(min_date_df+ pd.DateOffset(days=days)).strftime('%Y-%m-%d') for days in range(range_days)])\n",
    "            \n",
    "            # SHOW SOME METADATA FOR THE BASELINES FOUND\n",
    "            if _printStats:\n",
    "            \n",
    "                print '------------------------'\n",
    "                print ' Meta Data'\n",
    "                print '------------------------'\n",
    "                display(CorrParamsDF)\n",
    "        \n",
    "        # FILTER IT\n",
    "        dataframeResult[pollutant_column + '_filter'] = exponential_smoothing(dataframeResult[pollutant_column].fillna(0), filterExpSmoothing)\n",
    "                \n",
    "        ## RETRIEVE METADATA\n",
    "        CorrParamsDict[pollutant] = CorrParamsDF\n",
    "        \n",
    "        ## TODO - Make the check for outliers and mark them out\n",
    "        # CorrParamsDF['valid'] = deltaAuxBas_avg-deltaAuxBas_std <= CorrParamsDF['deltaAuxBas_avg'] <= deltaAuxBas_avg-deltaAuxBas_std\n",
    "        # CorrParamsDF['valid'] = ratioAuxBas_avg-ratioAuxBas_std <= CorrParamsDF['ratioAuxBas_avg'] <= ratioAuxBas_avg-ratioAuxBas_std\n",
    "        # \n",
    "        # print CorrParamsDF\n",
    "        # \n",
    "        # deltaAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].mean(skipna = True)\n",
    "        # deltaAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].std(skipna = True)\n",
    "        # ratioAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].mean(skipna = True)\n",
    "        # ratioAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].std(skipna = True)\n",
    "        \n",
    "        # PLOT THINGS IF REQUESTED\n",
    "        if _plotResult:\n",
    "            \n",
    "            fig1 = tls.make_subplots(rows=4, cols=1, shared_xaxes=True, print_grid=False)\n",
    "            \n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaW], 'type': 'scatter', 'line': dict(width = 2), 'name': dataframeResult[alphaW].name}, 1, 1)\n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaA], 'type': 'scatter', 'line': dict(width = 2), 'name': dataframeResult[alphaA].name}, 1, 1)\n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaA] * nWA, 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': 'AuxCor Alphasense'}, 1, 1)\n",
    "            if method == 'baseline':\n",
    "                fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaW + '_baseline'], 'type': 'scatter', 'line': dict(width = 2), 'name': 'Baseline'}, 1, 1)\n",
    "            \n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[pollutant_column], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframeResult[pollutant_column].name}, 2, 1)\n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[pollutant_column + '_filter'], 'type': 'scatter', 'name': (dataframeResult[pollutant_column + '_filter'].name)}, 2, 1)\n",
    "            \n",
    "            if _refAvail:\n",
    "                # take the reference and check if it's available\n",
    "                if pollutant in _dataframeRef.columns:\n",
    "                    # If all good, plot it\n",
    "                    fig1.append_trace({'x': _dataframeRef.index, 'y': _dataframeRef[pollutant], 'type': 'scatter', 'name': _dataframeRef[pollutant].name}, 2, 1)\n",
    "                \n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[temp], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframeResult[temp].name}, 3, 1)\n",
    "            fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[hum], 'type': 'scatter', 'name': (dataframeResult[hum].name)}, 4, 1)\n",
    "            \n",
    "            fig1['layout'].update(height = 1000, \n",
    "                                  legend=dict(x=-.1, y=5), \n",
    "                                  xaxis=dict(title='Time'), \n",
    "                                  title = 'Baseline Correction for {}'.format(pollutant),\n",
    "                                  yaxis1 = dict(title='Sensor Output - mV'), \n",
    "                                  yaxis2 = dict(title='Pollutant - ppm'),\n",
    "                                  yaxis3 = dict(title='Temperature - degC'),\n",
    "                                  yaxis4 = dict(title='Humidity - %'),\n",
    "                                 )\n",
    "                                   \n",
    "            ply.offline.iplot(fig1)\n",
    "    \n",
    "    return dataframeResult, CorrParamsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testAlphaSense = list()\n",
    "\n",
    "selectedTestsAD = tuple()\n",
    "def selectTestAD(x):\n",
    "    global selectedTestsAD\n",
    "    selectedTestsAD = list(x)\n",
    "    \n",
    "def calculateCorrectionAD(b):\n",
    "    clear_output()\n",
    "    for testAD in selectedTestsAD:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testAD]:\n",
    "            if 'is_reference' in readings[testAD][reading]:\n",
    "                print 'Reference found'\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testAD][reading]['data']\n",
    "\n",
    "        for kit in readings[testAD]:\n",
    "            if 'alphasense' in readings[testAD][kit]:\n",
    "                \n",
    "                sensorID = readings[testAD][kit]['alphasense']\n",
    "                sensorID_CO = readings[testAD][kit]['alphasense']['CO']\n",
    "                sensorID_NO2 = readings[testAD][kit]['alphasense']['NO2']\n",
    "                sensorID_OX = readings[testAD][kit]['alphasense']['O3']\n",
    "                sensorSlots = readings[testAD][kit]['alphasense']['SLOTS']\n",
    "                               \n",
    "                sensorID = (['CO', sensorID_CO, 'classic', '', sensorSlots.index('CO')+1], \n",
    "                            ['NO2', sensorID_NO2, 'baseline', 'single_aux', sensorSlots.index('NO2')+1], \n",
    "                            ['O3', sensorID_OX, 'baseline', 'single_aux', sensorSlots.index('O3')+1])\n",
    "                \n",
    "                # Calculate correction\n",
    "                readings[testAD][kit]['data'], CorrParams = calculatePollutants(\n",
    "                        _dataframe = readings[testAD][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = 'baseline',\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltas,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have alphasense values\n",
    "for test in readings:\n",
    "    for kit in readings[test]:\n",
    "        if 'alphasense' in readings[test][kit]:\n",
    "            testAlphaSense.append(test)\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing alphasense to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestAD,\n",
    "         x = widgets.SelectMultiple(options=testAlphaSense, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsAD, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionAD)\n",
    "\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Correction Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample For stats checks\n",
    "pollutant = 'NO2'\n",
    "display(CorrParams[pollutant])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax1.set_ylabel('Avg Temp-Hum / day')\n",
    "    ax1.grid(True)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax2.set_ylabel('Avg Temp / day')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig2, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax3.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_pollutant'], label = 'Avg Pollutant', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['deltaAuxBas_avg'], label = 'Delta', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['ratioAuxBas_avg'] , label = 'Ratio', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax3.legend(loc='best')\n",
    "    ax3.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax3.set_ylabel('Avg {} / day'.format(pollutant))\n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc='best')\n",
    "    ax4.set_xlabel('{} Average'.format(pollutant))\n",
    "    ax4.set_ylabel('Offset / Ratio Baseline vs Auxiliary')\n",
    "    ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Data Export\n",
    "Export data to csv with formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global selected\n",
    "selected = []\n",
    "\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "def exportFile(b):\n",
    "    for i in range(len(selected)):\n",
    "        b.f = selected[i]\n",
    "        exportDir = exportPath.value\n",
    "        if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "        savePath = os.path.join(exportDir, b.f)\n",
    "        if not os.path.exists(savePath):\n",
    "            readings[b.f].to_csv(savePath, sep=\",\")\n",
    "            display(FileLink(savePath))\n",
    "        else:\n",
    "            display(widgets.HTML(' File Already exists!'))\n",
    "\n",
    "display(widgets.HTML('<h3>Export Files</h3>'))\n",
    "exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='700px'))\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportFile)\n",
    "\n",
    "interact(selectedFilesChannels,x = widgets.SelectMultiple(options=readings.keys(), description='Select multiple files', selected_labels = selected,layout=widgets.Layout(width='700px')))\n",
    "exportBox = widgets.HBox([exportPath,eb])\n",
    "display(exportBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = readings[Source].index.min()._short_repr\n",
    "    _max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "    del axisshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        axisshow.append(_axis_drop.value)\n",
    "        \n",
    "    plot_data = readings[toshow[0][0]].loc[:,(toshow[0][1],)]\n",
    "    list_data_primary = []\n",
    "    list_data_secondary = []\n",
    "    list_data_terciary = []\n",
    "    \n",
    "    if b.slice_time:\n",
    "        plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "        plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            plot_data = pd.merge(plot_data, readings[toshow[i][0]].loc[:,(toshow[i][1],)], left_index=True, right_index=True)\n",
    "\n",
    "    print '-------------------------------------'\n",
    "    print ' Medias:\\n'\n",
    "    meanTable = []\n",
    "    for d in toshow:\n",
    "        myMean = ' ' + d[0]  + \"\\t\" + d[1] + \"\\t\"\n",
    "        meanTable.append(myMean)   \n",
    "    res = plot_data.mean()\n",
    "    for i in range(len(meanTable)): print meanTable[i] + '%.2f' % (res[i])\n",
    "    print '-------------------------------------'\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    for i in range(len(plot_data.columns)):\n",
    "        changed.append(toshow[i][0] + ' - '+ plot_data.columns[i])\n",
    "    plot_data.columns = changed\n",
    "    \n",
    "    subplot_rows = 0\n",
    "    if len(toshow) > 0:\n",
    "        for i in range(len(toshow)):\n",
    "            if axisshow[i]=='1': \n",
    "                list_data_primary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,1)\n",
    "            if axisshow[i]=='2': \n",
    "                list_data_secondary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,2)\n",
    "            if axisshow[i]=='3': \n",
    "                list_data_terciary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,3)\n",
    "          \n",
    "        \n",
    "    fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "\n",
    "    #if len(list_data_primary)>0:\n",
    "        #fig1 = plot_data.iplot(kind='scatter', y = list_data_primary, asFigure=True, layout = layout)\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    for i in range(len(list_data_primary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "\n",
    "    for i in range(len(list_data_secondary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "    \n",
    "    for i in range(len(list_data_terciary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "\n",
    "    if setLimits: \n",
    "        fig1['layout'].update(height = 600,\n",
    "                            legend=dict(x=-.1, y=5) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                          \n",
    "    else:\n",
    "        fig1['layout'].update(height = 600,\n",
    "                              legend=dict(x=-.1, y=5) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                           \n",
    "    ply.offline.iplot(fig1)\n",
    "            \n",
    "def reset_time(b):\n",
    "    _min_date.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(show_sensors, Source=_kit, layout=layout)\n",
    "\n",
    "_sensor_drop = widgets.Dropdown(layout=layout)\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, description='Synchronise X axis', disabled=False, layout=widgets.Layout(width='300px'))\n",
    "_min_date = widgets.Text(description='Start date:', layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', layout=widgets.Layout(width='330px'))\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "_b_reset_time.src = _kit\n",
    "\n",
    "_sensor_box = widgets.HBox([_kit_drop, _sensor_drop])\n",
    "_plot_box = widgets.HBox([_axis_drop, _synchroniseXaxis, _b_add , _b_reset_all])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_time_box, _sensor_box, _plot_box])\n",
    "display(_root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Basic Sensor Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Seaborn Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paint(Source):\n",
    "    clear_output()\n",
    "    sns.set(font_scale=1.4)\n",
    "    g = sns.PairGrid(readings.values()[0])\n",
    "    g = g.map(plt.scatter)\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(paint, Source=_kit, layout=layout)\n",
    "display(_kit_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Seaborn XYPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    A_sensors_drop.source = Source\n",
    "    minCropDate.value = readings[Source].index.min()._short_repr\n",
    "    maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    B_sensors_drop.source = Source\n",
    "    minCropDate.value = readings[Source].index.min()._short_repr\n",
    "    maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def redraw(b):\n",
    "    cropTime = cropTimeCheck.value\n",
    "    doubleAxis = doubleAxisCheck.value\n",
    "    min_date = minCropDate.value\n",
    "    max_date = maxCropDate.value\n",
    "    mergedData = pd.merge(readings[A_kit.value].loc[:,(A_sensors_drop.value,)], readings[B_kit.value].loc[:,(B_sensors_drop.value,)], left_index=True, right_index=True, suffixes=('_'+A_kit.value, '_'+B_kit.value))\n",
    "    clear_output()\n",
    "    \n",
    "    if cropTime:\n",
    "        mergedData = mergedData[mergedData.index > min_date]\n",
    "        mergedData = mergedData[mergedData.index < max_date]\n",
    "        \n",
    "    #jointplot\n",
    "    df = pd.DataFrame()\n",
    "    A = A_sensors_drop.value + '-' + A_kit.value\n",
    "    B = B_sensors_drop.value + '-' + B_kit.value\n",
    "    df[A] = mergedData.iloc[:,0]\n",
    "    df[B] = mergedData.iloc[:,1]\n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", size=12, scatter_kws={\"s\": 80});\n",
    "    print \"data from \" + str(df.index.min()) + \" to \" + str(df.index.max())                      \n",
    "    pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "    print 'Pearson correlation coefficient: ' + str(pearsonCorr)\n",
    "    print 'Coefficient of determination R²: ' + str(pearsonCorr*pearsonCorr)\n",
    "\n",
    "    if cropTime: \n",
    "        \n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "        \n",
    "    trace0 = go.Scatter(x=df[A].index, y=df[A], name = A,line = dict(color='rgb(0,97,255)'))\n",
    "    \n",
    "    if (doubleAxis):\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "    else:\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "    data = [trace0, trace1]\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(figure)\n",
    "\n",
    "  # Delta \n",
    "    delta = df[A]-df[B]\n",
    "    trace0 = go.Scatter(x = df[A].index, y = delta, mode = 'lines')\n",
    "    if cropTime: \n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Delta',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    else:\n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Delta',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    data = [trace0]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(fig)\n",
    "   \n",
    "    # Ratio\n",
    "    ratio = df[A]*1./df[B]\n",
    "    trace0 = go.Scatter(x = df[A].index, y = ratio, mode = 'lines')\n",
    "    if cropTime: \n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Ratio',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    else:\n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Ratio',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    data = [trace0]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(fig)\n",
    "    \n",
    "    # Rolling correlation\n",
    "  # fig = plt.figure(figsize=(15,6))\n",
    "  # roll = mergedData.iloc[:,0].rolling(12).corr(mergedData.iloc[:,1])\n",
    "  # trace0 = go.Scatter(x = df[A].index, y = roll, mode = 'lines')\n",
    "  # if cropTime: \n",
    "  #     layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Rolling Average',zerolinecolor='#990000',zerolinewidth=1))\n",
    "  # else:\n",
    "  #     layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Rolling Correlation',zerolinecolor='#990000',zerolinewidth=1))\n",
    "  # data = [trace0]\n",
    "  # fig = go.Figure(data=data, layout=layout)\n",
    "  # ply.offline.iplot(fig)\n",
    "    \n",
    "    # Interactive Correlation\n",
    "\n",
    "  # t = df[A].index\n",
    "  # x = df[A]\n",
    "  # y = df[B]\n",
    "\n",
    "  # trace1 = go.Scatter(\n",
    "  #     x=x, y=y, mode='markers', name='points',\n",
    "  #     marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n",
    "  # )\n",
    "  # trace2 = go.Histogram2dcontour(\n",
    "  #     x=x, y=y, name=df[A].column, ncontours=20,\n",
    "  #     colorscale='Hot', reversescale=True, showscale=False\n",
    "  # )\n",
    "  # trace3 = go.Histogram(\n",
    "  #     x=x, name=df[A].column,\n",
    "  #     marker=dict(color='rgb(102,0,0)'),\n",
    "  #     yaxis='y2'\n",
    "  # )\n",
    "  # trace4 = go.Histogram(\n",
    "  #     y=y, name=df[B].column, marker=dict(color='rgb(102,0,0)'),\n",
    "  #     xaxis='x2'\n",
    "  # )\n",
    "  # data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "  # layout = go.Layout(\n",
    "  #     showlegend=True,\n",
    "  #     autosize=False,\n",
    "  #     width=600,\n",
    "  #     height=550,\n",
    "  #     xaxis=dict(\n",
    "  #         domain=[0, 0.85],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     ),\n",
    "  #     yaxis=dict(\n",
    "  #         domain=[0, 0.85],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     ),\n",
    "  #     margin=dict(\n",
    "  #         t=50\n",
    "  #     ),\n",
    "  #     hovermode='closest',\n",
    "  #     bargap=0,\n",
    "  #     xaxis2=dict(\n",
    "  #         domain=[0.85, 1],\n",
    "  #     showgrid=True,\n",
    "  #     zeroline=False\n",
    "  #     ),\n",
    "  #     yaxis2=dict(\n",
    "  #         domain=[0.85, 1],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     )\n",
    "  # )\n",
    "\n",
    "  # fig = go.Figure(data=data, layout=layout)\n",
    "  # ply.offline.iplot(fig)\n",
    "    \n",
    "if len(readings) < 1: print \"Please load some data first...\"\n",
    "else:\n",
    "    \n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Redraw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    A_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px') ,value=readings.keys()[0])\n",
    "    A_kit_drop = widgets.interactive(show_sensors_A, Source=A_kit, layout=layout)\n",
    "    A_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    show_sensors_A(readings.keys()[0])\n",
    "    \n",
    "    B_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px'), value=readings.keys()[1])\n",
    "    B_kit_drop = widgets.interactive(show_sensors_B, Source= B_kit, layout=layout)\n",
    "    B_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    show_sensors_B(readings.keys()[1])\n",
    "    \n",
    "    draw_box = widgets.HBox([b_redraw, doubleAxisCheck], layout=widgets.Layout(justify_content='space-between'))\n",
    "    kit_box = widgets.HBox([A_kit, widgets.HTML('<h4><< Data source >></h4>') , B_kit], layout=widgets.Layout(justify_content='space-between'))\n",
    "    sensor_box = widgets.HBox([A_sensors_drop, widgets.HTML('<h4><< Sensor selection >></h4>') , B_sensors_drop], layout=widgets.Layout(justify_content='space-between'))\n",
    "    crop_box = widgets.HBox([cropTimeCheck, minCropDate, maxCropDate], layout=widgets.Layout(justify_content='space-between'))\n",
    "    root_box = widgets.VBox([draw_box, kit_box, sensor_box, crop_box])\n",
    "    \n",
    "    display(root_box)\n",
    "    \n",
    "    #redraw(b_redraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Anomaly Detection\n",
    "\n",
    "Check this here https://annals-csis.org/proceedings/2012/pliks/118.pdf.\n",
    "\n",
    "Below we'll use the Holt-Winters function as defined:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n",
    "\n",
    "$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "    \n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    \n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        \n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "                     \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functionality\n",
    "Below, we will define some basic functionality for smoothing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick** demonstration of smoothing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Smoothing tests\n",
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "dataframe.columns\n",
    "\n",
    "plotExponentialSmoothing(dataframe['NOX ug/m3'], [0.1])\n",
    "plotDoubleExponentialSmoothing(dataframe['NOX ug/m3'], alphas=[0.05], betas=[0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and data training split\n",
    "\n",
    "The following code uses cross validation on rolling basis structure:\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "## sklearn Time Series functions and data split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error#, mean_squared_log_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Select variables here \n",
    "\n",
    "Do Interface for variable selection and reference setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "df = dataframe[['NOX ug/m3', 'NO ug/m3']]\n",
    "df.names = [['NOX', ' NO']]\n",
    "print df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Preliminary Checks\n",
    "\n",
    "##### TODO Dicker-fuller test (ADF)\n",
    "\n",
    "Use this test to verify **data stationarity**.\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_fuller_results = sm.tsa.stattools.adfuller(df['NOX ug/m3'])\n",
    "\n",
    "adf = ad_fuller_results[0]\n",
    "pvalue = ad_fuller_results[1]\n",
    "usedlag = ad_fuller_results[2]\n",
    "nobs = ad_fuller_results[3]\n",
    "print 'ADF- Statistic: {}\\npvalue: {}\\nUsed Lag: {}\\nnobs: {}\\n'.format(adf, pvalue, usedlag, nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO Granger Casuality Test\n",
    "\n",
    "Use this test to determine the casuality of variables (which causes the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sm.tsa.stattools.grangercausalitytests(df[['NOX ug/m3','NO ug/m3']].dropna(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Naive Linear Regression\n",
    "\n",
    "Use this only for basic checking and baseline model setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "\n",
    "df['const']=1\n",
    "print df.head(4)\n",
    "\n",
    "model1=sm.OLS(endog=df['NOX ug/m3'],exog=df[['NO ug/m3','const']])\n",
    "results1=model1.fit()\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Linear Regression with differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['const']=1\n",
    "\n",
    "df['diff_NOX ug/m3']=df['NOX ug/m3'].diff()\n",
    "df['diffNO ug/m3']=df['NO ug/m3'].diff()\n",
    "model2=sm.OLS(endog=df['diff_NOX ug/m3'].dropna(),exog=df[['diffNO ug/m3','const']].dropna())\n",
    "results2=model1.fit()\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO ARIMA(X) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Master = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "Slave = readings['MA04_Formulae_2.csv'].dropna()\n",
    "max_date = min( Master.index[-1], Slave.index[-1])\n",
    " \n",
    "mergedData = pd.merge(Master.loc[:,], Slave.loc[:,], left_index=True, right_index=True)\n",
    "\n",
    "print mergedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predictors = Master[['UMID %' ,'NO ug/m3', 'VV m/s', 'O3 ug/m3']]\n",
    "\n",
    "Predictors = mergedData[['AD_2_TN_smooth (ppb)', 'AlphaDelta Humidity-%']]\n",
    "to_predict = mergedData['NOX ug/m3']\n",
    "\n",
    "TrainingSize = int(0.8*to_predict.shape[0])\n",
    "TestSize = to_predict.shape[0] - TrainingSize\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(to_predict[:TrainingSize],\n",
    "              exog=Predictors[:TrainingSize],\n",
    "              order= (2,1,3),\n",
    "              enforce_invertibility=False,trend='c')\n",
    "                    \n",
    "res = mod.fit(disp=0)\n",
    "                    \n",
    "frc =res.forecast(TestSize,exog=pd.DataFrame(Predictors)[TrainingSize:])\n",
    "\n",
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(mergedData.index[TrainingSize:], frc, 'r')\n",
    "    plt.plot(mergedData.index[TrainingSize:], to_predict[TrainingSize:], 'k.')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"SARIMAX Model Prediction\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Include R in Python Notebook and test it out below - do not modify the first line (%%R -i ...)\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRAN mirror for use in this session\n",
    "# Secure CRAN mirrors\n",
    "\n",
    "#1: 0-Cloud [https]                   2: Algeria [https]\n",
    "#3: Australia (Canberra) [https]      4: Australia (Melbourne 1) [https]\n",
    "#5: Australia (Melbourne 2) [https]   6: Australia (Perth) [https]\n",
    "#7: Austria [https]                   8: Belgium (Ghent) [https]\n",
    "#9: Brazil (PR) [https]              10: Brazil (RJ) [https]\n",
    "#11: Brazil (SP 1) [https]            12: Brazil (SP 2) [https]\n",
    "#13: Bulgaria [https]                 14: Chile 1 [https]\n",
    "#15: Chile 2 [https]                  16: China (Guangzhou) [https]\n",
    "#17: China (Lanzhou) [https]          18: China (Shanghai) [https]\n",
    "#19: Colombia (Cali) [https]          20: Czech Republic [https]\n",
    "#21: Denmark [https]                  22: East Asia [https]\n",
    "#23: Ecuador (Cuenca) [https]         24: Ecuador (Quito) [https]\n",
    "#25: Estonia [https]                  26: France (Lyon 1) [https]\n",
    "#27: France (Lyon 2) [https]          28: France (Marseille) [https]\n",
    "#29: France (Montpellier) [https]     30: France (Paris 2) [https]\n",
    "#31: Germany (Erlangen) [https]       32: Germany (Göttingen) [https]\n",
    "#33: Germany (Münster) [https]        34: Greece [https]\n",
    "#35: Iceland [https]                  36: India [https]\n",
    "#37: Indonesia (Jakarta) [https]      38: Ireland [https]\n",
    "#39: Italy (Padua) [https]            40: Japan (Tokyo) [https]\n",
    "#41: Japan (Yonezawa) [https]         42: Korea (Ulsan) [https]\n",
    "#43: Malaysia [https]                 44: Mexico (Mexico City) [https]\n",
    "#45: Norway [https]                   46: Philippines [https]\n",
    "#47: Serbia [https]                   48: Spain (A Coruña) [https]\n",
    "#49: Spain (Madrid) [https]           50: Sweden [https]\n",
    "#51: Switzerland [https]              52: Turkey (Denizli) [https]\n",
    "#53: Turkey (Mersin) [https]          54: UK (Bristol) [https]\n",
    "#55: UK (Cambridge) [https]           56: UK (London 1) [https]\n",
    "#57: USA (CA 1) [https]               58: USA (IA) [https]\n",
    "#59: USA (KS) [https]                 60: USA (MI 1) [https]\n",
    "#61: USA (NY) [https]                 62: USA (OR) [https]\n",
    "#63: USA (TN) [https]                 64: USA (TX 1) [https]\n",
    "#65: Vietnam [https]                  66: (other mirrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "base = rpackages.importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=49) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = [\"ggplot2\",\n",
    "             \"car\",\n",
    "             \"lattice\",\n",
    "             \"dyn\",\n",
    "             \"dynlm\",\n",
    "             \"zoo\",\n",
    "             \"tseries\",\n",
    "             \"lmtest\",\n",
    "             \"xts\",\n",
    "             \"tidyverse\",\n",
    "             \"lubridate\",\n",
    "             \"lme4\",\n",
    "             \"multcomp\",\n",
    "             \"signal\",\n",
    "             \"ggfortify\"]\n",
    "\n",
    "# Selectively install what needs to be install.\n",
    "for x in packnames:\n",
    "    if not rpackages.isinstalled(x):\n",
    "        utils.install_packages(StrVector(x))\n",
    "\n",
    "# import R's \"GlobalEnv\" to evaluate the function\n",
    "from rpy2.robjects import globalenv\n",
    "\n",
    "# ggplot2 = rpackages.importr('ggplot2')\n",
    "# graphics = rpackages.importr('graphics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load in the libraries\n",
    "library(\"ggplot2\")\n",
    "library(\"car\")\n",
    "library(\"lattice\")\n",
    "library(\"dyn\")\n",
    "library(\"dynlm\")\n",
    "library(\"zoo\")\n",
    "library(\"tseries\")\n",
    "library(\"lmtest\")\n",
    "library(\"xts\")\n",
    "library(\"tidyverse\")\n",
    "library(\"lubridate\")\n",
    "library(\"lme4\")\n",
    "library(\"multcomp\")\n",
    "library(\"signal\")\n",
    "library('GGally')\n",
    "library('plotly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to R Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "toshow = []\n",
    "min_date_training = 0\n",
    "max_date_training = 0\n",
    "min_date_eval = 0\n",
    "max_date_eval = 0\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date_training.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_training.value = readings[Source].index.max()._short_repr\n",
    "    _min_date_eval.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "      \n",
    "    \n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        \n",
    "    global dataframe_export\n",
    "    dataframe_export = readings[toshow[0][0]].loc[:,(toshow[0][1],)]\n",
    "    \n",
    "    #if b.slice_time:\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "    \n",
    "    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #print 'Min Date Training / Eval'\n",
    "    #print _min_date_training.value\n",
    "    #print _min_date_eval.value\n",
    "    #print min(_min_date_training.value,_min_date_eval.value)\n",
    "    #print 'Max Date Training / Eval'\n",
    "    #print _max_date_training.value\n",
    "    #print _max_date_eval.value\n",
    "    #print max(_max_date_training.value,_max_date_eval.value)\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            dataframe_export = pd.merge(dataframe_export, readings[toshow[i][0]].loc[:,(toshow[i][1],)], left_index=True, right_index=True)\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    \n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        changed.append(toshow[i][0] + '-'+ dataframe_export.columns[i])\n",
    "    dataframe_export.columns = changed\n",
    "    \n",
    "    #text=[i  for i in range(len(dataframe_export.columns))]\n",
    "    #for i in range(len(dataframe_export.columns)):\n",
    "    #    item = dataframe_export.columns[i]\n",
    "    #    #print \"data\" + str(i)\n",
    "    #    #print item\n",
    "    #    fileName = item[:item.find('.')]\n",
    "    #    #print fileName\n",
    "    #    channel = item[item.find('-')+1:].split('-')[0]\n",
    "    #    if (len(item[item.find('-')+1:].split('-'))>0):\n",
    "    #        unit = item[item.find('-')+1:].split('-')[1]\n",
    "    #    else:\n",
    "    #        unit = ''\n",
    "    #    #print channel\n",
    "    #    #print unit\n",
    "    #    text[i]='<br>File: '+'{:s}'+str(fileName)+'<br>Channel: '+'{:s}'+str(channel)+\\\n",
    "    #    '<br>Unit: '+'{:s}'+ str(unit)\n",
    "    #    #print text[i]\n",
    "    \n",
    "    fig2 = tls.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        fig2.append_trace({'x': dataframe_export.index, \n",
    "                          'y': dataframe_export.iloc[:,i], \n",
    "                          'type': 'scatter',\n",
    "                          'name': dataframe_export.columns[i]}, 1, 1)\n",
    "\n",
    "\n",
    "    fig2['layout'].update(\n",
    "        height=800,\n",
    "        showlegend = True,\n",
    "        legend=dict(x=-.1, y=5) ,\n",
    "        xaxis=dict(\n",
    "            rangeslider=dict(),\n",
    "            type='date'\n",
    "        ),\n",
    "        annotations=[dict(\n",
    "                        x=_min_date_training.value,\n",
    "                        y=1,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Training Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(44, 160, 101, 1)')\n",
    "                    ),\n",
    "                     dict(\n",
    "                        x=_min_date_eval.value,\n",
    "                        y=0.95,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Evaluation Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(160, 160, 0, 1)')\n",
    "                    )\n",
    "                    ],\n",
    "        shapes=[\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_training.value,\n",
    "                    x1=_max_date_training.value,\n",
    "                    y0=0.95,\n",
    "                    y1=1,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(44, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(44, 160, 101,0.6)'),\n",
    "                    ),\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_eval.value,\n",
    "                    x1=_max_date_eval.value,\n",
    "                    y0=0.9,\n",
    "                    y1=0.95,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(160, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(160, 160, 0, 0.6)')\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #fig1 = dataframe_export.iplot(kind='scatter', asFigure=True, layout = layout, hoverinfo='text')\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    print list(dataframe_export.columns.values.tolist())\n",
    "    \n",
    "    ply.offline.iplot(fig2)\n",
    "\n",
    "def reset_time_t(b):\n",
    "    _min_date_training.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_training.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "    \n",
    "def reset_time_e(b):\n",
    "    _min_date_eval.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "\n",
    "def export_dataFrame(b):\n",
    "    clear_output()\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects import r\n",
    "    \n",
    "    global r_train_dataframe, train_dataframe, r_eval_dataframe, eval_dataframe\n",
    "\n",
    "    train_dataframe = dataframe_export[dataframe_export.index > _min_date_training.value]\n",
    "    train_dataframe = train_dataframe[train_dataframe.index < _max_date_training.value]\n",
    "    train_dataframe.index=train_dataframe.index.to_datetime()\n",
    "    r_train_dataframe = pandas2ri.py2ri(train_dataframe)\n",
    "    \n",
    "    eval_dataframe = dataframe_export[dataframe_export.index > _min_date_eval.value]\n",
    "    eval_dataframe = eval_dataframe[eval_dataframe.index < _max_date_eval.value]\n",
    "    eval_dataframe.index=eval_dataframe.index.to_datetime()\n",
    "    r_eval_dataframe = pandas2ri.py2ri(eval_dataframe)\n",
    "    \n",
    "    %Rpush r_train_dataframe r_eval_dataframe\n",
    "    \n",
    "    print 'Export to R Training dataframe successful with following channels'\n",
    "    %R print(colnames(r_train_dataframe))\n",
    "    min_date_training = _min_date_training.value\n",
    "    max_date_training= _max_date_training.value\n",
    "    \n",
    "    print 'With Date Range'\n",
    "    print min_date_training\n",
    "    print max_date_training\n",
    "    \n",
    "    print ''\n",
    "   \n",
    "    print 'Export to R Evaluation dataframe successful with following channels'\n",
    "    %R print(colnames(r_eval_dataframe))\n",
    "    min_date_eval = str(_min_date_eval.value)\n",
    "    max_date_eval= str(_max_date_eval.value)\n",
    "\n",
    "    print 'With Date Range'\n",
    "    print min_date_eval\n",
    "    print max_date_eval\n",
    "        \n",
    "    refFile = str(_refList.value)\n",
    "    refFile = refFile[:refFile.find('.')]\n",
    "    \n",
    "    print ''\n",
    "    \n",
    "    print 'Reference Dataset'\n",
    "    print refFile\n",
    "    r_train_columns_renamed = False\n",
    "    r_eval_columns_renamed = False\n",
    "    %Rpush refFile r_train_columns_renamed r_eval_columns_renamed\n",
    "\n",
    "_layout=widgets.Layout(width='330px')\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=_layout)\n",
    "_kit_drop = widgets.interactive(show_sensors, Source=_kit, layout=_layout)\n",
    "\n",
    "_b_add = widgets.Button(description='Update plot', layout=widgets.Layout(width='100px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "\n",
    "_sensor_drop = widgets.Dropdown(layout=_layout)\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_b_reset_time_t = widgets.Button(description='Reset Training Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_t.on_click(reset_time_t)\n",
    "_b_reset_time_t.src = _kit\n",
    "\n",
    "_b_reset_time_e = widgets.Button(description='Reset Eval Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_e.on_click(reset_time_e)\n",
    "_b_reset_time_e.src = _kit\n",
    "\n",
    "_min_date_training = widgets.Text(description='Start Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_training = widgets.Text(description='End Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_min_date_eval = widgets.Text(description='Start Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_eval = widgets.Text(description='End Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "\n",
    "#_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "#_b_apply_time.on_click(add_sensor)\n",
    "#_b_apply_time.slice_time = True\n",
    "\n",
    "#_b_export = widgets.Button(description='Export to CSV', layout=widgets.Layout(width='150px'))\n",
    "#_b_export.on_click(export_dataFrame)\n",
    "_c_R = widgets.Button(description='Export to R dataframe', layout=widgets.Layout(width='150px'))\n",
    "_c_R.on_click(export_dataFrame)\n",
    "#_exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='600px'))\n",
    "#_fileName = widgets.Text(description = 'Name ', layout=widgets.Layout(width='200px'))\n",
    "\n",
    "_button_box = widgets.HBox([_b_add, _b_reset_all])\n",
    "_sensor_box = widgets.HBox([_kit_drop, _sensor_drop , _button_box])\n",
    "_timeT_box = widgets.HBox([_min_date_training,_max_date_training, _b_reset_time_t])\n",
    "_timeE_box = widgets.HBox([_min_date_eval,_max_date_eval, _b_reset_time_e])\n",
    "\n",
    "#_name_box = widgets.HBox([_b_export, _exportPath, _fileName])\n",
    "#_root_box = widgets.VBox([_time_box, _sensor_box, _name_box, _button_box])\n",
    "\n",
    "_refList = widgets.RadioButtons(\n",
    "    options=[k for k in readings.keys()],\n",
    "    #rows=10,\n",
    "    description='Reference Sensor File',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='400px'),\n",
    ")\n",
    "\n",
    "_cR = widgets.Button(description='Export datasets to R', layout=widgets.Layout(width='200px'))\n",
    "_cR.on_click(export_dataFrame)\n",
    "#_prev_dataset = widgets.Button(description='Preview Datasets', layout=widgets.Layout(width='250px'))\n",
    "#_prev_dataset.on_click(preview_datasets)\n",
    "_button_box = widgets.HBox([_refList,_cR])\n",
    "_root_box = widgets.VBox([_sensor_box, _timeT_box, _timeE_box, _button_box])\n",
    "display(widgets.HTML('<br>'))\n",
    "\n",
    "display(widgets.HTML('<h3>Use this box to create R compatible dataframes</h3>'))\n",
    "display(widgets.HTML('1. Select the signals from each source, and hit Preview Slice'))\n",
    "display(widgets.HTML('2. Apply dates from and to export trim'))\n",
    "display(widgets.HTML('3. Hit export to DataFrame'))\n",
    "display(widgets.HTML('<br>'))\n",
    "display(_root_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and timestamp reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#print(r_train_columns_renamed)\n",
    "#print(refFile)\n",
    "\n",
    "convertNames <- function(dataset){\n",
    "    for (i in colnames(dataset)){\n",
    "        #print(i)\n",
    "        index <- gregexpr('.csv.',i, fixed = TRUE)\n",
    "        #print(index)\n",
    "        #print('-')\n",
    "        fileName=substr(i, start=1, stop=index)\n",
    "        fileName=substr(fileName,start=1,stop=nchar(fileName)-1)\n",
    "        channel=substr(i, start=nchar(fileName)+6, stop=nchar(i))\n",
    "        #print('FileName')\n",
    "        #print(fileName)\n",
    "        #print('Channel')\n",
    "        #print(channel)\n",
    "        \n",
    "        if (fileName==refFile){\n",
    "            #fileName=paste('REF',fileName,sep=\"-\")\n",
    "            fileName='REF'\n",
    "            #print('Reference Dataset')\n",
    "        } else {\n",
    "            #fileName='CORR'\n",
    "        }\n",
    "\n",
    "        if (grepl('Carbon.monoxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_CO_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm',channel),nchar(channel)),'KIT_CO_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.sm',channel),nchar(channel)),'KIT_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide',channel),nchar(channel)),'KIT_CO_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_NO2_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm',channel),nchar(channel)),'KIT_NO2_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.sm',channel),nchar(channel)),'KIT_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide',channel),nchar(channel)),'KIT_NO2_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity..',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity..',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature.C',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Battery..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Battery..',channel),nchar(channel)),'BATT_R',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1W.mV',channel),nchar(channel)),'AD_1W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1A.mV',channel),nchar(channel)),'AD_1A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2W.mV',channel),nchar(channel)),'AD_2W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2A.mV',channel),nchar(channel)),'AD_2A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3W.mV',channel),nchar(channel)),'AD_3W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1cal.ppm',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2cal.ppm',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3cal.ppm',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_CO_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_CO_SM',channel),nchar(channel)),'AD_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.CO',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.CO',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_NO2_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_NO2_smooth',channel),nchar(channel)),'AD_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.NO2',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.NO2',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.OX',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.OX',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        }\n",
    "\n",
    "        #print(newName)\n",
    "        #name[i]<-newName\n",
    "        #colnames(r_train_dataframe) <- paste(fileName,\"-\",channel)\n",
    "        colnames(dataset)[colnames(dataset) == i] <- newName\n",
    "        #print('----')\n",
    "    }\n",
    "    return(dataset)\n",
    "}\n",
    "\n",
    "if (!r_train_columns_renamed){\n",
    "    print('Time format for Training Dataset')\n",
    "    r_train_dataframe = read.zoo(r_train_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_train_dataframe)[1])\n",
    "\n",
    "    r_train_dataframe=convertNames(r_train_dataframe)\n",
    "    r_train_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "if (!r_eval_columns_renamed){\n",
    "    print('Time format for Evaluation Dataset')\n",
    "    r_eval_dataframe = read.zoo(r_eval_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_eval_dataframe)[1])\n",
    "    r_eval_dataframe=convertNames(r_eval_dataframe)\n",
    "    r_eval_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "print('Renamed Training Dataset Columns')\n",
    "print(colnames(r_train_dataframe))\n",
    "print('Renamed Eval Dataset Columns')\n",
    "print(colnames(r_eval_dataframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 600\n",
    "options(warn=-1)\n",
    "fileNamePairs = 'KIT_1_4574_MICS'\n",
    "\n",
    "p_train<-ggpairs(data=r_train_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4,6,8), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Training Dataset\")\n",
    "pp_train<-p_train+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_train)\n",
    "\n",
    "#pp_train_plotly<-ggplotly(pp_train)\n",
    "#print(pp_train_plotly)\n",
    "\n",
    "p_eval<-ggpairs(data=r_eval_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Eval Dataset\")\n",
    "pp_eval<-p_eval+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_eval)\n",
    "\n",
    "#pp_eval_plotly <- ggplotly(pp_eval)\n",
    "#print(pp_eval_plotly)\n",
    "\n",
    "options(warn=0)\n",
    "\n",
    "### Eval DataFrame\n",
    "#pairs(~r_eval_dataframe[,\"REF_KIT_CO_RAW\"]+\n",
    "#      r_eval_dataframe[,\"REF_KIT_NO2_RAW\"]+\n",
    "#      r_eval_dataframe[,paste(fileNamePairs,\"KIT_CO_RAW\",sep=\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 700 -h 700\n",
    "\n",
    "p<-coplot(r_train_dataframe[,\"REF_AD_CO_PPM_SM\"] ~ \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_CO_RAW_SM\"] | \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_H_PRCT\"] + \n",
    "          r_train_dataframe[,\"REF_AD_T_PRCT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod1_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod1.name = \"NO2_MICS_O(1)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod1 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod1.reference= r_train_dataframe[,r_mod1_ref]\n",
    "r_train_dataframe.mod1.reference.name = r_mod1_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod1))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod1)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod1), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod1), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod1)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod2_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod2.name = \"NO2_MICS_O(2)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod2 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM+I(KIT_1_4574_KIT_NO2_RAW_SM^2), \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod2.reference= r_train_dataframe[,r_mod2_ref]\n",
    "r_train_dataframe.mod2.reference.name = r_mod2_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod2))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod2)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod2), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod2), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod2)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod3_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod3.name = \"NO2_MICS_O(1) + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod3 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod3.reference= r_train_dataframe[,r_mod3_ref]\n",
    "r_train_dataframe.mod3.reference.name = r_mod3_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod3))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod3)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod3), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod3), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod3)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod4_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod4.name = \"NO2_MICS_O(1) + T\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod4 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod4.reference= r_train_dataframe[,r_mod4_ref]\n",
    "r_train_dataframe.mod4.reference.name = r_mod4_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod4))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod4)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod4), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod4), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod4)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod5_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod5.name = \"NO2_MICS_O(1) + T + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod5 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C+KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod5.reference= r_train_dataframe[,r_mod5_ref]\n",
    "r_train_dataframe.mod5.reference.name = r_mod5_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod5))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod5)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod5), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod5), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod5)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "More into detail for model diagnostics [here](https://www.statmethods.net/stats/rdiagnostics.html)\n",
    "and [here](https://socialsciences.mcmaster.ca/jfox/Courses/Brazil-2009/index.html)\n",
    "\n",
    "All the plots explanations are [here]( https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output).\n",
    "As a rule of a thumb: if Pr(>|t|) is very small, means the value is significant - P-value < 0.05 is OK.\n",
    "\n",
    "Use **Jarque Bera Test** to check if the regression errors are normally \n",
    "distributed for DW test (assumes that the regression errors are normally distributed). The p-value is the probability of the null hypotesis (which for the Jarque Bera Test is that the distribution is normal)\n",
    "\n",
    "Check for autocorrelation of the residuals, with **DW Test** if the residuals are normally distributed: https://stats.stackexchange.com/questions/14914/how-to-test-the-autocorrelation-of-the-residuals\n",
    "\n",
    "Here more information about the plots. [Link](https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864)\n",
    "\n",
    "General rules:\n",
    "\n",
    "- **Residual vs Fitted**: we want a horizontal red line with homogeneus spread. It's a check for the heterodasticity of the distribution. If the residuals change is correlated with the fitted values or the original, our model assumptions are NOK (see https://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n",
    "- **Scale location**: we want the red line horizontal - same check as above (see https://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107#52107)\n",
    "- **Normal QQ**: You can interpret a qq-plot analytically by considering the values read from the axes compare for a given plotted point. If the data were well described by a normal distribution, the values should be about the same. For example, take the extreme point at the very far left bottom corner: its x value is somewhere past −3, but its y value is only a little past −.2, so it is much further out than it 'should' be. In general, a simple rubric to interpret a qq-plot is that if a given tail twists off counterclockwise from the reference line, there is more data in that tail of your distribution than in a theoretical normal, and if a tail twists off clockwise there is less data in that tail of your distribution than in a theoretical normal. In other words:\n",
    "\n",
    "    - if both tails twist counterclockwise you have heavy tails (leptokurtosis),\n",
    "    - if both tails twist clockwise, you have light tails (platykurtosis),\n",
    "    - if your right tail twists counterclockwise and your left tail twists clockwise, you have - right skew\n",
    "    - if your left tail twists counterclockwise and your right tail twists clockwise, you have left skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Only done for model #3\n",
    "#options(repr.plot.width=6, repr.plot.height=5)\n",
    "## Outlier tests\n",
    "#print('Outlier Tests')\n",
    "#outlierTest(r_train_dataframe.mod1)\n",
    "#leveragePlots(r_train_dataframe.mod1)\n",
    "#qqPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate homoscedasticity\n",
    "## non-constant error variance test\n",
    "#print('Homoscedasticity Tests')\n",
    "#ncvTest(r_train_dataframe.mod1)\n",
    "## plot studentized residuals vs. fitted values \n",
    "#spreadLevelPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Nonlinearity\n",
    "## component + residual plot \n",
    "#print('Nonlinearity Tests')\n",
    "#crPlots(r_train_dataframe.mod1)\n",
    "## Ceres plots \n",
    "#ceresPlots(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Collinearity\n",
    "#print('Nonlinearity Tests')\n",
    "#vif(r_train_dataframe.mod1) # variance inflation factors \n",
    "#sqrt(vif(r_train_dataframe.mod1)) > 2 # problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit Plot\n",
    "\n",
    "Use the cells below to plot model data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Traditional R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "plot(r_train_dataframe.mod1.reference ,col=\"black\")\n",
    "grid(5, 5, lwd = 0.85) # grid only in y-direction\n",
    "points(r_train_dataframe.mod1$index, r_train_dataframe.mod1$fit, col=\"red\", type='b')\n",
    "points(r_train_dataframe.mod2$index, r_train_dataframe.mod2$fit, col=\"green\", type='b')\n",
    "points(r_train_dataframe.mod3$index, r_train_dataframe.mod3$fit, col=\"blue\", type='b')\n",
    "points(r_train_dataframe.mod4$index, r_train_dataframe.mod4$fit, col=\"yellow\", type='b')\n",
    "points(r_train_dataframe.mod5$index, r_train_dataframe.mod5$fit, col=\"gray\", type='b')\n",
    "\n",
    "\n",
    "legend('topright', \n",
    "       legend=c(r_train_dataframe.mod1.reference.name, \n",
    "                r_train_dataframe.mod1.name,\n",
    "                r_train_dataframe.mod2.name, \n",
    "                r_train_dataframe.mod3.name,\n",
    "                r_train_dataframe.mod4.name,                \n",
    "                r_train_dataframe.mod5.name), \n",
    "       col=c(\"black\", \"red\", \"green\", \"blue\", \"yellow\", \"gray\"), lty=1:6, cex=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Plot data fitting with respect to sample\n",
    "p1 <- plot_ly()\n",
    "\n",
    "## Reference\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe), \n",
    "                y = ~r_train_dataframe.mod1.reference, \n",
    "                name = r_train_dataframe.mod1.reference.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 1\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod1), \n",
    "                y = ~r_train_dataframe.mod1$fit,\n",
    "               name = r_train_dataframe.mod1.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 2\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod2), \n",
    "                y = ~r_train_dataframe.mod2$fit,\n",
    "               name = r_train_dataframe.mod2.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 3\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod3), \n",
    "                y = ~r_train_dataframe.mod3$fit,\n",
    "               name = r_train_dataframe.mod3.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 4\n",
    "#p1 <- add_trace(p1, \n",
    "#                x = time(r_train_dataframe.mod4), \n",
    "#                y = ~r_train_dataframe.mod4$fit,\n",
    "#               name = r_train_dataframe.mod4.name)\n",
    "\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages('h2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "485px",
    "left": "63px",
    "top": "107px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
