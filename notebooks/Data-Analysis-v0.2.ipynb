{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Python-Framework\" data-toc-modified-id=\"Python-Framework-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Python Framework</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Libraries-and-dependencies\" data-toc-modified-id=\"Libraries-and-dependencies-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Libraries and dependencies</a></span></li><li><span><a href=\"#Import-packages-and-create-modules\" data-toc-modified-id=\"Import-packages-and-create-modules-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Import packages and create modules</a></span></li></ul></li><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Import</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Calibration-Data\" data-toc-modified-id=\"Load-Calibration-Data-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Load Calibration Data</a></span></li><li><span><a href=\"#Import-Local-File\" data-toc-modified-id=\"Import-Local-File-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Import Local File</a></span></li></ul></li><li><span><a href=\"#Formulas\" data-toc-modified-id=\"Formulas-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Formulas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-formulas\" data-toc-modified-id=\"Useful-formulas-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Useful formulas</a></span></li><li><span><a href=\"#Calculator\" data-toc-modified-id=\"Calculator-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Calculator</a></span></li></ul></li><li><span><a href=\"#AlphaSense-Baseline-Calibration\" data-toc-modified-id=\"AlphaSense-Baseline-Calibration-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>AlphaSense Baseline Calibration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Functions</a></span></li><li><span><a href=\"#Correction\" data-toc-modified-id=\"Correction-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Correction</a></span></li></ul></li><li><span><a href=\"#Data-Export\" data-toc-modified-id=\"Data-Export-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Data Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Local-Data-Export\" data-toc-modified-id=\"Local-Data-Export-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Local Data Export</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-Plots\" data-toc-modified-id=\"Time-Series-Plots-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Time Series Plots</a></span></li><li><span><a href=\"#TODO:-Basic-Sensor-Correlations\" data-toc-modified-id=\"TODO:-Basic-Sensor-Correlations-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>TODO: Basic Sensor Correlations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Full-Seaborn-Correlogram\" data-toc-modified-id=\"Full-Seaborn-Correlogram-1.5.2.1\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>Full Seaborn Correlogram</a></span></li><li><span><a href=\"#Basic-Seaborn-XYPlot\" data-toc-modified-id=\"Basic-Seaborn-XYPlot-1.5.2.2\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>Basic Seaborn XYPlot</a></span></li></ul></li><li><span><a href=\"#TODO:-Anomaly-Detection\" data-toc-modified-id=\"TODO:-Anomaly-Detection-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>TODO: Anomaly Detection</a></span></li></ul></li><li><span><a href=\"#Data-Model\" data-toc-modified-id=\"Data-Model-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Data Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Functionality\" data-toc-modified-id=\"Basic-Functionality-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Basic Functionality</a></span></li><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Feature selection and data training split</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO-Select-variables-here\" data-toc-modified-id=\"TODO-Select-variables-here-1.6.2.1\"><span class=\"toc-item-num\">1.6.2.1&nbsp;&nbsp;</span>TODO Select variables here</a></span></li><li><span><a href=\"#TODO-Preliminary-Checks\" data-toc-modified-id=\"TODO-Preliminary-Checks-1.6.2.2\"><span class=\"toc-item-num\">1.6.2.2&nbsp;&nbsp;</span>TODO Preliminary Checks</a></span></li></ul></li><li><span><a href=\"#TODO-Naive-Linear-Regression\" data-toc-modified-id=\"TODO-Naive-Linear-Regression-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>TODO Naive Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ordinary-Linear-Regression\" data-toc-modified-id=\"Ordinary-Linear-Regression-1.6.3.1\"><span class=\"toc-item-num\">1.6.3.1&nbsp;&nbsp;</span>Ordinary Linear Regression</a></span></li><li><span><a href=\"#Ordinary-Linear-Regression-with-differentiation\" data-toc-modified-id=\"Ordinary-Linear-Regression-with-differentiation-1.6.3.2\"><span class=\"toc-item-num\">1.6.3.2&nbsp;&nbsp;</span>Ordinary Linear Regression with differentiation</a></span></li></ul></li><li><span><a href=\"#TODO-ARIMA(X)-model\" data-toc-modified-id=\"TODO-ARIMA(X)-model-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>TODO ARIMA(X) model</a></span></li></ul></li></ul></li><li><span><a href=\"#R-Framework\" data-toc-modified-id=\"R-Framework-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>R Framework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-environment\" data-toc-modified-id=\"Initialise-environment-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initialise environment</a></span></li><li><span><a href=\"#Install-dependencies\" data-toc-modified-id=\"Install-dependencies-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Install dependencies</a></span></li><li><span><a href=\"#Load-in-R-libraries\" data-toc-modified-id=\"Load-in-R-libraries-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Load in R libraries</a></span></li><li><span><a href=\"#Export-Data-to-R-Dataframe\" data-toc-modified-id=\"Export-Data-to-R-Dataframe-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Export Data to R Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-and-timestamp-reading\" data-toc-modified-id=\"Renaming-and-timestamp-reading-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Renaming and timestamp reading</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pairs-Plot\" data-toc-modified-id=\"Pairs-Plot-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Pairs Plot</a></span></li><li><span><a href=\"#Coplot\" data-toc-modified-id=\"Coplot-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Coplot</a></span></li></ul></li><li><span><a href=\"#Model-Iterations\" data-toc-modified-id=\"Model-Iterations-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Model Iterations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Diagnostics\" data-toc-modified-id=\"Model-Diagnostics-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Model Diagnostics</a></span></li><li><span><a href=\"#Model-Fit-Plot\" data-toc-modified-id=\"Model-Fit-Plot-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Model Fit Plot</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Traditional-R\" data-toc-modified-id=\"Using-Traditional-R-2.6.2.1\"><span class=\"toc-item-num\">2.6.2.1&nbsp;&nbsp;</span>Using Traditional R</a></span></li><li><span><a href=\"#Using-interactive-Plot\" data-toc-modified-id=\"Using-interactive-Plot-2.6.2.2\"><span class=\"toc-item-num\">2.6.2.2&nbsp;&nbsp;</span>Using interactive Plot</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '''\n",
    "    <script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "        if (code_show){\n",
    "            $('div.input').hide();\n",
    "        } else {\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    \n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Libraries and dependencies\n",
    "Run this cell to install all necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install pytz==2017.2 fileupload==0.1.2 ipywidgets==6.0.0 pandas==0.20.1 numpy==1.12.1 matplotlib==2.0.2 seaborn==0.8.0\n",
    "! jupyter nbextension install --py fileupload \n",
    "! jupyter nbextension enable --py fileupload\n",
    "! jupyter nbextension install --py widgetsnbextension \n",
    "! jupyter nbextension enable --py widgetsnbextension\n",
    "! jupyter nbextension install https://rawgit.com/jfbercher/small_nbextensions/master/toc2.zip  --user\n",
    "#! jupyter nbextension install --py toc2/main \n",
    "! jupyter nbextension enable toc2/main\n",
    "! pip install cufflinks --upgrade\n",
    "! pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import packages and create modules\n",
    "Run this cell to load in all necessary Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, clear_output\n",
    "from plotly.widgets import GraphWidget\n",
    "    \n",
    "import io, pytz, os, time, datetime, fileupload\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks as cf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.figure_factory import create_2d_density\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import plotly.tools as tls\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aux Zero Current</th>\n",
       "      <th>Sensitivity 1</th>\n",
       "      <th>Serial No</th>\n",
       "      <th>Zero Current</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>6.900000e+01</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.658986</td>\n",
       "      <td>-74.043333</td>\n",
       "      <td>1.896450e+08</td>\n",
       "      <td>-10.244783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.354277</td>\n",
       "      <td>471.048491</td>\n",
       "      <td>1.934604e+07</td>\n",
       "      <td>45.344773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-71.900000</td>\n",
       "      <td>-489.690000</td>\n",
       "      <td>1.620313e+08</td>\n",
       "      <td>-99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-21.100000</td>\n",
       "      <td>-433.120000</td>\n",
       "      <td>1.625817e+08</td>\n",
       "      <td>-67.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>-352.000000</td>\n",
       "      <td>2.021604e+08</td>\n",
       "      <td>15.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.230000</td>\n",
       "      <td>568.300000</td>\n",
       "      <td>2.041601e+08</td>\n",
       "      <td>23.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.270000</td>\n",
       "      <td>621.600000</td>\n",
       "      <td>2.045603e+08</td>\n",
       "      <td>34.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Aux Zero Current  Sensitivity 1     Serial No  Zero Current\n",
       "count         69.000000      69.000000  6.900000e+01     69.000000\n",
       "mean          -1.658986     -74.043333  1.896450e+08    -10.244783\n",
       "std           26.354277     471.048491  1.934604e+07     45.344773\n",
       "min          -71.900000    -489.690000  1.620313e+08    -99.000000\n",
       "25%          -21.100000    -433.120000  1.625817e+08    -67.200000\n",
       "50%            8.500000    -352.000000  2.021604e+08     15.400000\n",
       "75%           19.230000     568.300000  2.041601e+08     23.650000\n",
       "max           30.270000     621.600000  2.045603e+08     34.050000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "alpha_calData = pd.read_json('https://raw.githubusercontent.com/fablabbcn/smartcitizen-iscape-data/internal_dev/calData/AlphaSense.json', orient='columns', lines = True)\n",
    "alpha_calData.index = alpha_calData['Serial No']\n",
    "\n",
    "\n",
    "print display(alpha_calData.describe())\n",
    "#print alpha_calData.loc[162581712,:]\n",
    "\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Aux Zero Current  Sensitivity 1 Sensitivity 2  Serial No Target 1  \\\n",
      "Serial No                                                                      \n",
      "162031254            -20.80         568.30            na  162031254       CO   \n",
      "162031257            -18.60         493.10            na  162031257       CO   \n",
      "162031256            -13.90         601.90            na  162031256       CO   \n",
      "162581706            -35.30         581.40            na  162581706       CO   \n",
      "162581707            -46.30         605.00            na  162581707       CO   \n",
      "162581708            -51.10         597.40            na  162581708       CO   \n",
      "162581709            -43.20         577.90            na  162581709       CO   \n",
      "162581710            -33.40         570.00            na  162581710       CO   \n",
      "162581711            -36.30         564.30            na  162581711       CO   \n",
      "162581712            -71.90         598.40            na  162581712       CO   \n",
      "162581713            -46.70         605.50            na  162581713       CO   \n",
      "162581714            -52.00         600.10            na  162581714       CO   \n",
      "162581715            -24.00         591.60            na  162581715       CO   \n",
      "162581716             -7.60         615.10            na  162581716       CO   \n",
      "162581717            -51.70         564.30            na  162581717       CO   \n",
      "162581718            -21.10         564.80            na  162581718       CO   \n",
      "162581720            -36.90         589.70            na  162581720       CO   \n",
      "162581721            -19.20         588.50            na  162581721       CO   \n",
      "162581722            -53.30         582.50            na  162581722       CO   \n",
      "162581723            -30.00         606.40            na  162581723       CO   \n",
      "162581724            -29.30         621.60            na  162581724       CO   \n",
      "162581725            -35.60         577.30            na  162581725       CO   \n",
      "162581726            -30.90         564.00            na  162581726       CO   \n",
      "202560427             15.40        -384.90            na  202560427      NO2   \n",
      "202560428             14.20        -385.90            na  202560428      NO2   \n",
      "202560430             17.70        -383.70            na  202560430      NO2   \n",
      "202160413             11.30        -341.60            na  202160413      NO2   \n",
      "202160404              9.80        -354.70            na  202160404      NO2   \n",
      "202160405              8.50        -361.10            na  202160405      NO2   \n",
      "202160406             13.60        -362.60            na  202160406      NO2   \n",
      "...                     ...            ...           ...        ...      ...   \n",
      "202160417              7.60        -333.90            na  202160417      NO2   \n",
      "202160418             11.30        -347.70            na  202160418      NO2   \n",
      "202160419             -7.60        -326.20            na  202160419      NO2   \n",
      "202160420             10.10        -360.20            na  202160420      NO2   \n",
      "202160421              6.60        -358.20            na  202160421      NO2   \n",
      "202160422             10.70        -356.90            na  202160422      NO2   \n",
      "202160423              4.70        -347.30            na  202160423      NO2   \n",
      "204560315             18.92        -421.58       -471.65  204560315       O3   \n",
      "204560316             19.86        -433.12       -466.29  204560316       O3   \n",
      "204560314             14.50        -446.36       -506.96  204560314       O3   \n",
      "204160142             27.43        -445.44       -499.71  204160142       O3   \n",
      "204160143             27.74        -482.50        -524.3  204160143       O3   \n",
      "204160144             19.86        -464.08       -602.96  204160144       O3   \n",
      "204160146             26.17        -455.28       -524.46  204160146       O3   \n",
      "204160147             22.38        -443.58       -556.46  204160147       O3   \n",
      "204160148             22.07        -438.50       -558.35  204160148       O3   \n",
      "204160149             15.76        -488.09        -527.3  204160149       O3   \n",
      "204160150             24.91        -455.83       -497.82  204160150       O3   \n",
      "204160151             23.65        -481.37       -530.92  204160151       O3   \n",
      "204160152             30.27        -420.44       -569.38  204160152       O3   \n",
      "204160153             26.17        -459.43       -613.68  204160153       O3   \n",
      "204160154             23.01        -470.04       -555.04  204160154       O3   \n",
      "204160155             19.23        -414.98        -516.1  204160155       O3   \n",
      "204160156             22.70        -410.40       -536.44  204160156       O3   \n",
      "204160157             21.44        -489.69       -528.87  204160157       O3   \n",
      "204160158             13.87        -489.16       -539.28  204160158       O3   \n",
      "204160159             21.12        -473.09       -556.77  204160159       O3   \n",
      "204160160             23.96        -423.82       -523.99  204160160       O3   \n",
      "204160162             18.92        -470.36       -554.25  204160162       O3   \n",
      "204160163             20.49        -459.95       -583.41  204160163       O3   \n",
      "\n",
      "          Target 2  Zero Current  \n",
      "Serial No                         \n",
      "162031254       na        -34.00  \n",
      "162031257       na        -69.40  \n",
      "162031256       na        -68.10  \n",
      "162581706       na        -72.80  \n",
      "162581707       na        -56.70  \n",
      "162581708       na        -61.50  \n",
      "162581709       na        -66.80  \n",
      "162581710       na        -82.90  \n",
      "162581711       na        -73.10  \n",
      "162581712       na        -90.20  \n",
      "162581713       na        -94.90  \n",
      "162581714       na        -80.40  \n",
      "162581715       na        -69.70  \n",
      "162581716       na        -67.20  \n",
      "162581717       na        -75.70  \n",
      "162581718       na        -63.70  \n",
      "162581720       na        -99.00  \n",
      "162581721       na        -68.40  \n",
      "162581722       na        -73.10  \n",
      "162581723       na        -78.80  \n",
      "162581724       na        -79.10  \n",
      "162581725       na        -73.10  \n",
      "162581726       na        -67.20  \n",
      "202560427       na         25.90  \n",
      "202560428       na         24.00  \n",
      "202560430       na         31.50  \n",
      "202160413       na         22.40  \n",
      "202160404       na         26.20  \n",
      "202160405       na         10.70  \n",
      "202160406       na         19.90  \n",
      "...            ...           ...  \n",
      "202160417       na         15.10  \n",
      "202160418       na         16.70  \n",
      "202160419       na         19.50  \n",
      "202160420       na         15.40  \n",
      "202160421       na         26.20  \n",
      "202160422       na         10.70  \n",
      "202160423       na         18.60  \n",
      "204560315      NO2         23.65  \n",
      "204560316      NO2         23.33  \n",
      "204560314      NO2         23.01  \n",
      "204160142      NO2         18.92  \n",
      "204160143      NO2         33.10  \n",
      "204160144      NO2         27.11  \n",
      "204160146      NO2         34.05  \n",
      "204160147      NO2         21.75  \n",
      "204160148      NO2         27.74  \n",
      "204160149      NO2         -1.58  \n",
      "204160150      NO2         28.06  \n",
      "204160151      NO2          8.83  \n",
      "204160152      NO2         28.69  \n",
      "204160153      NO2         29.95  \n",
      "204160154      NO2         18.60  \n",
      "204160155      NO2         15.13  \n",
      "204160156      NO2          5.99  \n",
      "204160157      NO2         29.64  \n",
      "204160158      NO2         23.96  \n",
      "204160159      NO2         19.55  \n",
      "204160160      NO2         27.43  \n",
      "204160162      NO2         19.55  \n",
      "204160163      NO2         29.95  \n",
      "\n",
      "[69 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print alpha_calData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = 0\n",
    "end_date = 0\n",
    "openData = list()\n",
    "\n",
    "def _upload():\n",
    "    \n",
    "    _upload_widget = fileupload.FileUploadWidget()\n",
    "    _tz_widget = widgets.Dropdown(options=pytz.common_timezones, value='UTC', description='Timezone: ')\n",
    "    \n",
    "    def _cb(change):\n",
    "        # get file\n",
    "        decoded = io.StringIO(change['owner'].data.decode('utf-8'))\n",
    "        filename = change['owner'].filename \n",
    "        fileData = io.StringIO(change['new'].decode('utf-8'))\n",
    "        df = pd.read_csv(fileData, verbose=True, skiprows=[1]).set_index('Time')\n",
    "          \n",
    "        # prepare dataframe\n",
    "        print df.index\n",
    "        df.index = pd.to_datetime(df.index).tz_localize('UTC').tz_convert(_tz_widget.value)\n",
    "        df.sort_index(inplace=True)\n",
    "        df = df.groupby(pd.TimeGrouper(freq='10Min')).aggregate(np.mean)\n",
    "        df.drop([i for i in df.columns if 'Unnamed' in i], axis=1, inplace=True)\n",
    "        \n",
    "        readings[filename] = df[df.index > '2001-01-01T00:00:01Z']\n",
    "        if start_date > 0: readings[filename] = df[df.index > start_date]\n",
    "        if end_date > 0: readings[filename] = df[df.index < end_date]\n",
    "        listFiles(filename)\n",
    "    \n",
    "    # widgets\n",
    "    _upload_widget.observe(_cb, names='data')\n",
    "    _hb = widgets.HBox([_upload_widget, _tz_widget, widgets.HTML(' ')])\n",
    "    display(_hb)\n",
    "\n",
    "def delFile(b):\n",
    "    clear_output()\n",
    "    for d in list(b.hbl.children): d.close()\n",
    "    readings.pop(b.f)\n",
    "\n",
    "def describeFile(b):\n",
    "    clear_output()\n",
    "    display(readings[b.f].describe())\n",
    "    \n",
    "def exportFile(b):\n",
    "    export_dir = 'exports'\n",
    "    if not os.path.exists(export_dir): os.mkdir(export_dir)\n",
    "    savePath = os.path.join(export_dir, b.f+'_clean_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%dT%H%M%S')\n",
    "+'.csv')\n",
    "    if not os.path.exists(savePath):\n",
    "        readings[b.f].to_csv(savePath, sep=\",\")\n",
    "        display(FileLink(savePath))\n",
    "    else:\n",
    "        display(widgets.HTML(' File Already exists!'))\n",
    "    \n",
    "def listFiles(filename):\n",
    "#     clear_output()\n",
    "    temp = list(fileList.children)\n",
    "    cb = widgets.Button(icon='close',layout=widgets.Layout(width='30px'))\n",
    "    cb.on_click(delFile)\n",
    "    cb.f = filename\n",
    "    eb = widgets.Button(description='Export processed CSV', layout=widgets.Layout(width='180px'))\n",
    "    eb.on_click(exportFile)\n",
    "    eb.f = filename\n",
    "    sb = widgets.Button(description='describe', layout=widgets.Layout(width='80px'))\n",
    "    sb.on_click(describeFile)\n",
    "    sb.f = filename  \n",
    "    hbl = widgets.HBox([cb, widgets.HTML(' <b>'+filename+'</b> \\t'), sb, eb])\n",
    "    cb.hbl = hbl\n",
    "    temp.append(hbl)\n",
    "    fileList.children = temp\n",
    "\n",
    "readings = {}\n",
    "display(widgets.HTML('<hr><h3>Select CSV files (remember to change the timezone!)</h3>'))\n",
    "_upload()\n",
    "fileList = widgets.VBox([widgets.HTML('<hr>')])\n",
    "display(fileList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful formulas\n",
    "\n",
    "There are formulas for calculating:\n",
    "- *MICS* = Poly(R, H, T) - **MICS_FORMULA**\n",
    "- *Alphasense* = f(Curr, Sens, Zero) - **AD_FORMULA**\n",
    "- *Smoothing* = f(Signal, Window) - **SMOOTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMOOTH(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def AD_FORMULA(WE, AE, SensorType, I0WE, I0AE, SENSITIVITY1, SENSITIVITY2, AUX, UNIT1, UNIT2, X1W, X2W, X1A, X2A):\n",
    "    \n",
    "    \n",
    "    MW,BW = LINE_COEFF(X1W, X2W, -1400, 1399)\n",
    "    MA,BA = LINE_COEFF(X1A, X2A, -1400, 1399)\n",
    "    \n",
    "    FACTORWE = LINE(WE,MW,BW) / WE\n",
    "    FACTORAE = LINE(AE,MA,BA) / AE\n",
    "    \n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.plot(FACTORWE)\n",
    "    # plt.plot(FACTORAE)\n",
    "\n",
    "    if SensorType == 1 or 2:\n",
    "        # CO OR NO2\n",
    "        # CO: BOARD 5 AD_FORMULA(A, B, 1, -69.4, -18.6, 493.1, 0, C, \"ppm\", \"\", -220.7, 220.42, -220.45, 220.09)\n",
    "        # NO2: BOARD U AD_FORMULA(A, B, 1, 31.5, 17.7, -383.7, 0, C, \"ppb\", \"\", -220.45, 220.19, -219.98, 219.61)\n",
    "        # NO2: BOARD 5 AD_FORMULA(A, B, 1, 24.0, 14.2, -385.9, 0, C, \"ppb\", \"\", -220.69, 220.58, -220.39, 220.17)\n",
    "        result = (FACTORWE*WE - I0WE/I0AE*(FACTORAE*AE))/abs(SENSITIVITY1)\n",
    "        \n",
    "        if UNIT1 == \"ppb\":\n",
    "            result = result*1000\n",
    "    \n",
    "    if SensorType== 3:\n",
    "        # O3\n",
    "        # BOARD U AD_FORMULA(A, B, 3, 1, 23.65, 18.92, -421.58, -471.6497, C, \"ppb\", \"ppb\", -220.69, 220.58, -220.39, 220.17)\n",
    "        # BOARD 5 AD_FORMULA(A, B, 3, 1, 23.33, 19.86, -433.12, -506.96, C, \"ppb\", \"ppb\", -220.69, 220.58, -220.39, 220.17)\n",
    "    \n",
    "        if UNIT2 ==\"ppb\":\n",
    "            result = (FACTORWE*WE - I0WE/I0AE*(FACTORAE*AE) - AUX*SENSITIVITY2/1000)/SENSITIVITY1\n",
    "        if UNIT2 ==\"ppm\":\n",
    "            result = (FACTORWE*WE - I0WE/I0AE*(FACTORAE*AE) - AUX*SENSITIVITY2)/SENSITIVITY1\n",
    "        if UNIT1 == \"ppb\":\n",
    "            result = result*1000\n",
    "\n",
    "    return result\n",
    "\n",
    "def LINE_COEFF(X1,X2,Y1,Y2):\n",
    "    a = float(Y2-Y1)/(X2-X1)\n",
    "    b = Y1-a*X1\n",
    "    return a,b\n",
    "\n",
    "def LINE(x,a,b):\n",
    "    y = [i*a+b for i in x]\n",
    "    return y\n",
    "\n",
    "def MICS_FORMULA(Sensor1Type, Sensor1, Sensor2Type, Sensor2, Sensor3Type, Sensor3, Intercept, B, C, D, E, F, G):\n",
    "    SensorType = [Sensor1Type, Sensor2Type, Sensor3Type]\n",
    "    Sensor = [Sensor1, Sensor2, Sensor3]\n",
    "    Sens = Sensor\n",
    "    for i in range(3):\n",
    "        if SensorType[i] == \"Inverse\":\n",
    "            Sens[i] = 1/Sensor[i]\n",
    "    \n",
    "    result =  Intercept + B*Sens[0] +C*Sens[0]*Sens[0] + D*Sens[1] + E*Sens[1]*Sens[1] + F*Sens[2] + G*Sens[2]*Sens[2]\n",
    "\n",
    "    # MICS_Formula(\"Inverse\", A, \"Direct\", B, \"Direct\", C, -1.615e-01 , 210.08064516, -10236.73257, 0, 0, 0, 0)\n",
    "    return result\n",
    "\n",
    "def greater(y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]>val): result[i]=True\n",
    "        else: result [i]=False        \n",
    "    return result\n",
    "\n",
    "def greaterequal(y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]<val): result[i] = False\n",
    "        else: result [i] = True\n",
    "    return result\n",
    "\n",
    "def lower (y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]<val): result[i]=True\n",
    "        else: result [i]=False        \n",
    "    return result\n",
    "\n",
    "def lowerequal(y, val):\n",
    "    result = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]>val): result[i] = False\n",
    "        else: result [i] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator\n",
    "Input your formulas into this cell for analysis in the plots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = readings[selected[0]].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = readings[selected[0]].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(readings[s].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "        \n",
    "display(widgets.HTML('<hr><h4>Select the Files for your formulas to apply</h4>'))\n",
    "\n",
    "selected=tuple()\n",
    "def selectedFilesChannels(x):\n",
    "    global selected\n",
    "    selected = list(x)\n",
    "    commonChannels(selected)\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    clear_output()\n",
    "    A = _Aterm.value\n",
    "    B = _Bterm.value\n",
    "    C = _Cterm.value\n",
    "    D = _Dterm.value\n",
    "    Name = _formulaName.value\n",
    "    for s in list(selected):\n",
    "        result = functionFormula(s,A,B,C,D)\n",
    "        readings[s][Name] = result\n",
    "    print \"Formula Added!\"\n",
    "\n",
    "def functionFormula(s, Aname, Bname, Cname, Dname): \n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(readings[s].loc[:,(Aname,)],readings[s].loc[:,(Bname,)],left_index=True, right_index=True), readings[s].loc[:,(Cname,)], left_index=True, right_index=True),readings[s].loc[:,(Dname,)],left_index=True, right_index=True)\n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "            \n",
    "# Nice formula to check increasing or decreasing signals: greater(np.gradient(smooth(A,10)),0)\n",
    "\n",
    "layout = widgets.Layout(width='400px')\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "interact(selectedFilesChannels,x = widgets.SelectMultiple(options=readings.keys(), description=' ', selected_labels = selected,layout=widgets.Layout(width='700px')))\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "display(widgets.HTML('<h4>Select the terms you wish to use in your formula </h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaSense Baseline Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AlphaDelta PCB factor\n",
    "factorPCB = 6.36\n",
    "\n",
    "# Background Concentration (model assumption) - (from Masbit et al.)\n",
    "backgroundConc = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "import plotly as ply                             # even more plots\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from scipy.stats.stats import linregress   \n",
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "from dateutil import relativedelta\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "\n",
    "def ExtractBaseline(_data, _delta):\n",
    "    '''\n",
    "        Input:\n",
    "            _data: dataframe containing signal to be baselined and index\n",
    "            _delta : float for delta time (N periods)\n",
    "        Output:\n",
    "            result: vector containing baselined values\n",
    "    ''' \n",
    "    \n",
    "    result = np.zeros(len(_data))\n",
    "    name = []\n",
    "    for n in range(0, len(_data)):\n",
    "        minIndex = max(n-_delta,0)\n",
    "        maxIndex = min(n+_delta, len(_data))\n",
    "        \n",
    "        chunk = _data.values[minIndex:maxIndex]\n",
    "        result[n] = min(chunk)\n",
    "\n",
    "    return result\n",
    "\n",
    "def findMax(_listF):\n",
    "    '''\n",
    "        Input: list to obtain maximum value\n",
    "        Output: value and index of maximum in the list\n",
    "    '''\n",
    "    \n",
    "    valMax=max(_listF)\n",
    "    indexMax = _listF.index(max(_listF))\n",
    "    # print 'Max Value found at index: {}, with value: {}'.format(indexMax, valMax)\n",
    "    \n",
    "    return valMax, indexMax\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def exponential_func(x, a, b, c):\n",
    "     return a * np.exp(b * x) + c\n",
    "\n",
    "def createBaselines(_dataBaseline, _dataCorr, _numberDeltas, _type_regress = 'linear', _plots = False, _verbose = False):\n",
    "    '''\n",
    "        Input:\n",
    "            _dataBaseline: dataframe containing signal and index to be baselined\n",
    "            _dataCorr: baseline data for regression\n",
    "            _type_regress= 'linear', 'exponential', 'best' (based on p_value of both)\n",
    "            _numberDeltas : vector of floats for deltas (N periods)\n",
    "            _plots:  display plots or not\n",
    "            _verbose: print info or not\n",
    "            _type_regress: regression type (linear, exp, ... )\n",
    "        Output:\n",
    "            baseline: pandas dataframe baseline\n",
    "        TODO:\n",
    "            implement other types of regression\n",
    "    '''\n",
    "    \n",
    "    resultData = _dataBaseline.copy()\n",
    "    vectorCorr = _dataCorr.values\n",
    "\n",
    "    name = resultData.name\n",
    "    pearsons =[]\n",
    "    \n",
    "    for delta in _numberDeltas:\n",
    "        resultData[(name +'_' +str(delta))] = ExtractBaseline(_dataBaseline, delta)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(np.transpose(resultData[(name +'_' +str(delta))]), np.transpose(vectorCorr))\n",
    "        pearsons.append(r_value)\n",
    "    \n",
    "    ## Find Max in the pearsons\n",
    "    valMax, indexMax = findMax(pearsons)\n",
    "        \n",
    "    ## Find regression between _dataCorr\n",
    "    baseline = pd.DataFrame(index = _dataBaseline.index)\n",
    "    if _type_regress == 'linear':\n",
    "        ## Fit with y = A + Bx\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(np.transpose(vectorCorr),resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        baseline[(name + '_' + 'baseline_' +  _type_regress)] = intercept + slope*vectorCorr\n",
    "    elif _type_regress == 'exponential':\n",
    "        ## Fit with y = Ae^(Bx) -> logy = logA + Bx\n",
    "        logy = np.log(resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(np.transpose(vectorCorr), logy)\n",
    "        baseline[(name + '_' + 'baseline_' +  _type_regress)] = exponential_func(np.transpose(vectorCorr), np.exp(intercept), slope, 0)\n",
    "    elif _type_regress == 'best':\n",
    "        ## Find linear r_value\n",
    "        slope_lin, intercept_lin, r_value_lin, p_value_lin, std_err_lin = linregress(np.transpose(vectorCorr),resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        \n",
    "        ## Find Exponential r_value\n",
    "        logy = np.log(resultData[(name + '_'+str(_numberDeltas[indexMax]))])\n",
    "        slope_exp, intercept_exp, r_value_exp, p_value_exp, std_err_exp = linregress(np.transpose(vectorCorr), logy)\n",
    "        \n",
    "        ## Pick which one is best\n",
    "        if r_value_lin > r_value_exp:\n",
    "            if _verbose:\n",
    "                print 'Using linear regression'\n",
    "            baseline[(name + '_' + 'baseline_' +  _type_regress)] = intercept_lin + slope_lin*vectorCorr\n",
    "        else:\n",
    "            if _verbose:\n",
    "                print 'Using exponential regression'\n",
    "            baseline[(name + '_' + 'baseline_' +  _type_regress)] = exponential_func(np.transpose(vectorCorr), np.exp(intercept_exp), slope_exp, 0)\n",
    "            \n",
    "    if _plots == True:\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "            \n",
    "            ax1.plot(_dataCorr.values, resultData[(name + '_'+str(_numberDeltas[indexMax]))], label = 'Baseline', linestyle='-', linewidth=0, marker='o')\n",
    "            ax1.plot(_dataCorr.values, baseline[(name + '_' + 'baseline_' +  _type_regress)] , label = 'Regressed value', linestyle='-', linewidth=1, marker=None)\n",
    "            legend = ax1.legend(loc='best')\n",
    "            ax1.set_xlabel(_dataCorr.name)\n",
    "            ax1.set_ylabel('Regression values')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            ax2.plot(_dataBaseline.index, _dataBaseline.values, label = \"Actual\", linestyle=':', linewidth=1, marker=None)\n",
    "            #[ax2.plot(resultData.index, resultData[(name +'_' +str(delta))].values, label=\"Delta {}\".format(delta), marker=None,  linestyle='-', linewidth=1) for delta in _numberDeltas]\n",
    "            ax2.plot(baseline.index, baseline.values, label='Baseline', marker = None)\n",
    "\n",
    "            ax2.axis('tight')\n",
    "            ax2.legend(loc='best')\n",
    "            ax2.set_title(\"Baseline Extraction\")\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            ax22 = ax2.twinx()\n",
    "            ax22.plot(_dataCorr.index, _dataCorr.values, color = 'red', label = _dataCorr.name, linestyle='-', linewidth=1, marker=None)\n",
    "            ax22.set_ylabel(_dataCorr.name, color = 'red')\n",
    "            ax22.tick_params(axis='y', labelcolor='red')\n",
    "            \n",
    "            fig2, ax3 = plt.subplots(figsize=(20,8)) # two axes on figure\n",
    "            ax3.plot(_numberDeltas, pearsons)\n",
    "            ax3.axis('tight')\n",
    "            ax3.set_title(\"R2 vs. Delta\")\n",
    "            ax3.set_xlabel('Delta')\n",
    "            ax3.set_ylabel('R2')\n",
    "            ax3.grid(True)\n",
    "\n",
    "    return baseline, indexMax\n",
    "\n",
    "def decompose(_data, plots = False):\n",
    "    '''\n",
    "            Function to decompose a signal into it's trend and normal variation\n",
    "            Input:\n",
    "                _data: signal to decompose\n",
    "                plots: print plots or not (default False)\n",
    "            Output:\n",
    "                DataDecomp = _data - slope*_data.index\n",
    "                slope, intercept = linear regression coefficients\n",
    "    '''\n",
    "    indexDecomp = np.arange(len(_data))\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(indexDecomp, np.transpose(_data.values))\n",
    "    dataDecomp=pd.DataFrame(index = _data.index)\n",
    "    name = _data.name\n",
    "    result = []\n",
    "    \n",
    "    for n in range(len(_data)):\n",
    "        result.append(float(_data.values[n]-slope*n))\n",
    "    dataDecomp[(name + '_' + '_flat')] = result\n",
    "    \n",
    "    trend = slope*indexDecomp + intercept\n",
    "    if plots == True:\n",
    "        \n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig, ax = plt.subplots(figsize=(20,10))\n",
    "            ax.plot(_data.index, _data.values, label = \"Actual\", marker = None)\n",
    "            ax.plot(_data.index, dataDecomp[(name + '_' +'_flat')], marker = None, label = 'Flattened')\n",
    "            ax.plot(_data.index, trend, label = 'Trend')\n",
    "            ax.legend(loc=\"best\")\n",
    "            ax.axis('tight')\n",
    "            ax.set_title(\"Signal Decomposition - \"+ name)\n",
    "            ax.set_xlabel('Index')\n",
    "            ax.set_ylabel('Signal')\n",
    "            ax.grid(True)\n",
    "            \n",
    "    return dataDecomp, slope, intercept\n",
    "\n",
    "def calculateBaselineDay(_dataFrame, _listNames, _baselined, _baseliner, _type_regress, _trydecomp = False, _plots = False, _verbose = True):\n",
    "    '''\n",
    "        Function to calculate day-based baseline corrections\n",
    "        Input:\n",
    "            _dataFrame: pandas dataframe with datetime index containing 1 day of measurements\n",
    "            _listNames: list containing column names of WE, AE, Temp and Hum\n",
    "            _baselined: channel to calculate the baseline of\n",
    "            _baseliner: channel to use as input for the baselined (baselined = f(baseliner)) \n",
    "            _type_regress: type of regression to perform (linear, exponential, best)\n",
    "            _trydecomp: try trend decomposition (not needed . remove it)\n",
    "            _plot: plot analytics or not\n",
    "            _verbose: print analytics or not\n",
    "        Output:\n",
    "            _data_baseline: dataframe with baseline\n",
    "            _baseline_corr: metadata containing analytics for long term analysis\n",
    "    '''\n",
    "    \n",
    "    ## Un-pack list names\n",
    "    alphaW, alphaA, temp, hum = _listNames\n",
    "    \n",
    "    ## Verify anticorrelation between temperature and humidity\n",
    "    if _plots == True:\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            fig2, (ax3, ax4) = plt.subplots(nrows = 2, ncols = 1,figsize=(20,10))\n",
    "            ax3.scatter(_dataFrame[hum], _dataFrame[temp], marker = 'o', linewidth = 0)\n",
    "            ax3.set_xlabel(_dataFrame[hum].name)\n",
    "            ax3.set_ylabel(_dataFrame[temp].name)\n",
    "            ax3.grid(True)\n",
    "            \n",
    "            colorH = 'red'\n",
    "            colorT = 'blue'\n",
    "            ax4.plot(_dataFrame.index, _dataFrame[hum], c = colorH, label = _dataFrame[hum].name, marker = None)\n",
    "            ax5 = ax4.twinx()\n",
    "            ax5.plot(_dataFrame.index, _dataFrame[temp], c = colorT, label = _dataFrame[temp].name, marker = None)\n",
    "            ax4.tick_params(axis='y', labelcolor=colorH)\n",
    "            ax5.tick_params(axis='y', labelcolor=colorT)\n",
    "            ax4.set_xlabel('Time')\n",
    "            ax4.set_ylabel(_dataFrame[temp].name, color = colorH)\n",
    "            ax5.set_ylabel(_dataFrame[hum].name, color = colorT)\n",
    "            ax4.grid(True)\n",
    "\n",
    "    ## Correlation between temperature and working electrode raw\n",
    "    slopenDC, interceptnDC, r_valuenDC, p_valuenDC, std_errnDC = linregress(np.transpose(_dataFrame[_baseliner].values), np.transpose(_dataFrame[_baselined].values))\n",
    "   \n",
    "    ## Create Baselines\n",
    "    ## TODO Automate this\n",
    "    deltas = np.arange(1,70,1)\n",
    "    data_baseline, indexMax = createBaselines(_dataFrame[_baselined], _dataFrame[_baseliner], deltas, _type_regress, _plots, _verbose)\n",
    "\n",
    "    if _trydecomp == True:\n",
    "        dataDecomp = pd.DataFrame(index = _dataFrame.index)\n",
    "        # Decompose Trend - Check if decomposition helps at all\n",
    "        dataDecomp[alphaW], dataWSlope, dataWIntercept = decompose(_dataFrame[alphaW], _plots)\n",
    "        dataDecomp[alphaA], dataASlope, dataAIntercept = decompose(_dataFrame[alphaA], _plots)\n",
    "        dataDecomp[temp], dataCorrSlope, dataCorrIntercept = decompose(_dataFrame[temp], _plots)\n",
    "\n",
    "        slopeDC, interceptDC, r_valueDC, p_valueDC, std_errDC = linregress(np.transpose(dataDecomp[_baseliner]), np.transpose(dataDecomp[_baselined]))\n",
    "        data_baselineDecomp, indexMaxDecomp = createBaselines(dataDecomp[_baselined], dataDecomp[_baseliner], deltas, _type_regress, _plots)\n",
    "        slopeBADecomp, interceptBADecomp, r_valueBADecomp, p_valuenBADecomp, std_errnBADecomp = linregress(np.transpose(data_baseline.values), np.transpose(_dataFrame[alphaA].values))\n",
    "\n",
    "    ## Try to find a correlation with the auxiliary electrode\n",
    "    ## Correlation between Baseline and original auxiliary\n",
    "    slopeBA, interceptBA, r_valueBA, p_valueBA, std_errBA = linregress(np.transpose(data_baseline.values), np.transpose(_dataFrame[alphaA].values))\n",
    "    \n",
    "    deltaAuxBas = data_baseline.values-_dataFrame[alphaA].values\n",
    "    ratioAuxBas = data_baseline.values/_dataFrame[alphaA].values\n",
    "    \n",
    "    deltaAuxBas_avg = np.mean(deltaAuxBas)\n",
    "    ratioAuxBas_avg = np.mean(ratioAuxBas)\n",
    "    \n",
    "    # Pre filter the metadata\n",
    "    if slopeBA > 0 and r_valueBA > 0.3:\n",
    "        valid = True\n",
    "    else:\n",
    "        valid = False\n",
    "    baselineCorr = (slopeBA, interceptBA, r_valueBA, p_valueBA, std_errBA, deltaAuxBas_avg, ratioAuxBas_avg, indexMax, valid)\n",
    "    \n",
    "    if _verbose == True:\n",
    "        \n",
    "        print '-------------------'\n",
    "        print 'Correlation coeffs'\n",
    "        print '-------------------'\n",
    "        print 'Correlation coefficient without decomposed trend: {}'.format(r_valuenDC)\n",
    "        if _trydecomp == True:\n",
    "            print 'Correlation coefficient with decomposed trend: {}'.format(r_valueDC)\n",
    "        \n",
    "            print '-------------------'\n",
    "            print 'Slopes'\n",
    "            print '-------------------'\n",
    "            print 'Slope Working Electrode: {} \\n Slope Aux Electrode: {} \\n Slope Temperature : {} \\t Ratio WE/T: {}'.format(dataWSlope, dataASlope, dataCorrSlope, dataWSlope/dataCorrSlope)\n",
    "    \n",
    "        print '-------------------'\n",
    "        print 'Auxiliary Electrode'\n",
    "        print '-------------------'\n",
    "        print 'Correlation coefficient of Baseline and Original auxiliary: {}'.format(r_valueBA)\n",
    "        print 'Baseline Correlation Slope: {} \\t Intercept: {}'.format(slopeBA, interceptBA)\n",
    "        if _trydecomp == True:\n",
    "            print 'Correlation coefficient of Baseline and Original auxiliary with Decomp: {}'.format(r_valueBADecomp)\n",
    "            print 'Baseline Correlation Slope: {} \\t Intercept with Decomp: {}'.format(slopeBADecomp, interceptBADecomp)\n",
    "        \n",
    "        print 'Average Delta: {} \\t Average Ratio: {}'.format(deltaAuxBas_avg, ratioAuxBas_avg)\n",
    "\n",
    "    if _plots == True:\n",
    "        with plt.style.context('seaborn-white'):\n",
    "            \n",
    "            if _trydecomp == False:\n",
    "                fig2, ax3 = plt.subplots(figsize=(20,8))\n",
    "            else: \n",
    "                fig2, (ax3, ax4) = plt.subplots(nrows = 1, ncols = 2, figsize=(20,8))\n",
    "                ax4.plot(data_baselineDecomp.index, data_baselineDecomp.values, label='Baseline', marker = None)\n",
    "                ax4.plot(dataDecomp.index, dataDecomp[alphaW], label = 'Working Decomp', marker = None)\n",
    "                ax4.plot(dataDecomp.index, dataDecomp[alphaA], label = 'Auxiliary Decomp', marker = None)\n",
    "                ax4.legend(loc=\"best\")\n",
    "                ax4.axis('tight')\n",
    "                ax4.set_title(\"Baseline Compensated\")\n",
    "                ax4.set(xlabel='Time', ylabel='Ouput-mV')\n",
    "                ax4.grid(True)\n",
    "                ax4.set_ylim(min(min(dataDecomp[temp]),min(data_baselineDecomp.values)) -5,max(max(dataDecomp[temp]),max(data_baselineDecomp.values))+5)\n",
    "                \n",
    "                ax6 = ax4.twinx()\n",
    "                ax6.plot(dataDecomp.index, dataDecomp[temp], label='Temperature Decomp', c = 'red', marker = None)\n",
    "                ax6.tick_params(axis='y', labelcolor ='red')\n",
    "                ax6.set_ylabel('Temperature (degC)', color = 'red')\n",
    "                ax6.set_ylim(min(min(dataDecomp[temp]),min(data_baselineDecomp.values)) -5,max(max(dataDecomp[temp]),max(data_baselineDecomp.values))+5)\n",
    "            \n",
    "            ax3.plot(data_baseline.index, data_baseline.values, label='Baseline', marker = None)\n",
    "            ax3.plot(_dataFrame.index, _dataFrame[alphaW], label='Original Working', marker = None)\n",
    "            ax3.plot(_dataFrame.index, _dataFrame[alphaA], label='Original Auxiliary', marker = None)\n",
    "\n",
    "            ax3.legend(loc=\"best\")\n",
    "            ax3.axis('tight')\n",
    "            ax3.set_title(\"Baseline Not Compensated\")\n",
    "            ax3.set(xlabel='Time', ylabel='Ouput-mV')\n",
    "            ax3.grid(True)\n",
    "            ax3.set_ylim(min(min(_dataFrame[temp]),min(data_baseline.values)) -5,max(max(_dataFrame[temp]),max(data_baseline.values))+5)\n",
    "            \n",
    "            if _trydecomp == True:\n",
    "                ax5 = ax3.twinx()\n",
    "                ax5.plot(dataDecomp.index, dataDecomp[temp], label='Temperature Decomp', c = 'red', marker = None)\n",
    "                ax5.tick_params(axis='y', labelcolor ='red')\n",
    "                ax5.set_ylabel(dataDecomp[temp].name, color = 'red')\n",
    "                ax5.set_ylim(min(min(dataDecomp[temp]),min(data_baselineDecomp.values)) -5,max(max(dataDecomp[temp]),max(data_baselineDecomp.values))+5)\n",
    "                \n",
    "            fig3, ax7 = plt.subplots(figsize=(20,8))\n",
    "            \n",
    "            ax7.plot(_dataFrame[temp], _dataFrame[alphaW], label='W - Raw', marker='o',  linestyle=None, linewidth = 0)\n",
    "            ax7.plot(_dataFrame[temp], _dataFrame[alphaA], label ='A - Raw', marker='v', linewidth=0)\n",
    "            \n",
    "            if _trydecomp == True:\n",
    "                ax7.plot(dataDecomp[temp], dataDecomp[alphaA], label ='A - Trend Decomposed', marker='v', linewidth=0)\n",
    "                ax7.plot(dataDecomp[temp], dataDecomp[alphaW], label = 'W - Trend Decomposed',marker='o', linestyle=None, linewidth = 0)\n",
    "            \n",
    "            ax7.legend(loc=\"best\")\n",
    "            ax7.axis('tight')\n",
    "            ax7.set_title(\"Output vs. Temperature\")\n",
    "            ax7.set(xlabel='Temperature', ylabel='Ouput-mV')\n",
    "            ax7.grid(True)\n",
    "    \n",
    "    return data_baseline, baselineCorr\n",
    "\n",
    "def findDates(_dataframe):\n",
    "    '''\n",
    "        Input: pandas dataframe with datetime index\n",
    "        Output: rounded up min day, floor max day and number of days between the min and max dates\n",
    "    '''\n",
    "    range_days = (_dataframe.index.max()-_dataframe.index.min()).days\n",
    "    min_date_df = _dataframe.index.min().ceil('D')\n",
    "    max_date_df = _dataframe.index.max().floor('D')\n",
    "    \n",
    "    return min_date_df, max_date_df, range_days\n",
    "\n",
    "def calculatePollutant(_dataframe, _pollutant, _sensorID, _baselineType, _refAvail, _dataframeRef, _refName, _listNames, _overlapHours = 0, _type_regress = 'best', _filterExpSmoothing = 0.2, _trydecomp = False, _plotsInter = False, _plotResult = True, _verbose = False):\n",
    "    \n",
    "    ## TODO: read this from DB\n",
    "\n",
    "    Sensitivity = alpha_calData.loc[_sensorID,'Sensitivity 1']\n",
    "    nWA = alpha_calData.loc[_sensorID,'Zero Current']/alpha_calData.loc[_sensorID,'Aux Zero Current']\n",
    "\n",
    "    dataframeResult = dataframe.copy()\n",
    "    alphaW, alphaA, temp, hum = _listNames\n",
    "    \n",
    "    if _baselineType == 'single_temp':\n",
    "        baseliner = temp\n",
    "    elif _baselineType == 'single_hum':\n",
    "        baseliner = hum\n",
    "    elif _baselineType == 'single_aux':\n",
    "        baseliner = alphaA\n",
    "    baselined = alphaW\n",
    "    \n",
    "    \n",
    "    if _pollutant == 'NO2':\n",
    "        molecularWeight = 46\n",
    "    elif _pollutant == 'CO':\n",
    "        molecularWeight = 28\n",
    "    elif _pollutant == 'O3':\n",
    "        molecularWeight = 48\n",
    "    else:\n",
    "        print 'Not Supported pollutant'\n",
    "        return\n",
    "    \n",
    "    # Temporary for comparison (just name it _pollutant in the future)\n",
    "    pollutant_column = (_pollutant + '_' + _baselineType)\n",
    "        \n",
    "    ## Find min, max and range of days\n",
    "    min_date_df, max_date_df, range_days = findDates(_dataframe)\n",
    "    print 'Data Range from {} to {} with {} days'.format(min_date_df, max_date_df, range_days)\n",
    "    \n",
    "    ## TODO: add metrics in dataframe calibration\n",
    "    #dataframeCalibration = pd.DataFrame(index = dataframe.index)\n",
    "    #dataframeCalibration = dataframeCalibration.groupby(pd.TimeGrouper(freq='day')).aggregate(np.mean)\n",
    "    \n",
    "    for i in range(range_days):\n",
    "    \n",
    "        min_date_ovl = max(_dataframe.index.min(), (min_date_df + pd.DateOffset(days=i) - pd.DateOffset(hours = _overlapHours)))\n",
    "        max_date_ovl = min(_dataframe.index.max(), (min_date_ovl + pd.DateOffset(days=1) + pd.DateOffset(hours = _overlapHours + relativedelta.relativedelta(min_date_df + pd.DateOffset(days=i),min_date_ovl).hours)))\n",
    "        \n",
    "        min_date_novl = max(min_date_df, (min_date_df + pd.DateOffset(days=i)))\n",
    "        max_date_novl = min(max_date_df, (min_date_novl + pd.DateOffset(days=1)))\n",
    "        \n",
    "        if _verbose:\n",
    "            print '------------------------------------------------------------------'\n",
    "            print 'Calculating day {}, with range: {} \\t to {}'.format(i, min_date_ovl, max_date_ovl)\n",
    "            print '------------------------------------------------------------------'\n",
    "        \n",
    "        dataframeTrim = _dataframe[_dataframe.index > min_date_ovl]\n",
    "        dataframeTrim = dataframeTrim[dataframeTrim.index <= max_date_ovl]\n",
    "        \n",
    "        if i == 0:\n",
    "                CorrParams = list()\n",
    "                \n",
    "        if dataframeTrim.empty:\n",
    "            if _verbose:\n",
    "                print 'No data between these dates'\n",
    "                \n",
    "            nanV =np.ones(9)\n",
    "            nanV.fill(np.nan)\n",
    "            \n",
    "            CorrParams.append(nanV)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dataframeTrim[alphaW + '_baseline'], CorrParamsTrim = calculateBaselineDay(dataframeTrim, _listNames, baselined, baseliner, _type_regress, _trydecomp, _plotsInter, _verbose)\n",
    "            \n",
    "            dataframeTrim = dataframeTrim[dataframeTrim.index > min_date_novl]\n",
    "            dataframeTrim = dataframeTrim[dataframeTrim.index <= max_date_novl]\n",
    "            \n",
    "            if _pollutant == 'CO' or _pollutant == 'NO2':\n",
    "                dataframeTrim[pollutant_column] = backgroundConc + factorPCB*(dataframeTrim[alphaW] - dataframeTrim[alphaW + '_baseline'])/Sensitivity*1000\n",
    "            #elif _pollutant == 'O3':\n",
    "                #dataframeTrim[pollutant_column]\n",
    "            \n",
    "            dataframeResult = dataframeResult.combine_first(dataframeTrim)\n",
    "            CorrParams.append(CorrParamsTrim)\n",
    "    \n",
    "    dataframeResult[pollutant_column + '_filter'] = exponential_smoothing(dataframeResult[pollutant_column].fillna(0), filterExpSmoothing)\n",
    "                \n",
    "    ## Retrieve metadata\n",
    "    labelsCP = ['slopeBA', 'interceptBA', 'r_valueBA', 'p_valueBA', 'std_errBA', 'deltaAuxBas_avg', 'ratioAuxBas_avg', 'indexMax', 'valid']\n",
    "    CorrParamsDF = pd.DataFrame(CorrParams, columns = labelsCP, index = [(min_date_df+ pd.DateOffset(days=i)).strftime('%Y-%m-%d') for i in range(range_days)])\n",
    "    \n",
    "    ## Find average ratio for hole dataset(maybe TODO: day by day)\n",
    "    deltaAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].mean(skipna = True)\n",
    "    deltaAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].std(skipna = True)\n",
    "    ratioAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].mean(skipna = True)\n",
    "    ratioAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].std(skipna = True)\n",
    "    \n",
    "    ## TODO - Make the check for outliers and mark them out\n",
    "    # CorrParamsDF['valid'] = deltaAuxBas_avg-deltaAuxBas_std <= CorrParamsDF['deltaAuxBas_avg'] <= deltaAuxBas_avg-deltaAuxBas_std\n",
    "    # CorrParamsDF['valid'] = ratioAuxBas_avg-ratioAuxBas_std <= CorrParamsDF['ratioAuxBas_avg'] <= ratioAuxBas_avg-ratioAuxBas_std\n",
    "    # \n",
    "    # print CorrParamsDF\n",
    "    # \n",
    "    # deltaAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].mean(skipna = True)\n",
    "    # deltaAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'deltaAuxBas_avg'].std(skipna = True)\n",
    "    # ratioAuxBas_avg = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].mean(skipna = True)\n",
    "    # ratioAuxBas_std = CorrParamsDF.loc[CorrParamsDF['valid'].fillna(False), 'ratioAuxBas_avg'].std(skipna = True)\n",
    "    \n",
    "    if _verbose:\n",
    "        \n",
    "        print '------------------------'\n",
    "        print ' Meta Data'\n",
    "        print '------------------------'\n",
    "        print CorrParamsDF\n",
    "        \n",
    "        print '------------------------'\n",
    "        print 'Average Delta between baseline and auxiliary electrode: {}, and ratio {}:'.format(deltaAuxBas_avg, ratioAuxBas_avg)\n",
    "        print 'Std Dev of Delta between baseline and auxiliary electrode: {}, and ratio {}:'.format(deltaAuxBas_std, ratioAuxBas_std)\n",
    "        print '------------------------'\n",
    "    \n",
    "    if _plotResult:\n",
    "        \n",
    "        fig1 = tls.make_subplots(rows=3, cols=1, shared_xaxes=True, print_grid=False)\n",
    "        \n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaW], 'type': 'scatter', 'line': dict(width = 2), 'name': dataframeResult[alphaW].name}, 1, 1)\n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaA], 'type': 'scatter', 'line': dict(width = 2), 'name': dataframeResult[alphaA].name}, 1, 1)\n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaA] * nWA, 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': 'AuxCor Alphasense'}, 1, 1)\n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[alphaW + '_baseline'], 'type': 'scatter', 'line': dict(width = 2), 'name': 'Baseline'}, 1, 1)\n",
    "        \n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[pollutant_column], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframeResult[pollutant_column].name}, 2, 1)\n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[pollutant_column + '_filter'], 'type': 'scatter', 'name': (dataframeResult[pollutant_column + '_filter'].name)}, 2, 1)\n",
    "        \n",
    "        if _refAvail:\n",
    "            fig1.append_trace({'x': dataframeRef.index, 'y': dataframeRef[_refName]*(24.45/molecularWeight), 'type': 'scatter', 'name': dataframeRef[_refName].name}, 2, 1)\n",
    "            \n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[temp], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframeResult[temp].name}, 3, 1)\n",
    "        fig1.append_trace({'x': dataframeResult.index, 'y': dataframeResult[hum], 'type': 'scatter', 'name': (dataframeResult[hum].name)}, 3, 1)\n",
    "        \n",
    "        fig1['layout'].update(height = 800, legend=dict(x=-.1, y=5), xaxis=dict(title='Time'), title = 'Baseline Correction for {}'.format(_pollutant))\n",
    "                               \n",
    "        ply.offline.iplot(fig1)\n",
    "    \n",
    "    return dataframeResult, CorrParamsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-eae7e3cc10eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#dataframe = readings['MA04_Formulae_2.csv'].dropna()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POST001_Alpha_sen_B.CSV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdataframeRef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CaARPAE_MAData_2_COR.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readings' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: convert to auto\n",
    "#alphaW = 'AlphaDelta 2W-mV'\n",
    "#alphaA = 'AlphaDelta 2A-mV'\n",
    "#temp = 'AlphaDelta Temperature-C'\n",
    "#hum = 'AlphaDelta Humidity-%'\n",
    "alphaW = 'AlphaDelta 2W'\n",
    "alphaA = 'AlphaDelta 2A'\n",
    "temp = 'AlphaDelta Temperature-C'\n",
    "hum = 'AlphaDelta Humidity-%'\n",
    "\n",
    "sensorID = 162581712\n",
    "Sensitivity = alpha_calData.loc[sensorID,'Sensitivity 1']\n",
    "nWA = alpha_calData.loc[sensorID,'Zero Current']/alpha_calData.loc[sensorID,'Aux Zero Current']\n",
    "\n",
    "\n",
    "listNames = (alphaW, alphaA, temp, hum)\n",
    "\n",
    "## Hyperparameters\n",
    "overlapHours = 2 # Overlap in hours for each day (index = [day(i)-overlapHours, day(i+1)+overlapHours])\n",
    "filterExpSmoothing = 0.2\n",
    "\n",
    "## TODO, get this from widgets?\n",
    "## Select data\n",
    "\n",
    "#dataframe = readings['MA04_Formulae_2.csv'].dropna()\n",
    "dataframe = readings['POST001_Alpha_sen_B.CSV'].dropna()\n",
    "\n",
    "dataframeRef = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "refName = 'NO2 ug/m3'\n",
    "\n",
    "dataframe, CorrParams = calculatePollutant(_dataframe = dataframe, \n",
    "                        _pollutant = 'NO2', \n",
    "                        _sensorID = sensorID, \n",
    "                        _baselineType = 'single_temp',\n",
    "                        _refAvail = False, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _refName = refName, \n",
    "                        _listNames = listNames, \n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = False,\n",
    "                        _verbose = False)\n",
    "\n",
    "dataframe, CorrParams = calculatePollutant(_dataframe = dataframe, \n",
    "                        _pollutant = 'NO2', \n",
    "                        _sensorID = '123456', \n",
    "                        _baselineType = 'single_hum', \n",
    "                        _refAvail = False, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _refName = refName, \n",
    "                        _listNames = listNames, \n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = False,\n",
    "                        _verbose = False)\n",
    "\n",
    "dataframe, CorrParams = calculatePollutant(_dataframe = dataframe, \n",
    "                        _pollutant = 'NO2', \n",
    "                        _sensorID = '123456', \n",
    "                        _baselineType = 'single_aux', \n",
    "                        _refAvail = False, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _refName = refName, \n",
    "                        _listNames = listNames, \n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = False,\n",
    "                        _verbose = False)\n",
    "\n",
    "fig1 = tls.make_subplots(rows=4, cols=1, shared_xaxes=True, print_grid=False)\n",
    "        \n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe[alphaW], 'type': 'scatter', 'line': dict(width = 2), 'name': dataframe[alphaW].name}, 1, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe[alphaA], 'type': 'scatter', 'line': dict(width = 2), 'name': dataframe[alphaA].name}, 1, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe[alphaA] * nWA, 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': 'AuxCor Alphasense'}, 1, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe[alphaW + '_baseline'], 'type': 'scatter', 'line': dict(width = 2), 'name': 'Baseline'}, 1, 1)\n",
    "        \n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe['NO2_single_aux'], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframe['NO2_single_aux'].name}, 2, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe['NO2_single_aux' + '_filter'], 'type': 'scatter', 'name': (dataframe['NO2_single_aux' + '_filter'].name)}, 2, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe['NO2_single_temp'], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframe['NO2_single_temp'].name}, 2, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe['NO2_single_temp' + '_filter'], 'type': 'scatter', 'name': (dataframe['NO2_single_temp' + '_filter'].name)}, 2, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe['NO2_single_hum'], 'type': 'scatter', 'line': dict(width = 1, dash = 'dot'), 'name': dataframe['NO2_single_hum'].name}, 2, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe['NO2_single_hum' + '_filter'], 'type': 'scatter', 'name': (dataframe['NO2_single_hum' + '_filter'].name)}, 2, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe[temp], 'type': 'scatter', 'name': dataframe[temp].name, 'yaxis' : 'y2'}, 3, 1)\n",
    "fig1.append_trace({'x': dataframe.index, 'y': dataframe[hum], 'type': 'scatter', 'name': dataframe[hum].name}, 4, 1)         \n",
    "\n",
    "fig1.append_trace({'x': dataframeRef.index, 'y': dataframeRef[refName]*(24.45/molecularWeight), 'type': 'scatter', 'name': dataframeRef[refName].name}, 2, 1)\n",
    "            \n",
    "fig1['layout'].update(height = 1000, \n",
    "                      legend=dict(x=-.1, y=5), \n",
    "                      xaxis=dict(title='Time'), \n",
    "                      yaxis1 = dict(title='Sensor Output - mV'), \n",
    "                      yaxis2 = dict(title='Pollutant - ppm'),\n",
    "                      yaxis3 = dict(title='Temperature - degC'),\n",
    "                      yaxis4 = dict(title='Humidity - %'),\n",
    "                      title = 'Baseline Correction for {}'.format('NO2'))\n",
    "            \n",
    "ply.offline.iplot(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(20,10))\n",
    "    ax2.scatter(dataW_baseline.values, dataA.values, c = dataCorrT, cmap = 'autumn', label='Original Auxiliary', marker = 'o', linewidth = 0) \n",
    "    ax2.scatter(dataW_baseline.values, dataADecomp.values, c = dataCorrT, cmap = 'winter', label='Decomp Auxiliary', marker = 'o', linewidth = 0) \n",
    "    legend = ax2.legend(loc=\"best\")\n",
    "    legend.legendHandles[0].set_color(plt.cm.autumn(.8))\n",
    "    legend.legendHandles[1].set_color(plt.cm.winter(.8))\n",
    "    #legend.legendHandles[2].set_color(plt.cm.Reds(.8))\n",
    "    ax2.axis('tight')\n",
    "    ax2.set_title(\"Auxiliary vs. Baseline\")\n",
    "    ax2.set_xlabel('Baseline -mV')\n",
    "    ax2.set_ylabel('Auxiliary electrode output -mV')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig3, (ax3, ax4) = plt.subplots(nrows = 1, ncols = 2, figsize=(20,8))\n",
    "    ax3.scatter(dataW.index, delta, c = dataCorrT.values, cmap = 'autumn', label = 'Delta Decomp', marker = None)\n",
    "    ax4.scatter(dataW.index, ratio, c = dataCorrT.values, cmap = 'autumn', label = 'Ratio Baseline/Aux Decomp', marker = None)\n",
    "    ax3.legend(loc=\"best\")\n",
    "    ax3.axis('tight')\n",
    "    ax3.set_title(\"Delta Baseline - Auxiliary electrode\")\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_ylabel('Electrode output -mV')\n",
    "    ax3.set_xlim(min_date, max_date)\n",
    "    \n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc=\"best\")\n",
    "    ax4.axis('tight')\n",
    "    ax4.set_title(\"Ratio Baseline / Auxiliary electrode\")\n",
    "    ax4.set_xlabel('Time')\n",
    "    ax4.set_ylabel('Electrode output -mV')\n",
    "    ax4.grid(True)\n",
    "    ax4.set_xlim(min_date, max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Data Export\n",
    "Export data to csv with formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global selected\n",
    "selected = []\n",
    "\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "def exportFile(b):\n",
    "    for i in range(len(selected)):\n",
    "        b.f = selected[i]\n",
    "        exportDir = exportPath.value\n",
    "        if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "        savePath = os.path.join(exportDir, b.f)\n",
    "        if not os.path.exists(savePath):\n",
    "            readings[b.f].to_csv(savePath, sep=\",\")\n",
    "            display(FileLink(savePath))\n",
    "        else:\n",
    "            display(widgets.HTML(' File Already exists!'))\n",
    "\n",
    "display(widgets.HTML('<h3>Export Files</h3>'))\n",
    "exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='700px'))\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportFile)\n",
    "\n",
    "interact(selectedFilesChannels,x = widgets.SelectMultiple(options=readings.keys(), description='Select multiple files', selected_labels = selected,layout=widgets.Layout(width='700px')))\n",
    "exportBox = widgets.HBox([exportPath,eb])\n",
    "display(exportBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = readings[Source].index.min()._short_repr\n",
    "    _max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "    del axisshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        axisshow.append(_axis_drop.value)\n",
    "        \n",
    "    plot_data = readings[toshow[0][0]].loc[:,(toshow[0][1],)]\n",
    "    list_data_primary = []\n",
    "    list_data_secondary = []\n",
    "    list_data_terciary = []\n",
    "    \n",
    "    if b.slice_time:\n",
    "        plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "        plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            plot_data = pd.merge(plot_data, readings[toshow[i][0]].loc[:,(toshow[i][1],)], left_index=True, right_index=True)\n",
    "\n",
    "    print '-------------------------------------'\n",
    "    print ' Medias:\\n'\n",
    "    meanTable = []\n",
    "    for d in toshow:\n",
    "        myMean = ' ' + d[0]  + \"\\t\" + d[1] + \"\\t\"\n",
    "        meanTable.append(myMean)   \n",
    "    res = plot_data.mean()\n",
    "    for i in range(len(meanTable)): print meanTable[i] + '%.2f' % (res[i])\n",
    "    print '-------------------------------------'\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    for i in range(len(plot_data.columns)):\n",
    "        changed.append(toshow[i][0] + ' - '+ plot_data.columns[i])\n",
    "    plot_data.columns = changed\n",
    "    \n",
    "    subplot_rows = 0\n",
    "    if len(toshow) > 0:\n",
    "        for i in range(len(toshow)):\n",
    "            if axisshow[i]=='1': \n",
    "                list_data_primary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,1)\n",
    "            if axisshow[i]=='2': \n",
    "                list_data_secondary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,2)\n",
    "            if axisshow[i]=='3': \n",
    "                list_data_terciary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,3)\n",
    "          \n",
    "        \n",
    "    fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "\n",
    "    #if len(list_data_primary)>0:\n",
    "        #fig1 = plot_data.iplot(kind='scatter', y = list_data_primary, asFigure=True, layout = layout)\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    for i in range(len(list_data_primary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "\n",
    "    for i in range(len(list_data_secondary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "    \n",
    "    for i in range(len(list_data_terciary)):\n",
    "        fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "\n",
    "    if setLimits: \n",
    "        fig1['layout'].update(height = 600,\n",
    "                            legend=dict(x=-.1, y=5) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                          \n",
    "    else:\n",
    "        fig1['layout'].update(height = 600,\n",
    "                              legend=dict(x=-.1, y=5) ,\n",
    "                           xaxis=dict(title='Time'))\n",
    "                           \n",
    "    ply.offline.iplot(fig1)\n",
    "            \n",
    "def reset_time(b):\n",
    "    _min_date.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(show_sensors, Source=_kit, layout=layout)\n",
    "\n",
    "_sensor_drop = widgets.Dropdown(layout=layout)\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, description='Synchronise X axis', disabled=False, layout=widgets.Layout(width='300px'))\n",
    "_min_date = widgets.Text(description='Start date:', layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', layout=widgets.Layout(width='330px'))\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "_b_reset_time.src = _kit\n",
    "\n",
    "_sensor_box = widgets.HBox([_kit_drop, _sensor_drop])\n",
    "_plot_box = widgets.HBox([_axis_drop, _synchroniseXaxis, _b_add , _b_reset_all])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_time_box, _sensor_box, _plot_box])\n",
    "display(_root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Basic Sensor Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Seaborn Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paint(Source):\n",
    "    clear_output()\n",
    "    sns.set(font_scale=1.4)\n",
    "    g = sns.PairGrid(readings.values()[0])\n",
    "    g = g.map(plt.scatter)\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(paint, Source=_kit, layout=layout)\n",
    "display(_kit_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Seaborn XYPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    A_sensors_drop.source = Source\n",
    "    minCropDate.value = readings[Source].index.min()._short_repr\n",
    "    maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensors_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    B_sensors_drop.source = Source\n",
    "    minCropDate.value = readings[Source].index.min()._short_repr\n",
    "    maxCropDate.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def redraw(b):\n",
    "    cropTime = cropTimeCheck.value\n",
    "    doubleAxis = doubleAxisCheck.value\n",
    "    min_date = minCropDate.value\n",
    "    max_date = maxCropDate.value\n",
    "    mergedData = pd.merge(readings[A_kit.value].loc[:,(A_sensors_drop.value,)], readings[B_kit.value].loc[:,(B_sensors_drop.value,)], left_index=True, right_index=True, suffixes=('_'+A_kit.value, '_'+B_kit.value))\n",
    "    clear_output()\n",
    "    \n",
    "    if cropTime:\n",
    "        mergedData = mergedData[mergedData.index > min_date]\n",
    "        mergedData = mergedData[mergedData.index < max_date]\n",
    "        \n",
    "    #jointplot\n",
    "    df = pd.DataFrame()\n",
    "    A = A_sensors_drop.value + '-' + A_kit.value\n",
    "    B = B_sensors_drop.value + '-' + B_kit.value\n",
    "    df[A] = mergedData.iloc[:,0]\n",
    "    df[B] = mergedData.iloc[:,1]\n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", size=12, scatter_kws={\"s\": 80});\n",
    "    print \"data from \" + str(df.index.min()) + \" to \" + str(df.index.max())                      \n",
    "    pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "    print 'Pearson correlation coefficient: ' + str(pearsonCorr)\n",
    "    print 'Coefficient of determination R²: ' + str(pearsonCorr*pearsonCorr)\n",
    "\n",
    "    if cropTime: \n",
    "        \n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "        \n",
    "    trace0 = go.Scatter(x=df[A].index, y=df[A], name = A,line = dict(color='rgb(0,97,255)'))\n",
    "    \n",
    "    if (doubleAxis):\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "    else:\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "    data = [trace0, trace1]\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(figure)\n",
    "\n",
    "  # Delta \n",
    "    delta = df[A]-df[B]\n",
    "    trace0 = go.Scatter(x = df[A].index, y = delta, mode = 'lines')\n",
    "    if cropTime: \n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Delta',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    else:\n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Delta',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    data = [trace0]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(fig)\n",
    "   \n",
    "    # Ratio\n",
    "    ratio = df[A]*1./df[B]\n",
    "    trace0 = go.Scatter(x = df[A].index, y = ratio, mode = 'lines')\n",
    "    if cropTime: \n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Ratio',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    else:\n",
    "        layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Ratio',zerolinecolor='#990000',zerolinewidth=1))\n",
    "    data = [trace0]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(fig)\n",
    "    \n",
    "    # Rolling correlation\n",
    "  # fig = plt.figure(figsize=(15,6))\n",
    "  # roll = mergedData.iloc[:,0].rolling(12).corr(mergedData.iloc[:,1])\n",
    "  # trace0 = go.Scatter(x = df[A].index, y = roll, mode = 'lines')\n",
    "  # if cropTime: \n",
    "  #     layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(range=[min_date, max_date],title='Time'), yaxis=dict(zeroline=True,title='Rolling Average',zerolinecolor='#990000',zerolinewidth=1))\n",
    "  # else:\n",
    "  #     layout = go.Layout(legend=dict(x=-.1, y=1.2) ,xaxis=dict(title='Time'), yaxis=dict(zeroline=True,title='Rolling Correlation',zerolinecolor='#990000',zerolinewidth=1))\n",
    "  # data = [trace0]\n",
    "  # fig = go.Figure(data=data, layout=layout)\n",
    "  # ply.offline.iplot(fig)\n",
    "    \n",
    "    # Interactive Correlation\n",
    "\n",
    "  # t = df[A].index\n",
    "  # x = df[A]\n",
    "  # y = df[B]\n",
    "\n",
    "  # trace1 = go.Scatter(\n",
    "  #     x=x, y=y, mode='markers', name='points',\n",
    "  #     marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n",
    "  # )\n",
    "  # trace2 = go.Histogram2dcontour(\n",
    "  #     x=x, y=y, name=df[A].column, ncontours=20,\n",
    "  #     colorscale='Hot', reversescale=True, showscale=False\n",
    "  # )\n",
    "  # trace3 = go.Histogram(\n",
    "  #     x=x, name=df[A].column,\n",
    "  #     marker=dict(color='rgb(102,0,0)'),\n",
    "  #     yaxis='y2'\n",
    "  # )\n",
    "  # trace4 = go.Histogram(\n",
    "  #     y=y, name=df[B].column, marker=dict(color='rgb(102,0,0)'),\n",
    "  #     xaxis='x2'\n",
    "  # )\n",
    "  # data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "  # layout = go.Layout(\n",
    "  #     showlegend=True,\n",
    "  #     autosize=False,\n",
    "  #     width=600,\n",
    "  #     height=550,\n",
    "  #     xaxis=dict(\n",
    "  #         domain=[0, 0.85],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     ),\n",
    "  #     yaxis=dict(\n",
    "  #         domain=[0, 0.85],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     ),\n",
    "  #     margin=dict(\n",
    "  #         t=50\n",
    "  #     ),\n",
    "  #     hovermode='closest',\n",
    "  #     bargap=0,\n",
    "  #     xaxis2=dict(\n",
    "  #         domain=[0.85, 1],\n",
    "  #     showgrid=True,\n",
    "  #     zeroline=False\n",
    "  #     ),\n",
    "  #     yaxis2=dict(\n",
    "  #         domain=[0.85, 1],\n",
    "  #         showgrid=True,\n",
    "  #         zeroline=False\n",
    "  #     )\n",
    "  # )\n",
    "\n",
    "  # fig = go.Figure(data=data, layout=layout)\n",
    "  # ply.offline.iplot(fig)\n",
    "    \n",
    "if len(readings) < 1: print \"Please load some data first...\"\n",
    "else:\n",
    "    \n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Redraw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    A_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px') ,value=readings.keys()[0])\n",
    "    A_kit_drop = widgets.interactive(show_sensors_A, Source=A_kit, layout=layout)\n",
    "    A_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    show_sensors_A(readings.keys()[0])\n",
    "    \n",
    "    B_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px'), value=readings.keys()[1])\n",
    "    B_kit_drop = widgets.interactive(show_sensors_B, Source= B_kit, layout=layout)\n",
    "    B_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    show_sensors_B(readings.keys()[1])\n",
    "    \n",
    "    draw_box = widgets.HBox([b_redraw, doubleAxisCheck], layout=widgets.Layout(justify_content='space-between'))\n",
    "    kit_box = widgets.HBox([A_kit, widgets.HTML('<h4><< Data source >></h4>') , B_kit], layout=widgets.Layout(justify_content='space-between'))\n",
    "    sensor_box = widgets.HBox([A_sensors_drop, widgets.HTML('<h4><< Sensor selection >></h4>') , B_sensors_drop], layout=widgets.Layout(justify_content='space-between'))\n",
    "    crop_box = widgets.HBox([cropTimeCheck, minCropDate, maxCropDate], layout=widgets.Layout(justify_content='space-between'))\n",
    "    root_box = widgets.VBox([draw_box, kit_box, sensor_box, crop_box])\n",
    "    \n",
    "    display(root_box)\n",
    "    \n",
    "    #redraw(b_redraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Anomaly Detection\n",
    "\n",
    "Check this here https://annals-csis.org/proceedings/2012/pliks/118.pdf.\n",
    "\n",
    "Below we'll use the Holt-Winters function as defined:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n",
    "\n",
    "$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "    \n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    \n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        \n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "                     \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functionality\n",
    "Below, we will define some basic functionality for smoothing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick** demonstration of smoothing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Smoothing tests\n",
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "dataframe.columns\n",
    "\n",
    "plotExponentialSmoothing(dataframe['NOX ug/m3'], [0.1])\n",
    "plotDoubleExponentialSmoothing(dataframe['NOX ug/m3'], alphas=[0.05], betas=[0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and data training split\n",
    "\n",
    "The following code uses cross validation on rolling basis structure:\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "## sklearn Time Series functions and data split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error#, mean_squared_log_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Select variables here \n",
    "\n",
    "Do Interface for variable selection and reference setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "df = dataframe[['NOX ug/m3', 'NO ug/m3']]\n",
    "df.names = [['NOX', ' NO']]\n",
    "print df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Preliminary Checks\n",
    "\n",
    "##### TODO Dicker-fuller test (ADF)\n",
    "\n",
    "Use this test to verify **data stationarity**.\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad_fuller_results = sm.tsa.stattools.adfuller(df['NOX ug/m3'])\n",
    "\n",
    "adf = ad_fuller_results[0]\n",
    "pvalue = ad_fuller_results[1]\n",
    "usedlag = ad_fuller_results[2]\n",
    "nobs = ad_fuller_results[3]\n",
    "print 'ADF- Statistic: {}\\npvalue: {}\\nUsed Lag: {}\\nnobs: {}\\n'.format(adf, pvalue, usedlag, nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO Granger Casuality Test\n",
    "\n",
    "Use this test to determine the casuality of variables (which causes the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sm.tsa.stattools.grangercausalitytests(df[['NOX ug/m3','NO ug/m3']].dropna(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Naive Linear Regression\n",
    "\n",
    "Use this only for basic checking and baseline model setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "\n",
    "df['const']=1\n",
    "print df.head(4)\n",
    "\n",
    "model1=sm.OLS(endog=df['NOX ug/m3'],exog=df[['NO ug/m3','const']])\n",
    "results1=model1.fit()\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Linear Regression with differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['const']=1\n",
    "\n",
    "df['diff_NOX ug/m3']=df['NOX ug/m3'].diff()\n",
    "df['diffNO ug/m3']=df['NO ug/m3'].diff()\n",
    "model2=sm.OLS(endog=df['diff_NOX ug/m3'].dropna(),exog=df[['diffNO ug/m3','const']].dropna())\n",
    "results2=model1.fit()\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO ARIMA(X) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Master = readings['CaARPAE_MAData_2_COR.csv'].dropna()\n",
    "Slave = readings['MA04_Formulae_2.csv'].dropna()\n",
    "max_date = min( Master.index[-1], Slave.index[-1])\n",
    " \n",
    "mergedData = pd.merge(Master.loc[:,], Slave.loc[:,], left_index=True, right_index=True)\n",
    "\n",
    "print mergedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predictors = Master[['UMID %' ,'NO ug/m3', 'VV m/s', 'O3 ug/m3']]\n",
    "\n",
    "Predictors = mergedData[['AD_2_TN_smooth (ppb)', 'AlphaDelta Humidity-%']]\n",
    "to_predict = mergedData['NOX ug/m3']\n",
    "\n",
    "TrainingSize = int(0.8*to_predict.shape[0])\n",
    "TestSize = to_predict.shape[0] - TrainingSize\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(to_predict[:TrainingSize],\n",
    "              exog=Predictors[:TrainingSize],\n",
    "              order= (2,1,3),\n",
    "              enforce_invertibility=False,trend='c')\n",
    "                    \n",
    "res = mod.fit(disp=0)\n",
    "                    \n",
    "frc =res.forecast(TestSize,exog=pd.DataFrame(Predictors)[TrainingSize:])\n",
    "\n",
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(mergedData.index[TrainingSize:], frc, 'r')\n",
    "    plt.plot(mergedData.index[TrainingSize:], to_predict[TrainingSize:], 'k.')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"SARIMAX Model Prediction\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Include R in Python Notebook and test it out below - do not modify the first line (%%R -i ...)\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CRAN mirror for use in this session\n",
    "# Secure CRAN mirrors\n",
    "\n",
    "#1: 0-Cloud [https]                   2: Algeria [https]\n",
    "#3: Australia (Canberra) [https]      4: Australia (Melbourne 1) [https]\n",
    "#5: Australia (Melbourne 2) [https]   6: Australia (Perth) [https]\n",
    "#7: Austria [https]                   8: Belgium (Ghent) [https]\n",
    "#9: Brazil (PR) [https]              10: Brazil (RJ) [https]\n",
    "#11: Brazil (SP 1) [https]            12: Brazil (SP 2) [https]\n",
    "#13: Bulgaria [https]                 14: Chile 1 [https]\n",
    "#15: Chile 2 [https]                  16: China (Guangzhou) [https]\n",
    "#17: China (Lanzhou) [https]          18: China (Shanghai) [https]\n",
    "#19: Colombia (Cali) [https]          20: Czech Republic [https]\n",
    "#21: Denmark [https]                  22: East Asia [https]\n",
    "#23: Ecuador (Cuenca) [https]         24: Ecuador (Quito) [https]\n",
    "#25: Estonia [https]                  26: France (Lyon 1) [https]\n",
    "#27: France (Lyon 2) [https]          28: France (Marseille) [https]\n",
    "#29: France (Montpellier) [https]     30: France (Paris 2) [https]\n",
    "#31: Germany (Erlangen) [https]       32: Germany (Göttingen) [https]\n",
    "#33: Germany (Münster) [https]        34: Greece [https]\n",
    "#35: Iceland [https]                  36: India [https]\n",
    "#37: Indonesia (Jakarta) [https]      38: Ireland [https]\n",
    "#39: Italy (Padua) [https]            40: Japan (Tokyo) [https]\n",
    "#41: Japan (Yonezawa) [https]         42: Korea (Ulsan) [https]\n",
    "#43: Malaysia [https]                 44: Mexico (Mexico City) [https]\n",
    "#45: Norway [https]                   46: Philippines [https]\n",
    "#47: Serbia [https]                   48: Spain (A Coruña) [https]\n",
    "#49: Spain (Madrid) [https]           50: Sweden [https]\n",
    "#51: Switzerland [https]              52: Turkey (Denizli) [https]\n",
    "#53: Turkey (Mersin) [https]          54: UK (Bristol) [https]\n",
    "#55: UK (Cambridge) [https]           56: UK (London 1) [https]\n",
    "#57: USA (CA 1) [https]               58: USA (IA) [https]\n",
    "#59: USA (KS) [https]                 60: USA (MI 1) [https]\n",
    "#61: USA (NY) [https]                 62: USA (OR) [https]\n",
    "#63: USA (TN) [https]                 64: USA (TX 1) [https]\n",
    "#65: Vietnam [https]                  66: (other mirrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "base = rpackages.importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=49) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = [\"ggplot2\",\n",
    "             \"car\",\n",
    "             \"lattice\",\n",
    "             \"dyn\",\n",
    "             \"dynlm\",\n",
    "             \"zoo\",\n",
    "             \"tseries\",\n",
    "             \"lmtest\",\n",
    "             \"xts\",\n",
    "             \"tidyverse\",\n",
    "             \"lubridate\",\n",
    "             \"lme4\",\n",
    "             \"multcomp\",\n",
    "             \"signal\",\n",
    "             \"ggfortify\"]\n",
    "\n",
    "# Selectively install what needs to be install.\n",
    "for x in packnames:\n",
    "    if not rpackages.isinstalled(x):\n",
    "        utils.install_packages(StrVector(x))\n",
    "\n",
    "# import R's \"GlobalEnv\" to evaluate the function\n",
    "from rpy2.robjects import globalenv\n",
    "\n",
    "# ggplot2 = rpackages.importr('ggplot2')\n",
    "# graphics = rpackages.importr('graphics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load in the libraries\n",
    "library(\"ggplot2\")\n",
    "library(\"car\")\n",
    "library(\"lattice\")\n",
    "library(\"dyn\")\n",
    "library(\"dynlm\")\n",
    "library(\"zoo\")\n",
    "library(\"tseries\")\n",
    "library(\"lmtest\")\n",
    "library(\"xts\")\n",
    "library(\"tidyverse\")\n",
    "library(\"lubridate\")\n",
    "library(\"lme4\")\n",
    "library(\"multcomp\")\n",
    "library(\"signal\")\n",
    "library('GGally')\n",
    "library('plotly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to R Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "toshow = []\n",
    "min_date_training = 0\n",
    "max_date_training = 0\n",
    "min_date_eval = 0\n",
    "max_date_eval = 0\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[Source].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date_training.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_training.value = readings[Source].index.max()._short_repr\n",
    "    _min_date_eval.value = readings[Source].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "      \n",
    "    \n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        \n",
    "    global dataframe_export\n",
    "    dataframe_export = readings[toshow[0][0]].loc[:,(toshow[0][1],)]\n",
    "    \n",
    "    #if b.slice_time:\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    #    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #for k in readings.keys():\n",
    "    #    _min_date_training.value = min(_min_date_training.value, readings[k].index.min()._short_repr)\n",
    "    #    _max_date_training.value = max(_max_date_training.value, readings[k].index.max()._short_repr)\n",
    "    #    _min_date_eval.value = min(_min_date_eval.value,readings[k].index.min()._short_repr)\n",
    "    #    _max_date_eval.value = max(_max_date_eval.value,readings[k].index.max()._short_repr)\n",
    "    \n",
    "    dataframe_export = dataframe_export[dataframe_export.index > min(_min_date_training.value,_min_date_eval.value)]\n",
    "    dataframe_export = dataframe_export[dataframe_export.index < max(_max_date_training.value,_max_date_eval.value)]\n",
    "    \n",
    "    #print 'Min Date Training / Eval'\n",
    "    #print _min_date_training.value\n",
    "    #print _min_date_eval.value\n",
    "    #print min(_min_date_training.value,_min_date_eval.value)\n",
    "    #print 'Max Date Training / Eval'\n",
    "    #print _max_date_training.value\n",
    "    #print _max_date_eval.value\n",
    "    #print max(_max_date_training.value,_max_date_eval.value)\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            dataframe_export = pd.merge(dataframe_export, readings[toshow[i][0]].loc[:,(toshow[i][1],)], left_index=True, right_index=True)\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    \n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        changed.append(toshow[i][0] + '-'+ dataframe_export.columns[i])\n",
    "    dataframe_export.columns = changed\n",
    "    \n",
    "    #text=[i  for i in range(len(dataframe_export.columns))]\n",
    "    #for i in range(len(dataframe_export.columns)):\n",
    "    #    item = dataframe_export.columns[i]\n",
    "    #    #print \"data\" + str(i)\n",
    "    #    #print item\n",
    "    #    fileName = item[:item.find('.')]\n",
    "    #    #print fileName\n",
    "    #    channel = item[item.find('-')+1:].split('-')[0]\n",
    "    #    if (len(item[item.find('-')+1:].split('-'))>0):\n",
    "    #        unit = item[item.find('-')+1:].split('-')[1]\n",
    "    #    else:\n",
    "    #        unit = ''\n",
    "    #    #print channel\n",
    "    #    #print unit\n",
    "    #    text[i]='<br>File: '+'{:s}'+str(fileName)+'<br>Channel: '+'{:s}'+str(channel)+\\\n",
    "    #    '<br>Unit: '+'{:s}'+ str(unit)\n",
    "    #    #print text[i]\n",
    "    \n",
    "    fig2 = tls.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    for i in range(len(dataframe_export.columns)):\n",
    "        fig2.append_trace({'x': dataframe_export.index, \n",
    "                          'y': dataframe_export.iloc[:,i], \n",
    "                          'type': 'scatter',\n",
    "                          'name': dataframe_export.columns[i]}, 1, 1)\n",
    "\n",
    "\n",
    "    fig2['layout'].update(\n",
    "        height=800,\n",
    "        showlegend = True,\n",
    "        legend=dict(x=-.1, y=5) ,\n",
    "        xaxis=dict(\n",
    "            rangeslider=dict(),\n",
    "            type='date'\n",
    "        ),\n",
    "        annotations=[dict(\n",
    "                        x=_min_date_training.value,\n",
    "                        y=1,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Training Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(44, 160, 101, 1)')\n",
    "                    ),\n",
    "                     dict(\n",
    "                        x=_min_date_eval.value,\n",
    "                        y=0.95,\n",
    "                        xref='x',\n",
    "                        yref='paper',\n",
    "                        text='Evaluation Dataset',\n",
    "                        showarrow=False,\n",
    "                        xanchor=\"left\",\n",
    "                        font=dict(color= 'rgba(160, 160, 0, 1)')\n",
    "                    )\n",
    "                    ],\n",
    "        shapes=[\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_training.value,\n",
    "                    x1=_max_date_training.value,\n",
    "                    y0=0.95,\n",
    "                    y1=1,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(44, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(44, 160, 101,0.6)'),\n",
    "                    ),\n",
    "                dict(type='rect',\n",
    "                    layer='below',\n",
    "                    x0=_min_date_eval.value,\n",
    "                    x1=_max_date_eval.value,\n",
    "                    y0=0.9,\n",
    "                    y1=0.95,\n",
    "                    yref= \"paper\",\n",
    "                    fillcolor='rgba(160, 160, 0, 0.2)',\n",
    "                    line=dict(color= 'rgba(160, 160, 0, 0.6)')\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #fig1 = dataframe_export.iplot(kind='scatter', asFigure=True, layout = layout, hoverinfo='text')\n",
    "    #ply.offline.iplot(fig1)\n",
    "    \n",
    "    print list(dataframe_export.columns.values.tolist())\n",
    "    \n",
    "    ply.offline.iplot(fig2)\n",
    "\n",
    "def reset_time_t(b):\n",
    "    _min_date_training.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_training.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "    \n",
    "def reset_time_e(b):\n",
    "    _min_date_eval.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date_eval.value = readings[b.src.value].index.max()._short_repr\n",
    "    add_sensor(b)\n",
    "\n",
    "def export_dataFrame(b):\n",
    "    clear_output()\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects import r\n",
    "    \n",
    "    global r_train_dataframe, train_dataframe, r_eval_dataframe, eval_dataframe\n",
    "\n",
    "    train_dataframe = dataframe_export[dataframe_export.index > _min_date_training.value]\n",
    "    train_dataframe = train_dataframe[train_dataframe.index < _max_date_training.value]\n",
    "    train_dataframe.index=train_dataframe.index.to_datetime()\n",
    "    r_train_dataframe = pandas2ri.py2ri(train_dataframe)\n",
    "    \n",
    "    eval_dataframe = dataframe_export[dataframe_export.index > _min_date_eval.value]\n",
    "    eval_dataframe = eval_dataframe[eval_dataframe.index < _max_date_eval.value]\n",
    "    eval_dataframe.index=eval_dataframe.index.to_datetime()\n",
    "    r_eval_dataframe = pandas2ri.py2ri(eval_dataframe)\n",
    "    \n",
    "    %Rpush r_train_dataframe r_eval_dataframe\n",
    "    \n",
    "    print 'Export to R Training dataframe successful with following channels'\n",
    "    %R print(colnames(r_train_dataframe))\n",
    "    min_date_training = _min_date_training.value\n",
    "    max_date_training= _max_date_training.value\n",
    "    \n",
    "    print 'With Date Range'\n",
    "    print min_date_training\n",
    "    print max_date_training\n",
    "    \n",
    "    print ''\n",
    "   \n",
    "    print 'Export to R Evaluation dataframe successful with following channels'\n",
    "    %R print(colnames(r_eval_dataframe))\n",
    "    min_date_eval = str(_min_date_eval.value)\n",
    "    max_date_eval= str(_max_date_eval.value)\n",
    "\n",
    "    print 'With Date Range'\n",
    "    print min_date_eval\n",
    "    print max_date_eval\n",
    "        \n",
    "    refFile = str(_refList.value)\n",
    "    refFile = refFile[:refFile.find('.')]\n",
    "    \n",
    "    print ''\n",
    "    \n",
    "    print 'Reference Dataset'\n",
    "    print refFile\n",
    "    r_train_columns_renamed = False\n",
    "    r_eval_columns_renamed = False\n",
    "    %Rpush refFile r_train_columns_renamed r_eval_columns_renamed\n",
    "\n",
    "_layout=widgets.Layout(width='330px')\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=_layout)\n",
    "_kit_drop = widgets.interactive(show_sensors, Source=_kit, layout=_layout)\n",
    "\n",
    "_b_add = widgets.Button(description='Update plot', layout=widgets.Layout(width='100px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "\n",
    "_sensor_drop = widgets.Dropdown(layout=_layout)\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_b_reset_time_t = widgets.Button(description='Reset Training Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_t.on_click(reset_time_t)\n",
    "_b_reset_time_t.src = _kit\n",
    "\n",
    "_b_reset_time_e = widgets.Button(description='Reset Eval Dates', layout=widgets.Layout(width='200px'))\n",
    "_b_reset_time_e.on_click(reset_time_e)\n",
    "_b_reset_time_e.src = _kit\n",
    "\n",
    "_min_date_training = widgets.Text(description='Start Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_training = widgets.Text(description='End Date Train:', layout=widgets.Layout(width='250px'))\n",
    "_min_date_eval = widgets.Text(description='Start Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "_max_date_eval = widgets.Text(description='End Date Eval:', layout=widgets.Layout(width='250px'))\n",
    "\n",
    "#_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "#_b_apply_time.on_click(add_sensor)\n",
    "#_b_apply_time.slice_time = True\n",
    "\n",
    "#_b_export = widgets.Button(description='Export to CSV', layout=widgets.Layout(width='150px'))\n",
    "#_b_export.on_click(export_dataFrame)\n",
    "_c_R = widgets.Button(description='Export to R dataframe', layout=widgets.Layout(width='150px'))\n",
    "_c_R.on_click(export_dataFrame)\n",
    "#_exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='600px'))\n",
    "#_fileName = widgets.Text(description = 'Name ', layout=widgets.Layout(width='200px'))\n",
    "\n",
    "_button_box = widgets.HBox([_b_add, _b_reset_all])\n",
    "_sensor_box = widgets.HBox([_kit_drop, _sensor_drop , _button_box])\n",
    "_timeT_box = widgets.HBox([_min_date_training,_max_date_training, _b_reset_time_t])\n",
    "_timeE_box = widgets.HBox([_min_date_eval,_max_date_eval, _b_reset_time_e])\n",
    "\n",
    "#_name_box = widgets.HBox([_b_export, _exportPath, _fileName])\n",
    "#_root_box = widgets.VBox([_time_box, _sensor_box, _name_box, _button_box])\n",
    "\n",
    "_refList = widgets.RadioButtons(\n",
    "    options=[k for k in readings.keys()],\n",
    "    #rows=10,\n",
    "    description='Reference Sensor File',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='400px'),\n",
    ")\n",
    "\n",
    "_cR = widgets.Button(description='Export datasets to R', layout=widgets.Layout(width='200px'))\n",
    "_cR.on_click(export_dataFrame)\n",
    "#_prev_dataset = widgets.Button(description='Preview Datasets', layout=widgets.Layout(width='250px'))\n",
    "#_prev_dataset.on_click(preview_datasets)\n",
    "_button_box = widgets.HBox([_refList,_cR])\n",
    "_root_box = widgets.VBox([_sensor_box, _timeT_box, _timeE_box, _button_box])\n",
    "display(widgets.HTML('<br>'))\n",
    "\n",
    "display(widgets.HTML('<h3>Use this box to create R compatible dataframes</h3>'))\n",
    "display(widgets.HTML('1. Select the signals from each source, and hit Preview Slice'))\n",
    "display(widgets.HTML('2. Apply dates from and to export trim'))\n",
    "display(widgets.HTML('3. Hit export to DataFrame'))\n",
    "display(widgets.HTML('<br>'))\n",
    "display(_root_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and timestamp reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#print(r_train_columns_renamed)\n",
    "#print(refFile)\n",
    "\n",
    "convertNames <- function(dataset){\n",
    "    for (i in colnames(dataset)){\n",
    "        #print(i)\n",
    "        index <- gregexpr('.csv.',i, fixed = TRUE)\n",
    "        #print(index)\n",
    "        #print('-')\n",
    "        fileName=substr(i, start=1, stop=index)\n",
    "        fileName=substr(fileName,start=1,stop=nchar(fileName)-1)\n",
    "        channel=substr(i, start=nchar(fileName)+6, stop=nchar(i))\n",
    "        #print('FileName')\n",
    "        #print(fileName)\n",
    "        #print('Channel')\n",
    "        #print(channel)\n",
    "        \n",
    "        if (fileName==refFile){\n",
    "            #fileName=paste('REF',fileName,sep=\"-\")\n",
    "            fileName='REF'\n",
    "            #print('Reference Dataset')\n",
    "        } else {\n",
    "            #fileName='CORR'\n",
    "        }\n",
    "\n",
    "        if (grepl('Carbon.monoxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_CO_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.kOhm.ppm',channel),nchar(channel)),'KIT_CO_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide.sm',channel),nchar(channel)),'KIT_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Carbon.monoxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Carbon.monoxide',channel),nchar(channel)),'KIT_CO_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm.sm',channel),nchar(channel)),'KIT_NO2_RAW_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.kOhm.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.kOhm.ppm',channel),nchar(channel)),'KIT_NO2_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide.sm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide.sm',channel),nchar(channel)),'KIT_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('Nitrogen.dioxide',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Nitrogen.dioxide',channel),nchar(channel)),'KIT_NO2_PPM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity..',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'AD_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity..',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Humidity',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Humidity',channel),nchar(channel)),'KIT_H_PRCT',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature.C',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.Temperature',channel),nchar(channel)),'AD_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature.C',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Temperature',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Temperature.C',channel),nchar(channel)),'KIT_T_C',channel),sep=\"_\")\n",
    "        } else if (grepl('Battery..',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('Battery..',channel),nchar(channel)),'BATT_R',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1W.mV',channel),nchar(channel)),'AD_1W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1A.mV',channel),nchar(channel)),'AD_1A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2W.mV',channel),nchar(channel)),'AD_2W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2A.mV',channel),nchar(channel)),'AD_2A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3W.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3W.mV',channel),nchar(channel)),'AD_3W_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3A.mV',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3A.mV',channel),nchar(channel)),'AD_3A_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.1cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.1cal.ppm',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.2cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.2cal.ppm',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.3cal.ppm',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.3cal.ppm',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_CO_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_CO_SM',channel),nchar(channel)),'AD_CO_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.CO',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.CO',channel),nchar(channel)),'AD_CO_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta_NO2_SM',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta_NO2_smooth',channel),nchar(channel)),'AD_NO2_PPM_SM',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.NO2',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.NO2',channel),nchar(channel)),'AD_NO2_PPM_RAW',channel),sep=\"_\")\n",
    "        } else if (grepl('AlphaDelta.OX',channel)){\n",
    "            newName = paste(fileName,gsub(channel,substr(channel,regexpr('AlphaDelta.OX',channel),nchar(channel)),'AD_O3_PPM_RAW',channel),sep=\"_\")\n",
    "        }\n",
    "\n",
    "        #print(newName)\n",
    "        #name[i]<-newName\n",
    "        #colnames(r_train_dataframe) <- paste(fileName,\"-\",channel)\n",
    "        colnames(dataset)[colnames(dataset) == i] <- newName\n",
    "        #print('----')\n",
    "    }\n",
    "    return(dataset)\n",
    "}\n",
    "\n",
    "if (!r_train_columns_renamed){\n",
    "    print('Time format for Training Dataset')\n",
    "    r_train_dataframe = read.zoo(r_train_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_train_dataframe)[1])\n",
    "\n",
    "    r_train_dataframe=convertNames(r_train_dataframe)\n",
    "    r_train_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "if (!r_eval_columns_renamed){\n",
    "    print('Time format for Evaluation Dataset')\n",
    "    r_eval_dataframe = read.zoo(r_eval_dataframe, index = \"Time\",\n",
    "      format = \"%Y-%m-%d %H:%M:00\", tz = \"GMT+2\")\n",
    "    print(time(r_eval_dataframe)[1])\n",
    "    r_eval_dataframe=convertNames(r_eval_dataframe)\n",
    "    r_eval_columns_renamed = TRUE \n",
    "}\n",
    "\n",
    "print('Renamed Training Dataset Columns')\n",
    "print(colnames(r_train_dataframe))\n",
    "print('Renamed Eval Dataset Columns')\n",
    "print(colnames(r_eval_dataframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 600\n",
    "options(warn=-1)\n",
    "fileNamePairs = 'KIT_1_4574_MICS'\n",
    "\n",
    "p_train<-ggpairs(data=r_train_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4,6,8), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Training Dataset\")\n",
    "pp_train<-p_train+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_train)\n",
    "\n",
    "#pp_train_plotly<-ggplotly(pp_train)\n",
    "#print(pp_train_plotly)\n",
    "\n",
    "p_eval<-ggpairs(data=r_eval_dataframe, # data.frame with variables\n",
    "             columns=c(2,3,4), # columns to plot, default to all.\n",
    "             title=\"Pairs Plot For Eval Dataset\")\n",
    "pp_eval<-p_eval+theme(axis.text=element_text(size=8),\n",
    "        axis.title=element_text(size=6))\n",
    "print(pp_eval)\n",
    "\n",
    "#pp_eval_plotly <- ggplotly(pp_eval)\n",
    "#print(pp_eval_plotly)\n",
    "\n",
    "options(warn=0)\n",
    "\n",
    "### Eval DataFrame\n",
    "#pairs(~r_eval_dataframe[,\"REF_KIT_CO_RAW\"]+\n",
    "#      r_eval_dataframe[,\"REF_KIT_NO2_RAW\"]+\n",
    "#      r_eval_dataframe[,paste(fileNamePairs,\"KIT_CO_RAW\",sep=\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 700 -h 700\n",
    "\n",
    "p<-coplot(r_train_dataframe[,\"REF_AD_CO_PPM_SM\"] ~ \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_CO_RAW_SM\"] | \n",
    "          r_train_dataframe[,\"KIT_1_4574_KIT_H_PRCT\"] + \n",
    "          r_train_dataframe[,\"REF_AD_T_PRCT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(colnames(r_train_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod1_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod1.name = \"NO2_MICS_O(1)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod1 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod1.reference= r_train_dataframe[,r_mod1_ref]\n",
    "r_train_dataframe.mod1.reference.name = r_mod1_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod1))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod1)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod1), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod1), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod1)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod2_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod2.name = \"NO2_MICS_O(2)\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod2 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM+I(KIT_1_4574_KIT_NO2_RAW_SM^2), \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod2.reference= r_train_dataframe[,r_mod2_ref]\n",
    "r_train_dataframe.mod2.reference.name = r_mod2_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod2))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod2)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod2), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod2), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod2)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod3_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod3.name = \"NO2_MICS_O(1) + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod3 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod3.reference= r_train_dataframe[,r_mod3_ref]\n",
    "r_train_dataframe.mod3.reference.name = r_mod3_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod3))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod3)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod3), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod3), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod3)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod4_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod4.name = \"NO2_MICS_O(1) + T\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod4 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod4.reference= r_train_dataframe[,r_mod4_ref]\n",
    "r_train_dataframe.mod4.reference.name = r_mod4_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod4))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod4)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod4), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod4), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod4)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "library(ggfortify)\n",
    "\n",
    "# Model 1 Names - Specify var and model names below\n",
    "r_mod5_ref = \"REF_AD_NO2_PPM_SM\"\n",
    "r_train_dataframe.mod5.name = \"NO2_MICS_O(1) + T + H\"\n",
    "# Model 1 Formulation - Specify model below\n",
    "r_train_dataframe.mod5 = dynlm(REF_AD_NO2_PPM_SM ~ KIT_1_4574_KIT_NO2_RAW_SM + KIT_1_4574_KIT_T_C+KIT_1_4574_KIT_H_PRCT, \n",
    "                               data = r_train_dataframe)\n",
    "\n",
    "# Model Reference\n",
    "r_train_dataframe.mod5.reference= r_train_dataframe[,r_mod5_ref]\n",
    "r_train_dataframe.mod5.reference.name = r_mod5_ref\n",
    "\n",
    "print(summary(r_train_dataframe.mod5))\n",
    "\n",
    "# Use Jarque Bera Test to check if the regression errors are normally \n",
    "# distributed for DW test (assumes that the regression errors are normally distributed)\n",
    "# The p-value is the probability of the null hypotesis \n",
    "# (which for the Jarque Bera Test is that the distribution is normal)\n",
    "print(jarque.bera.test(resid(r_train_dataframe.mod5)))\n",
    "\n",
    "plot(resid(r_train_dataframe.mod5), xlab = \"Date\", main =\"\", type = \"o\")\n",
    "\n",
    "# Check for autocorrelation of the residuals, \n",
    "# with DW test if the residuals are normally distributed\n",
    "print(dwtest((r_train_dataframe.mod5), alternative = \"two.sided\"))\n",
    "\n",
    "par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))\n",
    "autoplot(r_train_dataframe.mod5)\n",
    "# https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "More into detail for model diagnostics [here](https://www.statmethods.net/stats/rdiagnostics.html)\n",
    "and [here](https://socialsciences.mcmaster.ca/jfox/Courses/Brazil-2009/index.html)\n",
    "\n",
    "All the plots explanations are [here]( https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output).\n",
    "As a rule of a thumb: if Pr(>|t|) is very small, means the value is significant - P-value < 0.05 is OK.\n",
    "\n",
    "Use **Jarque Bera Test** to check if the regression errors are normally \n",
    "distributed for DW test (assumes that the regression errors are normally distributed). The p-value is the probability of the null hypotesis (which for the Jarque Bera Test is that the distribution is normal)\n",
    "\n",
    "Check for autocorrelation of the residuals, with **DW Test** if the residuals are normally distributed: https://stats.stackexchange.com/questions/14914/how-to-test-the-autocorrelation-of-the-residuals\n",
    "\n",
    "Here more information about the plots. [Link](https://stats.stackexchange.com/questions/58141/interpreting-plot-lm#65864)\n",
    "\n",
    "General rules:\n",
    "\n",
    "- **Residual vs Fitted**: we want a horizontal red line with homogeneus spread. It's a check for the heterodasticity of the distribution. If the residuals change is correlated with the fitted values or the original, our model assumptions are NOK (see https://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n",
    "- **Scale location**: we want the red line horizontal - same check as above (see https://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107#52107)\n",
    "- **Normal QQ**: You can interpret a qq-plot analytically by considering the values read from the axes compare for a given plotted point. If the data were well described by a normal distribution, the values should be about the same. For example, take the extreme point at the very far left bottom corner: its x value is somewhere past −3, but its y value is only a little past −.2, so it is much further out than it 'should' be. In general, a simple rubric to interpret a qq-plot is that if a given tail twists off counterclockwise from the reference line, there is more data in that tail of your distribution than in a theoretical normal, and if a tail twists off clockwise there is less data in that tail of your distribution than in a theoretical normal. In other words:\n",
    "\n",
    "    - if both tails twist counterclockwise you have heavy tails (leptokurtosis),\n",
    "    - if both tails twist clockwise, you have light tails (platykurtosis),\n",
    "    - if your right tail twists counterclockwise and your left tail twists clockwise, you have - right skew\n",
    "    - if your left tail twists counterclockwise and your right tail twists clockwise, you have left skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Only done for model #3\n",
    "#options(repr.plot.width=6, repr.plot.height=5)\n",
    "## Outlier tests\n",
    "#print('Outlier Tests')\n",
    "#outlierTest(r_train_dataframe.mod1)\n",
    "#leveragePlots(r_train_dataframe.mod1)\n",
    "#qqPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate homoscedasticity\n",
    "## non-constant error variance test\n",
    "#print('Homoscedasticity Tests')\n",
    "#ncvTest(r_train_dataframe.mod1)\n",
    "## plot studentized residuals vs. fitted values \n",
    "#spreadLevelPlot(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Nonlinearity\n",
    "## component + residual plot \n",
    "#print('Nonlinearity Tests')\n",
    "#crPlots(r_train_dataframe.mod1)\n",
    "## Ceres plots \n",
    "#ceresPlots(r_train_dataframe.mod1)\n",
    "#\n",
    "## Evaluate Collinearity\n",
    "#print('Nonlinearity Tests')\n",
    "#vif(r_train_dataframe.mod1) # variance inflation factors \n",
    "#sqrt(vif(r_train_dataframe.mod1)) > 2 # problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit Plot\n",
    "\n",
    "Use the cells below to plot model data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Traditional R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 500\n",
    "\n",
    "plot(r_train_dataframe.mod1.reference ,col=\"black\")\n",
    "grid(5, 5, lwd = 0.85) # grid only in y-direction\n",
    "points(r_train_dataframe.mod1$index, r_train_dataframe.mod1$fit, col=\"red\", type='b')\n",
    "points(r_train_dataframe.mod2$index, r_train_dataframe.mod2$fit, col=\"green\", type='b')\n",
    "points(r_train_dataframe.mod3$index, r_train_dataframe.mod3$fit, col=\"blue\", type='b')\n",
    "points(r_train_dataframe.mod4$index, r_train_dataframe.mod4$fit, col=\"yellow\", type='b')\n",
    "points(r_train_dataframe.mod5$index, r_train_dataframe.mod5$fit, col=\"gray\", type='b')\n",
    "\n",
    "\n",
    "legend('topright', \n",
    "       legend=c(r_train_dataframe.mod1.reference.name, \n",
    "                r_train_dataframe.mod1.name,\n",
    "                r_train_dataframe.mod2.name, \n",
    "                r_train_dataframe.mod3.name,\n",
    "                r_train_dataframe.mod4.name,                \n",
    "                r_train_dataframe.mod5.name), \n",
    "       col=c(\"black\", \"red\", \"green\", \"blue\", \"yellow\", \"gray\"), lty=1:6, cex=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Plot data fitting with respect to sample\n",
    "p1 <- plot_ly()\n",
    "\n",
    "## Reference\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe), \n",
    "                y = ~r_train_dataframe.mod1.reference, \n",
    "                name = r_train_dataframe.mod1.reference.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 1\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod1), \n",
    "                y = ~r_train_dataframe.mod1$fit,\n",
    "               name = r_train_dataframe.mod1.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 2\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod2), \n",
    "                y = ~r_train_dataframe.mod2$fit,\n",
    "               name = r_train_dataframe.mod2.name, mode = 'lines')\n",
    "\n",
    "## Model Fit 3\n",
    "p1 <- add_trace(p1, \n",
    "                x = time(r_train_dataframe.mod3), \n",
    "                y = ~r_train_dataframe.mod3$fit,\n",
    "               name = r_train_dataframe.mod3.name, mode = 'lines')\n",
    "\n",
    "### Model Fit 4\n",
    "#p1 <- add_trace(p1, \n",
    "#                x = time(r_train_dataframe.mod4), \n",
    "#                y = ~r_train_dataframe.mod4$fit,\n",
    "#               name = r_train_dataframe.mod4.name)\n",
    "\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages('h2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "485px",
    "left": "63px",
    "top": "107px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
