{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Data-Import/Export\" data-toc-modified-id=\"Data-Import/Export-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Import/Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Local-Test\" data-toc-modified-id=\"Import-Local-Test-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import Local Test</a></span></li><li><span><a href=\"#Import-from-API\" data-toc-modified-id=\"Import-from-API-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Import from API</a></span></li><li><span><a href=\"#Data-Export\" data-toc-modified-id=\"Data-Export-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Export</a></span></li></ul></li><li><span><a href=\"#Calculator\" data-toc-modified-id=\"Calculator-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Calculator</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-shot-tests\" data-toc-modified-id=\"One-shot-tests-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>One-shot tests</a></span></li><li><span><a href=\"#Heating-tests\" data-toc-modified-id=\"Heating-tests-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Heating tests</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-Plots\" data-toc-modified-id=\"Time-Series-Plots-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Time Series Plots</a></span></li><li><span><a href=\"#Back2Back-Correlation-Plot\" data-toc-modified-id=\"Back2Back-Correlation-Plot-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Back2Back Correlation Plot</a></span></li><li><span><a href=\"#Full-Seaborn-Correlogram\" data-toc-modified-id=\"Full-Seaborn-Correlogram-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Full Seaborn Correlogram</a></span></li><li><span><a href=\"#Plots\" data-toc-modified-id=\"Plots-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-dataframe\" data-toc-modified-id=\"Prepare-dataframe-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Prepare dataframe</a></span></li><li><span><a href=\"#Heatmap\" data-toc-modified-id=\"Heatmap-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Heatmap</a></span></li><li><span><a href=\"#Vertical-Bar-Plot\" data-toc-modified-id=\"Vertical-Bar-Plot-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Vertical Bar Plot</a></span></li></ul></li><li><span><a href=\"#TODO:-Anomaly-Detection\" data-toc-modified-id=\"TODO:-Anomaly-Detection-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>TODO: Anomaly Detection</a></span></li></ul></li><li><span><a href=\"#AlphaSense-Baseline-Calibration\" data-toc-modified-id=\"AlphaSense-Baseline-Calibration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>AlphaSense Baseline Calibration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Model-Metrics\" data-toc-modified-id=\"Baseline-Model-Metrics-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Baseline Model Metrics</a></span></li><li><span><a href=\"#TODO:-Correction-Checks\" data-toc-modified-id=\"TODO:-Correction-Checks-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>TODO: Correction Checks</a></span></li></ul></li><li><span><a href=\"#TODO:-MICS-Baseline-Correction\" data-toc-modified-id=\"TODO:-MICS-Baseline-Correction-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TODO: MICS Baseline Correction</a></span></li><li><span><a href=\"#Data-Model-Creation\" data-toc-modified-id=\"Data-Model-Creation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Model Creation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combine-dataframe\" data-toc-modified-id=\"Combine-dataframe-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Combine dataframe</a></span></li><li><span><a href=\"#Batch-Model-Process\" data-toc-modified-id=\"Batch-Model-Process-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Batch Model Process</a></span></li><li><span><a href=\"#Ordinary-Linear-Regression\" data-toc-modified-id=\"Ordinary-Linear-Regression-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Ordinary Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preliminary-Checks\" data-toc-modified-id=\"Preliminary-Checks-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>Preliminary Checks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-stationarity\" data-toc-modified-id=\"Data-stationarity-7.3.1.1\"><span class=\"toc-item-num\">7.3.1.1&nbsp;&nbsp;</span>Data stationarity</a></span></li><li><span><a href=\"#Autocorrelation\" data-toc-modified-id=\"Autocorrelation-7.3.1.2\"><span class=\"toc-item-num\">7.3.1.2&nbsp;&nbsp;</span>Autocorrelation</a></span></li><li><span><a href=\"#Granger-Casuality-Test-(use-with-caution)\" data-toc-modified-id=\"Granger-Casuality-Test-(use-with-caution)-7.3.1.3\"><span class=\"toc-item-num\">7.3.1.3&nbsp;&nbsp;</span>Granger Casuality Test (use with caution)</a></span></li></ul></li><li><span><a href=\"#Model-Definition\" data-toc-modified-id=\"Model-Definition-7.3.2\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;</span>Model Definition</a></span></li><li><span><a href=\"#Model-prediction\" data-toc-modified-id=\"Model-prediction-7.3.3\"><span class=\"toc-item-num\">7.3.3&nbsp;&nbsp;</span>Model prediction</a></span></li><li><span><a href=\"#Model-Export-to-Disk\" data-toc-modified-id=\"Model-Export-to-Disk-7.3.4\"><span class=\"toc-item-num\">7.3.4&nbsp;&nbsp;</span>Model Export to Disk</a></span></li><li><span><a href=\"#Model-Load-from-Disk\" data-toc-modified-id=\"Model-Load-from-Disk-7.3.5\"><span class=\"toc-item-num\">7.3.5&nbsp;&nbsp;</span>Model Load from Disk</a></span></li></ul></li><li><span><a href=\"#ML\" data-toc-modified-id=\"ML-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>ML</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection-and-data-training-split\" data-toc-modified-id=\"Feature-selection-and-data-training-split-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Feature selection and data training split</a></span></li><li><span><a href=\"#Model-Fit\" data-toc-modified-id=\"Model-Fit-7.4.2\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;</span>Model Fit</a></span></li><li><span><a href=\"#Model-prediction\" data-toc-modified-id=\"Model-prediction-7.4.3\"><span class=\"toc-item-num\">7.4.3&nbsp;&nbsp;</span>Model prediction</a></span></li><li><span><a href=\"#Model-Export-to-Disk\" data-toc-modified-id=\"Model-Export-to-Disk-7.4.4\"><span class=\"toc-item-num\">7.4.4&nbsp;&nbsp;</span>Model Export to Disk</a></span></li></ul></li><li><span><a href=\"#Model-Comparison\" data-toc-modified-id=\"Model-Comparison-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Model Comparison</a></span><ul class=\"toc-item\"><li><span><a href=\"#TimeSeries-Comparison\" data-toc-modified-id=\"TimeSeries-Comparison-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>TimeSeries Comparison</a></span></li><li><span><a href=\"#Model-Metrics-Comparison\" data-toc-modified-id=\"Model-Metrics-Comparison-7.5.2\"><span class=\"toc-item-num\">7.5.2&nbsp;&nbsp;</span>Model Metrics Comparison</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Model-Load\" data-toc-modified-id=\"Data-Model-Load-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Data Model Load</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Model\" data-toc-modified-id=\"Import-Model-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Import Model</a></span></li><li><span><a href=\"#Apply-Model\" data-toc-modified-id=\"Apply-Model-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Apply Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     26
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    code_show=true; \n",
       "    function code_toggle() {\n",
       "        if (code_show){\n",
       "            $('div.input').show();\n",
       "        } else {\n",
       "            $('div.input').hide();\n",
       "        }\n",
       "        code_show = !code_show\n",
       "    } \n",
       "    $( document ).ready(code_toggle);\n",
       "    </script>\n",
       "    \n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import getcwd, pardir\n",
    "from os.path import join, abspath\n",
    "\n",
    "# ! pip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions\n",
    "# ! jupyter contrib nbextension install\n",
    "# ! jupyter nbextension install --py fileupload \n",
    "# ! jupyter nbextension enable --py fileupload\n",
    "# ! jupyter nbextension install --py widgetsnbextension \n",
    "# ! jupyter nbextension enable --py widgetsnbextension\n",
    "# ! jupyter nbextensions_configurator enable\n",
    "# ! jupyter nbextension enable codefolding/main\n",
    "# ! jupyter nbextension enable toc2/main\n",
    "\n",
    "! jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "## Get Root Directory\n",
    "rootDirectory = abspath(join(abspath(join(getcwd(), pardir)), pardir))\n",
    "\n",
    "# `do not disturb` mode\n",
    "import warnings                                  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Create a button that hides cells\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '''\n",
    "    <script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "        if (code_show){\n",
    "            $('div.input').show();\n",
    "        } else {\n",
    "            $('div.input').hide();\n",
    "        }\n",
    "        code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    \n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Import/Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Local Test\n",
    "\n",
    "Import test from local test database. \n",
    "\n",
    "**Requirement**:\n",
    "\n",
    "- Include where the directory of your tests is (GIT LFS directory)\n",
    "- Make sure that the desired test is available and has been created with the yaml tool\n",
    "\n",
    "**The cell below will**:\n",
    "\n",
    "- Load all the kits within the test\n",
    "- Check if there were alphasense sensors and retrieve their calibration data and order\n",
    "- Check if there was a reference and convert it units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925280fd87c341898969cd0755d49233"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544ef7c003da4964bcdac21dcf90cc39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fcd7c82b734820a12d3ae0a16900c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from test_utils import *\n",
    "\n",
    "testsDir = join(rootDirectory, 'smartcitizen-iscape-tests')\n",
    "\n",
    "def loadButton(b):\n",
    "    frequency = frequency_text.value + frequency_drop.value\n",
    "    global readings\n",
    "    readings = loadTest(frequency)\n",
    "\n",
    "def clearButton(b):\n",
    "    global readings\n",
    "    clearTests()\n",
    "    readings = {}\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Import Local Tests</h4>'))\n",
    "\n",
    "tests = getTests(testsDir)\n",
    "interact(selectTests,\n",
    "         x = widgets.SelectMultiple(options=tests, \n",
    "                           selected_labels = selectedTests, \n",
    "                           layout=widgets.Layout(width='700px')))\n",
    "\n",
    "loadB = widgets.Button(description='Load Local Tests')\n",
    "loadB.on_click(loadButton)\n",
    "\n",
    "frequency_text = widgets.Text(description = 'Frequency',\n",
    "                              value = '1',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "frequency_drop = widgets.Dropdown(options = ['H', 'Min', 'S'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='100px'))\n",
    "\n",
    "frequency_box = widgets.HBox([frequency_text, frequency_drop])\n",
    "\n",
    "resetB = widgets.Button(description='Clear Tests')\n",
    "resetB.on_click(clearButton)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB, resetB])\n",
    "totalBox = widgets.VBox([frequency_box, buttonBox])\n",
    "display(totalBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     3,
     24
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading device 4814\n",
      "Kit ID 19\n",
      "\tFrom Date 2018-10-04 00:00:00 to Date 2018-10-16 00:00:00\n",
      "\tDevice located in Europe/Madrid\n",
      "\tDevice ID says it had alphasense sensors, loading them\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "from api_utils import *\n",
    "import re\n",
    "\n",
    "def getDeviceDataInteractive(b):\n",
    "    clear_output()\n",
    "    frequency = frequency_text.value + frequency_drop.value\n",
    "    devices = kitList.value.strip('').split(',')\n",
    "    devicesCorrected = list()\n",
    "    for device in devices: \n",
    "        device = re.sub(' ', '', device)\n",
    "        devicesCorrected.append(device)\n",
    "    test_name = testName.value\n",
    "    # print devicesCorrected\n",
    "    if test_name != '':\n",
    "        try:\n",
    "            readings[test_name] = dict()\n",
    "            readings[test_name] = getReadingsAPI(devicesCorrected, frequency, start_date_widget.value, end_date_widget.value)\n",
    "        except NameError:\n",
    "            global readings\n",
    "            readings = dict()\n",
    "            readings[test_name] = getReadingsAPI(devicesCorrected, frequency, start_date_widget.value, end_date_widget.value)\n",
    "    else:\n",
    "        print 'Input test '\n",
    "\n",
    "def getKitIDInteractive(b):\n",
    "    clear_output()\n",
    "    devices = kitList.value.strip('').split(',')\n",
    "    kitIDs = list()\n",
    "    for device in devices:\n",
    "        kitID = getKitID(device, False)\n",
    "        kitIDs.append(kitID)\n",
    "        print('Device {} has kitID {}'.format(device, kitID))\n",
    "    return kitIDs\n",
    "\n",
    "kitList = widgets.Text(description = 'Kit List: ')\n",
    "testName = widgets.Text(description = 'Input Test Name')\n",
    "\n",
    "getKitIDb = widgets.Button(description='Get Kit ID')\n",
    "getKitIDb.on_click(getKitIDInteractive)\n",
    "\n",
    "loadAPIb = widgets.Button(description='Load API Kit')\n",
    "loadAPIb.on_click(getDeviceDataInteractive)\n",
    "\n",
    "frequency_text = widgets.Text(description = 'Frequency',\n",
    "                              value = '1',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "frequency_drop = widgets.Dropdown(options = ['H', 'Min', 'S'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='50px'))\n",
    "\n",
    "start_date_widget = widgets.DatePicker(description='Start Date')\n",
    "end_date_widget = widgets.DatePicker(description='End Date')\n",
    "dateBox = widgets.HBox([start_date_widget, end_date_widget])\n",
    "\n",
    "frequency_box = widgets.HBox([frequency_text, frequency_drop])\n",
    "\n",
    "Hbox = widgets.HBox([kitList, testName])\n",
    "ButtonBox = widgets.HBox([getKitIDb, loadAPIb])\n",
    "Box = widgets.VBox([Hbox, frequency_box, dateBox, ButtonBox])\n",
    "\n",
    "display(Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merging_test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS' # \n",
    "merging_device = 'STATION_CASE'\n",
    "merged_to_test = 'STATION' # Final test\n",
    "merged_to_device = '4748'\n",
    "\n",
    "def mergeCSVAPI(merging_test, merging_device, merged_to_test, merged_to_device):\n",
    "    data_merged = readings[merged_to_test]['devices'][merged_to_device]['data']\n",
    "    data_merging = readings[merging_test]['devices'][merging_device]['data']\n",
    "\n",
    "    #print data_merged.index\n",
    "    #print data_merging.index\n",
    "    \n",
    "    #data = pd.concat([data_merged, data_merging], axis = 0)\n",
    "    #data = pd.merge(data_merged, data_merging, how='left', left_index=True, right_index=True)\n",
    "    data = data_merged.combine_first(data_merging)\n",
    "    # display(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "try: \n",
    "    data = mergeCSVAPI(merging_test, merging_device, merged_to_test, merged_to_device)\n",
    "    readings[merged_to_test]['devices'][merged_to_device]['data'] = data\n",
    "except:\n",
    "    print 'Data could not be combined, review data'\n",
    "else:\n",
    "    print 'Data Combined'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     8,
     12,
     17
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "global selected\n",
    "selected = []\n",
    "\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "selected_export=tuple()\n",
    "def selectedDevices_export(Source):\n",
    "    global selected_export\n",
    "    selected_export = list(Source)\n",
    "    \n",
    "def show_device_export(Source):\n",
    "    _devices_select_export.options = [s for s in list(readings[_test_export.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def exportFile(b):\n",
    "    for i in range(len(selected_export)):\n",
    "        b.f = selected_export[i]\n",
    "        exportDir = exportPath.value\n",
    "        if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "        savePath = os.path.join(exportDir, b.f)\n",
    "        if not os.path.exists(savePath):\n",
    "            readings[_test_export.value]['devices'][b.f]['data'].to_csv(savePath + '.csv', sep=\",\")\n",
    "            display(FileLink(savePath))\n",
    "        else:\n",
    "            display(widgets.HTML('File Already exists!'))\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test_export = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_export_drop = widgets.interactive(show_device_export, \n",
    "                                Source=_test_export, \n",
    "                                layout=layout)\n",
    "\n",
    "_devices_select_export = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "_devices_select_export_drop = interact(selectedDevices_export,\n",
    "                                 Source = _devices_select_export)\n",
    "\n",
    "display(widgets.HTML('<h3>Export Files</h3>'))\n",
    "exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='700px'))\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportFile)\n",
    "\n",
    "selectBox = widgets.VBox([_test_export_drop, _devices_select_export])\n",
    "exportBox = widgets.HBox([exportPath,eb])\n",
    "_BOX=widgets.VBox([selectBox, exportBox])\n",
    "display(_BOX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculator\n",
    "Input your formulas into this cell for analysis in the plots below\n",
    "\n",
    "There are formulas for calculating:\n",
    "- *MICS* = Poly(R, H, T) - **MICS_FORMULA**\n",
    "- *Alphasense's correction proposal* = f(Curr, Sens, Zero) - **AD_FORMULA**\n",
    "- *Smoothing* = f(Signal, Window) - **SMOOTH**\n",
    "- *Absolute humidity* = f(Temperature, Humidity, Pressure) - **ABS_HUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     10,
     28,
     40,
     55
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from formula_utils import *\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def show_device_list(Source):\n",
    "    _devices_select.options = [s for s in list(readings[_test.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(readings[_test.value]['devices'][s]['data'].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    clear_output()\n",
    "    A = _Aterm.value\n",
    "    B = _Bterm.value\n",
    "    C = _Cterm.value\n",
    "    D = _Dterm.value\n",
    "    Name = _formulaName.value\n",
    "    for s in list(selected):\n",
    "        result = functionFormula(s,A,B,C,D,readings)\n",
    "        readings[_test.value]['devices'][s]['data'][Name] = result\n",
    "    print \"Formula {} Added in test {}\".format(Name, _test.value)\n",
    "    \n",
    "def functionFormula(s, Aname, Bname, Cname, Dname, _readings): \n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(_readings[_test.value]['devices'][s]['data'].loc[:,(Aname,)],_readings[_test.value]['devices'][s]['data'].loc[:,(Bname,)],left_index=True, right_index=True), _readings[_test.value]['devices'][s]['data'].loc[:,(Cname,)], left_index=True, right_index=True),_readings[_test.value]['devices'][s]['data'].loc[:,(Dname,)],left_index=True, right_index=True)\n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "        \n",
    "selected=tuple()\n",
    "def selectedDevices(Source):\n",
    "    global selected\n",
    "    selected = list(Source)\n",
    "    commonChannels(selected)\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_device_list, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "_devices_select = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "_devices_select_drop = interact(selectedDevices,\n",
    "                                 Source = _devices_select)\n",
    "\n",
    "_selectBox = widgets.VBox([_test_drop, _devices_select])\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_selectBox, _ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Select the Files for your formulas to apply</h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)\n",
    "\n",
    "## Vapour equilibrium: B is temperature in degC, assumed 1013mbar\n",
    "# (1.0007 + 3.46*1e-6*1013)*6.1121*np.exp(17.502*B/(240.97+B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot One-shot tests\n",
    "from signal_utils import plot_oneshots\n",
    "channels_pm = ['PM 1.0', 'PM 2.5', 'PM 10.0']\n",
    "device_one_shot = 'KIT_1_ALTERNATE'\n",
    "plot_oneshots(readings, channels_pm, device_one_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heating tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To run in temperature tests with current that have 'ON_OFF' calculated with greater(current, 20)\n",
    "from signal_utils import split_agnostisise\n",
    "\n",
    "for reading in readings:\n",
    "    print (reading)\n",
    "    dataframeResult = split_agnostisise(readings, reading, 'ON_OFF')\n",
    "    # dataframeResult = split_agnostisise(readings, reading, 'measuring')\n",
    "    readings[reading]['devices']['analysis'] = dict()\n",
    "    readings[reading]['devices']['analysis']['data'] = dict()\n",
    "    readings[reading]['devices']['analysis']['data'] = dataframeResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     34,
     40,
     165,
     200,
     208,
     213,
     220,
     222
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# --\n",
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_devices(Source):\n",
    "    _device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    _device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    _max_date.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "    del axisshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        axisshow.append(_axis_drop.value)\n",
    "        \n",
    "    plot_data = readings[toshow[0][0]]['devices'][toshow[0][1]]['data'].loc[:,(toshow[0][2],)]\n",
    "    list_data_primary = []\n",
    "    list_data_secondary = []\n",
    "    list_data_terciary = []\n",
    "    \n",
    "    if b.slice_time:\n",
    "        plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "        plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            plot_data = pd.merge(plot_data, readings[toshow[i][0]]['devices'][toshow[i][1]]['data'].loc[:,(toshow[i][2],)], left_index=True, right_index=True)\n",
    "\n",
    "    print ('-------------------------------------')\n",
    "    print (' Medias:\\n')\n",
    "    meanTable = []\n",
    "    for d in toshow:\n",
    "        myMean = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        meanTable.append(myMean)   \n",
    "    res = plot_data.mean()\n",
    "    for i in range(len(meanTable)): print (meanTable[i] + '%.2f' % (res[i]))\n",
    "    print ('-------------------------------------')\n",
    "    \n",
    "    print ('-------------------------------------')\n",
    "    print (' Std Deviation:\\n')\n",
    "    stdTable = []\n",
    "    for d in toshow:\n",
    "        myStd = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        stdTable.append(myStd)   \n",
    "    std = plot_data.std()\n",
    "    for i in range(len(stdTable)): print stdTable[i] + '%.2f' % (std[i])\n",
    "    print ('-------------------------------------')\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    for i in range(len(plot_data.columns)):\n",
    "        changed.append(toshow[i][0] + ' - '+ toshow[i][1] + ' - '+ plot_data.columns[i])\n",
    "    plot_data.columns = changed\n",
    "    \n",
    "    subplot_rows = 0\n",
    "    if len(toshow) > 0:\n",
    "        for i in range(len(toshow)):\n",
    "            if axisshow[i]=='1': \n",
    "                list_data_primary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,1)\n",
    "            if axisshow[i]=='2': \n",
    "                list_data_secondary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,2)\n",
    "            if axisshow[i]=='3': \n",
    "                list_data_terciary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,3)\n",
    "          \n",
    "    \n",
    "    if _matplotly.value == 'Plotly':\n",
    "        fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "    \n",
    "        #if len(list_data_primary)>0:\n",
    "            #fig1 = plot_data.iplot(kind='scatter', y = list_data_primary, asFigure=True, layout = layout)\n",
    "        #ply.offline.iplot(fig1)\n",
    "        \n",
    "        for i in range(len(list_data_primary)):\n",
    "            fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "    \n",
    "        for i in range(len(list_data_secondary)):\n",
    "            fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "        \n",
    "        for i in range(len(list_data_terciary)):\n",
    "            fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "    \n",
    "        if setLimits: \n",
    "            fig1['layout'].update(height = 800,\n",
    "                                legend=dict(x=-.1, y=1.2) ,\n",
    "                               xaxis=dict(title='Time'))\n",
    "                              \n",
    "        else:\n",
    "            fig1['layout'].update(height = 800,\n",
    "                                  legend=dict(x=-.1, y=1.2) ,\n",
    "                               xaxis=dict(title='Time'))\n",
    "                               \n",
    "        ply.offline.iplot(fig1)\n",
    "        \n",
    "    elif _matplotly.value == 'Matplotlib':\n",
    "        \n",
    "        fig, axes = plot.subplots(subplot_rows, 1, figsize=(15,10))\n",
    "        # Four axes, returned as a 2-d array\n",
    "        \n",
    "        if subplot_rows == 1:\n",
    "            for i in range(len(list_data_primary)):\n",
    "                axes.plot(plot_data.index, plot_data[list_data_primary[i]], label =  list_data_primary[i])\n",
    "                axes.legend(loc='best')\n",
    "\n",
    "        else:\n",
    "            for i in range(len(list_data_primary)):\n",
    "                axes[0].plot(plot_data.index, plot_data[list_data_primary[i]], label =  list_data_primary[i])\n",
    "                axes[0].legend(loc='best')\n",
    "                axes[0].grid(visible = True)\n",
    "\n",
    "            for i in range(len(list_data_secondary)):\n",
    "                axes[1].plot(plot_data.index, plot_data[list_data_secondary[i]], label =  list_data_secondary[i])\n",
    "                axes[1].legend(loc='best')\n",
    "                axes[1].grid(visible = True)\n",
    "\n",
    "            for i in range(len(list_data_terciary)):\n",
    "                axes[2].plot(plot_data.index, plot_data[list_data_terciary[i]], label =  list_data_terciary[i])\n",
    "                axes[2].legend(loc='best')\n",
    "                axes[2].grid(visible = True)\n",
    "\n",
    "        plot.xlabel('Date') \n",
    "        plot.grid(visible = True)\n",
    "        plot.show()\n",
    "        \n",
    "    \n",
    "def reset_time(b):\n",
    "    _min_date.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "\n",
    "# Test dropdown\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "# Device dropdown\n",
    "_device = widgets.Dropdown(layout=layout,\n",
    "                        description = 'Device')\n",
    "\n",
    "_device_drop = widgets.interactive(show_sensors, \n",
    "                                Source=_device, \n",
    "                                layout=layout)\n",
    "\n",
    "# Sensor dropdown\n",
    "_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "\n",
    "# Buttons\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "# Axis dropdown\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Synchronise Checkbox\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, \n",
    "                                     description='Synchronise X axis', \n",
    "                                     disabled=False, \n",
    "                                     layout=widgets.Layout(width='300px'))\n",
    "\n",
    "_matplotly = widgets.RadioButtons(\n",
    "    options=['Matplotlib', 'Plotly'], value='Matplotlib',\n",
    "    description='Plot Type',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Date fields\n",
    "_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Date buttons\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "#_b_reset_time.src = _kit\n",
    "\n",
    "\n",
    "_device_box = widgets.HBox([_test_drop, _device_drop])\n",
    "_sensor_box = widgets.HBox([_sensor_drop, _axis_drop, _synchroniseXaxis])\n",
    "_plot_type_box = widgets.VBox([_matplotly])\n",
    "\n",
    "_plot_box = widgets.HBox([_b_add , _b_reset_all])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_matplotly, _time_box, _device_box, _sensor_box, _plot_box])\n",
    "display(_root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back2Back Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17,
     26,
     32,
     113
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "\n",
    "def show_devices(Source):\n",
    "    A_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    A_device.source = Source\n",
    "    B_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    B_device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    A_sensor_drop.source = Source\n",
    "    minCropDate.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    B_sensor_drop.source = Source\n",
    "    minCropDate.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr    \n",
    "\n",
    "    \n",
    "def redraw(b):\n",
    "    cropTime = cropTimeCheck.value\n",
    "    doubleAxis = doubleAxisCheck.value\n",
    "    min_date = minCropDate.value\n",
    "    max_date = maxCropDate.value\n",
    "    mergedData = pd.merge(readings[_test.value]['devices'][A_device.value]['data'].loc[:,(A_sensor_drop.value,)], \n",
    "                          readings[_test.value]['devices'][B_device.value]['data'].loc[:,(B_sensor_drop.value,)], \n",
    "                          left_index=True, right_index=True, suffixes=('_'+A_sensor_drop.value, '_'+B_sensor_drop.value))\n",
    "    clear_output()\n",
    "    \n",
    "    if cropTime:\n",
    "        mergedData = mergedData[mergedData.index > min_date]\n",
    "        mergedData = mergedData[mergedData.index < max_date]\n",
    "        \n",
    "    #jointplot\n",
    "    df = pd.DataFrame()\n",
    "    A = A_sensor_drop.value + '-' + A_device.value\n",
    "    B = B_sensor_drop.value + '-' + B_device.value\n",
    "    df[A] = mergedData.iloc[:,0]\n",
    "    df[B] = mergedData.iloc[:,1]\n",
    "    \n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", size=12, scatter_kws={\"s\": 80});\n",
    "    print \"data from \" + str(df.index.min()) + \" to \" + str(df.index.max())                      \n",
    "    pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "    rmse = sqrt(mean_squared_error(df[A].fillna(0), df[B].fillna(0)))\n",
    "    print 'Pearson correlation coefficient: ' + str(pearsonCorr)\n",
    "    print 'Coefficient of determination R²: ' + str(pearsonCorr*pearsonCorr)\n",
    "    print 'RMSE: ' + str(rmse)\n",
    "\n",
    "    if cropTime: \n",
    "        \n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "        \n",
    "    trace0 = go.Scatter(x=df[A].index, y=df[A], name = A,line = dict(color='rgb(0,97,255)'))\n",
    "    \n",
    "    if (doubleAxis):\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "    else:\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "    data = [trace0, trace1]\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(figure)\n",
    "    \n",
    "if len(readings) < 1: print (\"Please load some data first...\")\n",
    "else:\n",
    "    \n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Redraw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    # Test dropdown\n",
    "    _test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout = widgets.Layout(width='500px'),\n",
    "                        description = 'Test')\n",
    "\n",
    "    _test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout = widgets.Layout(width='500px'))\n",
    "\n",
    "    # Device dropdown\n",
    "    A_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device')\n",
    "    \n",
    "    A_device_drop = widgets.interactive(show_sensors_A, \n",
    "                                    Source=A_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    B_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device')\n",
    "    \n",
    "    B_device_drop = widgets.interactive(show_sensors_B, \n",
    "                                    Source=B_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    A_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    B_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "    \n",
    "    # A_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px') ,value=readings.keys()[0])\n",
    "    # A_kit_drop = widgets.interactive(show_sensors_A, Source=A_kit, layout=layout)\n",
    "    # A_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    # show_sensors_A(readings.keys()[0])\n",
    "    # \n",
    "    # B_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=widgets.Layout(width='350px'), value=readings.keys()[1])\n",
    "    # B_kit_drop = widgets.interactive(show_sensors_B, Source= B_kit, layout=layout)\n",
    "    # B_sensors_drop = widgets.Dropdown(layout=widgets.Layout(width='350px'))\n",
    "    # show_sensors_B(readings.keys()[1])\n",
    "    \n",
    "    draw_box = widgets.HBox([b_redraw, doubleAxisCheck], layout=widgets.Layout(justify_content='space-between'))\n",
    "    test_box = widgets.HBox([_test_drop], layout = widgets.Layout(width='500px'))\n",
    "    device_box = widgets.HBox([A_device, widgets.HTML('<h4><< Data source >></h4>') , B_device], layout=widgets.Layout(justify_content='space-between'))\n",
    "    sensor_box = widgets.HBox([A_sensor_drop, widgets.HTML('<h4><< Sensor selection >></h4>') , B_sensor_drop], layout=widgets.Layout(justify_content='space-between'))\n",
    "    crop_box = widgets.HBox([cropTimeCheck, minCropDate, maxCropDate], layout=widgets.Layout(justify_content='space-between'))\n",
    "    root_box = widgets.VBox([draw_box, test_box, device_box, sensor_box, crop_box])\n",
    "    \n",
    "    display(root_box)\n",
    "    \n",
    "    #redraw(b_redraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Seaborn Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paint(Source):\n",
    "    clear_output()\n",
    "    sns.set(font_scale=1.4)\n",
    "    g = sns.PairGrid(readings.values()[0])\n",
    "    g = g.map(plt.scatter)\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(paint, Source=_kit, layout=layout)\n",
    "display(_kit_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'STATION_CASE'\n",
    "test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "\n",
    "min_date = '2018-08-03 00:00:00'\n",
    "max_date = '2018-09-17 00:00:00'\n",
    "\n",
    "# Assign by time-frames\n",
    "freq_time = 2\n",
    "\n",
    "# Select labels\n",
    "list_channels = ['EXT_PM_1', 'EXT_PM_25', 'EXT_PM_10', 'TEMP']\n",
    "\n",
    "## -----------\n",
    "dataframePlot = pd.DataFrame()\n",
    "dataframePlot = readings[test]['devices'][device]['data'].copy()\n",
    "dataframePlot = dataframePlot[dataframePlot.index>min_date]\n",
    "dataframePlot = dataframePlot[dataframePlot.index<max_date]\n",
    "\n",
    "if freq_time == 6:\n",
    "    labels = ['Morning','Afternoon','Evening', 'Night']\n",
    "    len_labels = 4\n",
    "elif freq_time == 12:\n",
    "    labels = ['Morning', 'Evening']\n",
    "    len_labels = 2\n",
    "else:\n",
    "    labels = [str(i) for i in np.arange(0, 24, freq_time)]\n",
    "    len_labels = freq_time * len(labels)\n",
    "    \n",
    "vector_time = np.arange(0, 25, freq_time)\n",
    "\n",
    "dataframePlot = dataframePlot.assign(session=pd.cut(dataframePlot.index.hour,\n",
    "                                            vector_time,\n",
    "                                            labels=labels))\n",
    "# Group them by session\n",
    "df_se = dataframePlot.groupby(['session']).mean()\n",
    "df_se = df_se[list_channels]\n",
    "\n",
    "# Calculate average\n",
    "df_se_avg = df_se.mean(axis = 0)\n",
    "\n",
    "display(df_se)\n",
    "\n",
    "# Add additional columns\n",
    "append_rel = '_AVG_REL'\n",
    "\n",
    "list_all = []\n",
    "list_all.append('session')\n",
    "for column in list_channels:\n",
    "    dataframePlot[column + append_rel] = dataframePlot[column]/df_se_avg[column]\n",
    "    list_all.append(column)\n",
    "    list_all.append(column + append_rel)\n",
    "\n",
    "## Full dataframe\n",
    "dataframePlot = dataframePlot[list_all]\n",
    "dataframePlot.dropna()\n",
    "\n",
    "## Dataframe by session\n",
    "dataframePlot_avg = dataframePlot.groupby(['session']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "channel =  'EXT_PM_25'\n",
    "colorscale = [[0, '#edf8fb'], [.3, '#b3cde3'],  [.6, '#8856a7'],  [1, '#810f7c']]\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=dataframePlot[channel],\n",
    "        x=dataframePlot.index.date,\n",
    "        y=dataframePlot['session'],\n",
    "        colorscale=colorscale,\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Daily Pollutant',\n",
    "    xaxis = dict(ticks=''),\n",
    "    yaxis = dict(ticks='' , categoryarray=labels, autorange = 'reversed')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "\n",
    "channel_1 =  'EXT_PM_1' # Goes on the left side (x1)\n",
    "channel_2 =  'TEMP' # Goes on the right side (x2)\n",
    "relative = False\n",
    "\n",
    "if relative:\n",
    "    channel_1 = channel_1 + append_rel\n",
    "    channel_2 = channel_2 + append_rel\n",
    "    limits = ([0.5, 1.5], [0.5, 1.5])\n",
    "    marks = ()\n",
    "else:\n",
    "    limits = ([0, 20], [15, 40]) # x1, x2\n",
    "    marks = ([10, 'x'], [20, 'x'], [300, 'x2'], [400, 'x2'])\n",
    "\n",
    "dict_shapes = [{'type': 'line', \n",
    "                      'x0': mark[0],\n",
    "                      'y0': -1,\n",
    "                      'x1': mark[0],\n",
    "                      'y1': len_labels+1,\n",
    "                      'xref': mark[1],\n",
    "                      'line': {\n",
    "                          'color': 'rgba(5, 0, 0, 0.8)',\n",
    "                          'width': 1,\n",
    "                          'dash': 'dot'\n",
    "                      }}\n",
    "               for mark in marks]\n",
    "\n",
    "trace1 = go.Bar(\n",
    "            x=dataframePlot_avg[channel_1],\n",
    "            y=labels,\n",
    "            orientation = 'h',\n",
    "            xaxis = 'x',\n",
    "            yaxis = 'y',\n",
    "            name = channel_1,\n",
    "            marker = dict(color = 'rgba(58, 78, 255, 0.6)',\n",
    "                          line = dict(color = 'rgba(58, 78, 255, 1.0)', width = 2))\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "            x=dataframePlot_avg[channel_2],\n",
    "            y=labels,\n",
    "            orientation = 'h',\n",
    "            xaxis = 'x2',\n",
    "            yaxis='y',\n",
    "            name = channel_2,\n",
    "            marker = dict(color = 'rgba(58, 71, 80, 0.6)',\n",
    "                          line = dict(color = 'rgba(58, 71, 80, 1.0)', width = 2))\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(title='Daily average measurement (freq = {}h)'.format(freq_time),\n",
    "                   xaxis = dict(domain=[0, 0.5], autorange='reversed', title = channel_1), # range=(limits[0][0], limits[0][1])),\n",
    "                   xaxis2 = dict(domain=[0.5, 1], title = channel_2), # range=(limits[1][0], limits[1][1])),\n",
    "                   yaxis = dict(range=(-1, len_labels+1), autorange='reversed'),\n",
    "                   yaxis2 = dict(range=(-1, len_labels+1), autorange='reversed'),\n",
    "                   bargap = 0)#,\n",
    "                   #shapes = dict_shapes)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Anomaly Detection\n",
    "\n",
    "Check this here https://annals-csis.org/proceedings/2012/pliks/118.pdf.\n",
    "Inspired by the code of Dmitriy Sergeev at https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3\n",
    "\n",
    "Below we'll use the Holt-Winters function as defined:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n",
    "\n",
    "$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     12
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n",
    "from scipy.optimize import minimize # for function minimization\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "class HoltWinters:\n",
    "    \n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    \n",
    "    # series - initial time series\n",
    "    # slen - length of a season\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        \n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "                     \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     31
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n",
    "    \"\"\"\n",
    "        Returns error on CV  \n",
    "        \n",
    "        params - vector of parameters for optimization\n",
    "        series - dataset with timeseries\n",
    "        slen - season length for Holt-Winters model\n",
    "    \"\"\"\n",
    "    # errors array\n",
    "    errors = []\n",
    "    \n",
    "    values = series.values\n",
    "    alpha, beta, gamma = params\n",
    "    \n",
    "    # set the number of folds for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3) \n",
    "    \n",
    "    # iterating over folds, train model on each, forecast and calculate error\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(series=values[train], slen=slen, \n",
    "                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "        \n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = loss_function(predictions, actual)\n",
    "        errors.append(error)\n",
    "        \n",
    "    return np.mean(np.array(errors))\n",
    "\n",
    "def plotHoltWinters(series, model, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    plt.plot(series.index, model.result, label = \"Model\")\n",
    "    plt.plot(series.index, series.values, label = \"Actual\")\n",
    "    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    \n",
    "    if plot_anomalies:\n",
    "        anomalies = np.array([np.NaN]*len(series))\n",
    "        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n",
    "            series.values[series.values<model.LowerBond[:len(series)]]\n",
    "        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n",
    "            series.values[series.values>model.UpperBond[:len(series)]]\n",
    "        plt.plot(series.index, anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    if plot_intervals:\n",
    "        plt.plot(series.index, model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n",
    "        plt.plot(series.index, model.LowerBond, \"r--\", alpha=0.5)\n",
    "        plt.fill_between(series.index, y1=model.UpperBond, \n",
    "                         y2=model.LowerBond, alpha=0.2, color = \"grey\")  \n",
    "    # plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n",
    "    # plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n",
    "    plt.grid(True)\n",
    "    plt.axis('tight')\n",
    "    plt.legend(loc=\"best\", fontsize=13)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data = readings[test_name]['devices'][device_name]['data'].loc[:,'NO2_AD_BASE'].copy()\n",
    "min_date = '2018-08-31 00:00:00'\n",
    "max_date = '2018-09-18 00:00:00'\n",
    "\n",
    "data = data[data.index > min_date]\n",
    "data = data[data.index < max_date]\n",
    "data = data.dropna()\n",
    "\n",
    "# initializing model parameters alpha, beta and gamma\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Minimizing the loss function \n",
    "#opt = minimize(timeseriesCVscore, x0=x, \n",
    "#               args=(data, mean_squared_error), \n",
    "#               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n",
    "#              )\n",
    "\n",
    "# ...and train the model with them, forecasting for the next 50 hours\n",
    "\n",
    "# alpha_final, beta_final, gamma_final = opt.x\n",
    "alpha_final, beta_final, gamma_final = (0.005890895737085178, 0.004673709964305939, 0.01299124317761835)\n",
    "\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "slen = 24*60/10\n",
    "\n",
    "_model = HoltWinters(data, slen = slen, \n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 0, scaling_factor = 4)\n",
    "\n",
    "_model.triple_exponential_smoothing()\n",
    "\n",
    "plotHoltWinters(data, _model, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data)\n",
    "column_data = dataframe.columns\n",
    "dataframe['model'] = _model.result\n",
    "dataframe['UpperBond'] = _model.UpperBond\n",
    "dataframe['LowerBond'] = _model.LowerBond\n",
    "\n",
    "for item in dataframe.index:\n",
    "    print item\n",
    "    if (dataframe.loc[item,column_data] > dataframe.loc[item,['UpperBond']] or dataframe.loc[item,column_data] < dataframe.loc[item,['LowerBond']]):\n",
    "        dataframe.loc[item,'anomalies'] = dataframe.loc[item,column_data]\n",
    "\n",
    "print dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     9,
     19,
     38,
     58
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(series.values, label = \"Actual\")\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaSense Baseline Calibration\n",
    "\n",
    "These functions are used to create the alphasense pollutant correction based on Working, Auxiliary and calibration data provided by alphasense. Run the 1.1.1.1 AlphaSense Sensors calibration data cell to load in the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GB_1W', 'GB_1A', 'PM_DALLAS_TEMP', 'EXT_HUM')\n",
      "------------------------------------------------------------------\n",
      "Calculation of \u001b[1mCO        \u001b[0m\n",
      "Data Range from 2018-10-05 00:00:00+02:00 to 2018-10-16 00:00:00+02:00 with 10 days\n",
      "------------------------------------------------------------------\n",
      "------------------------\n",
      " Meta Data\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hum_avg</th>\n",
       "      <th>hum_stderr</th>\n",
       "      <th>pollutant_avg</th>\n",
       "      <th>pollutant_max</th>\n",
       "      <th>pollutant_min</th>\n",
       "      <th>pollutant_std</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>39.183480</td>\n",
       "      <td>13.738696</td>\n",
       "      <td>0.905798</td>\n",
       "      <td>3.863239</td>\n",
       "      <td>2.819809e-05</td>\n",
       "      <td>0.362454</td>\n",
       "      <td>22.273216</td>\n",
       "      <td>11.242547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>22.979521</td>\n",
       "      <td>21.936709</td>\n",
       "      <td>0.436067</td>\n",
       "      <td>1.000183</td>\n",
       "      <td>1.497945e-58</td>\n",
       "      <td>0.414591</td>\n",
       "      <td>14.309410</td>\n",
       "      <td>13.639272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>0.083812</td>\n",
       "      <td>1.840895</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.332594</td>\n",
       "      <td>1.091688e-60</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>1.180303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>20.301718</td>\n",
       "      <td>19.924869</td>\n",
       "      <td>0.476550</td>\n",
       "      <td>1.024117</td>\n",
       "      <td>1.516672e-79</td>\n",
       "      <td>0.454452</td>\n",
       "      <td>12.894931</td>\n",
       "      <td>12.666290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-09</th>\n",
       "      <td>42.266063</td>\n",
       "      <td>6.273150</td>\n",
       "      <td>0.769788</td>\n",
       "      <td>0.926027</td>\n",
       "      <td>4.364523e-01</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>24.805972</td>\n",
       "      <td>3.039398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-10</th>\n",
       "      <td>38.215521</td>\n",
       "      <td>20.959241</td>\n",
       "      <td>0.572452</td>\n",
       "      <td>9.041929</td>\n",
       "      <td>2.928508e-32</td>\n",
       "      <td>0.564168</td>\n",
       "      <td>19.419146</td>\n",
       "      <td>10.645390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>49.055403</td>\n",
       "      <td>7.723570</td>\n",
       "      <td>1.006214</td>\n",
       "      <td>1.388117</td>\n",
       "      <td>7.023711e-02</td>\n",
       "      <td>0.211964</td>\n",
       "      <td>25.489667</td>\n",
       "      <td>4.085361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>49.859240</td>\n",
       "      <td>5.200128</td>\n",
       "      <td>0.992756</td>\n",
       "      <td>1.325452</td>\n",
       "      <td>5.257705e-01</td>\n",
       "      <td>0.192630</td>\n",
       "      <td>26.377306</td>\n",
       "      <td>2.800475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>51.604319</td>\n",
       "      <td>5.019332</td>\n",
       "      <td>0.978878</td>\n",
       "      <td>1.116048</td>\n",
       "      <td>6.197780e-01</td>\n",
       "      <td>0.066594</td>\n",
       "      <td>26.586333</td>\n",
       "      <td>2.541474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>52.203285</td>\n",
       "      <td>4.394231</td>\n",
       "      <td>0.857419</td>\n",
       "      <td>1.179371</td>\n",
       "      <td>5.840381e-01</td>\n",
       "      <td>0.197573</td>\n",
       "      <td>26.273785</td>\n",
       "      <td>2.207805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hum_avg  hum_stderr  pollutant_avg  pollutant_max  \\\n",
       "2018-10-05  39.183480   13.738696       0.905798       3.863239   \n",
       "2018-10-06  22.979521   21.936709       0.436067       1.000183   \n",
       "2018-10-07   0.083812    1.840895       0.002641       0.332594   \n",
       "2018-10-08  20.301718   19.924869       0.476550       1.024117   \n",
       "2018-10-09  42.266063    6.273150       0.769788       0.926027   \n",
       "2018-10-10  38.215521   20.959241       0.572452       9.041929   \n",
       "2018-10-11  49.055403    7.723570       1.006214       1.388117   \n",
       "2018-10-12  49.859240    5.200128       0.992756       1.325452   \n",
       "2018-10-13  51.604319    5.019332       0.978878       1.116048   \n",
       "2018-10-14  52.203285    4.394231       0.857419       1.179371   \n",
       "\n",
       "            pollutant_min  pollutant_std   temp_avg  temp_stderr  \n",
       "2018-10-05   2.819809e-05       0.362454  22.273216    11.242547  \n",
       "2018-10-06   1.497945e-58       0.414591  14.309410    13.639272  \n",
       "2018-10-07   1.091688e-60       0.018122   0.053903     1.180303  \n",
       "2018-10-08   1.516672e-79       0.454452  12.894931    12.666290  \n",
       "2018-10-09   4.364523e-01       0.081400  24.805972     3.039398  \n",
       "2018-10-10   2.928508e-32       0.564168  19.419146    10.645390  \n",
       "2018-10-11   7.023711e-02       0.211964  25.489667     4.085361  \n",
       "2018-10-12   5.257705e-01       0.192630  26.377306     2.800475  \n",
       "2018-10-13   6.197780e-01       0.066594  26.586333     2.541474  \n",
       "2018-10-14   5.840381e-01       0.197573  26.273785     2.207805  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GB_2W', 'GB_2A', 'PM_DALLAS_TEMP', 'EXT_HUM')\n",
      "------------------------------------------------------------------\n",
      "Calculation of \u001b[1mNO2       \u001b[0m\n",
      "Data Range from 2018-10-05 00:00:00+02:00 to 2018-10-16 00:00:00+02:00 with 10 days\n",
      "------------------------------------------------------------------\n",
      "Using GB_2A as baseliner\n",
      "------------------------\n",
      " Meta Data\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_valid</th>\n",
       "      <th>deltaAuxBase_avg</th>\n",
       "      <th>hum_avg</th>\n",
       "      <th>hum_stderr</th>\n",
       "      <th>indexMax</th>\n",
       "      <th>pollutant_avg</th>\n",
       "      <th>pollutant_max</th>\n",
       "      <th>pollutant_min</th>\n",
       "      <th>pollutant_std</th>\n",
       "      <th>rAuxBase</th>\n",
       "      <th>ratioAuxBase_avg</th>\n",
       "      <th>slopeAuxBase</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>True</td>\n",
       "      <td>3.040391</td>\n",
       "      <td>43.893591</td>\n",
       "      <td>2.150597</td>\n",
       "      <td>3</td>\n",
       "      <td>29.197504</td>\n",
       "      <td>3605.453804</td>\n",
       "      <td>-49.369029</td>\n",
       "      <td>155.978266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.845833</td>\n",
       "      <td>13.932668</td>\n",
       "      <td>27.935369</td>\n",
       "      <td>0.402234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>True</td>\n",
       "      <td>3.577630</td>\n",
       "      <td>43.828490</td>\n",
       "      <td>1.846713</td>\n",
       "      <td>0</td>\n",
       "      <td>14.596159</td>\n",
       "      <td>24.002440</td>\n",
       "      <td>5.261880</td>\n",
       "      <td>3.526442</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>1.542362</td>\n",
       "      <td>1.155258</td>\n",
       "      <td>27.292119</td>\n",
       "      <td>0.489868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>True</td>\n",
       "      <td>3.612658</td>\n",
       "      <td>40.230000</td>\n",
       "      <td>3.960770</td>\n",
       "      <td>0</td>\n",
       "      <td>9.250134</td>\n",
       "      <td>10.481393</td>\n",
       "      <td>7.554612</td>\n",
       "      <td>1.517619</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.501553</td>\n",
       "      <td>3.524757</td>\n",
       "      <td>25.873333</td>\n",
       "      <td>0.542709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>True</td>\n",
       "      <td>3.002299</td>\n",
       "      <td>39.828983</td>\n",
       "      <td>0.751778</td>\n",
       "      <td>0</td>\n",
       "      <td>11.938585</td>\n",
       "      <td>22.892406</td>\n",
       "      <td>3.640581</td>\n",
       "      <td>2.936685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.519283</td>\n",
       "      <td>0.714021</td>\n",
       "      <td>25.297956</td>\n",
       "      <td>0.872296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-09</th>\n",
       "      <td>True</td>\n",
       "      <td>2.603194</td>\n",
       "      <td>42.891564</td>\n",
       "      <td>3.617668</td>\n",
       "      <td>0</td>\n",
       "      <td>12.828448</td>\n",
       "      <td>23.915251</td>\n",
       "      <td>0.254154</td>\n",
       "      <td>3.735835</td>\n",
       "      <td>0.998091</td>\n",
       "      <td>1.517862</td>\n",
       "      <td>0.690610</td>\n",
       "      <td>25.173080</td>\n",
       "      <td>0.356385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-10</th>\n",
       "      <td>True</td>\n",
       "      <td>2.217827</td>\n",
       "      <td>49.576892</td>\n",
       "      <td>2.497567</td>\n",
       "      <td>32</td>\n",
       "      <td>15.660104</td>\n",
       "      <td>347.277138</td>\n",
       "      <td>-22.409507</td>\n",
       "      <td>13.941209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.445678</td>\n",
       "      <td>16.830774</td>\n",
       "      <td>25.192405</td>\n",
       "      <td>1.213371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>True</td>\n",
       "      <td>2.377737</td>\n",
       "      <td>50.241664</td>\n",
       "      <td>1.206205</td>\n",
       "      <td>0</td>\n",
       "      <td>14.430611</td>\n",
       "      <td>29.169828</td>\n",
       "      <td>-0.755470</td>\n",
       "      <td>4.839265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.759568</td>\n",
       "      <td>0.838548</td>\n",
       "      <td>26.106060</td>\n",
       "      <td>0.995498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>True</td>\n",
       "      <td>2.143156</td>\n",
       "      <td>50.348741</td>\n",
       "      <td>1.626011</td>\n",
       "      <td>0</td>\n",
       "      <td>12.522687</td>\n",
       "      <td>19.885821</td>\n",
       "      <td>3.953952</td>\n",
       "      <td>2.618308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.705869</td>\n",
       "      <td>0.874458</td>\n",
       "      <td>26.636269</td>\n",
       "      <td>1.008486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>True</td>\n",
       "      <td>2.539663</td>\n",
       "      <td>52.074436</td>\n",
       "      <td>0.961849</td>\n",
       "      <td>0</td>\n",
       "      <td>12.043008</td>\n",
       "      <td>18.803989</td>\n",
       "      <td>3.292032</td>\n",
       "      <td>2.564847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.547453</td>\n",
       "      <td>0.866028</td>\n",
       "      <td>26.828535</td>\n",
       "      <td>0.124468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>True</td>\n",
       "      <td>2.415302</td>\n",
       "      <td>52.568343</td>\n",
       "      <td>0.490381</td>\n",
       "      <td>0</td>\n",
       "      <td>11.626024</td>\n",
       "      <td>19.568788</td>\n",
       "      <td>2.056789</td>\n",
       "      <td>2.750113</td>\n",
       "      <td>0.999611</td>\n",
       "      <td>1.503324</td>\n",
       "      <td>1.035657</td>\n",
       "      <td>26.457517</td>\n",
       "      <td>0.209756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_valid  deltaAuxBase_avg    hum_avg  hum_stderr  indexMax  \\\n",
       "2018-10-05     True          3.040391  43.893591    2.150597         3   \n",
       "2018-10-06     True          3.577630  43.828490    1.846713         0   \n",
       "2018-10-07     True          3.612658  40.230000    3.960770         0   \n",
       "2018-10-08     True          3.002299  39.828983    0.751778         0   \n",
       "2018-10-09     True          2.603194  42.891564    3.617668         0   \n",
       "2018-10-10     True          2.217827  49.576892    2.497567        32   \n",
       "2018-10-11     True          2.377737  50.241664    1.206205         0   \n",
       "2018-10-12     True          2.143156  50.348741    1.626011         0   \n",
       "2018-10-13     True          2.539663  52.074436    0.961849         0   \n",
       "2018-10-14     True          2.415302  52.568343    0.490381         0   \n",
       "\n",
       "            pollutant_avg  pollutant_max  pollutant_min  pollutant_std  \\\n",
       "2018-10-05      29.197504    3605.453804     -49.369029     155.978266   \n",
       "2018-10-06      14.596159      24.002440       5.261880       3.526442   \n",
       "2018-10-07       9.250134      10.481393       7.554612       1.517619   \n",
       "2018-10-08      11.938585      22.892406       3.640581       2.936685   \n",
       "2018-10-09      12.828448      23.915251       0.254154       3.735835   \n",
       "2018-10-10      15.660104     347.277138     -22.409507      13.941209   \n",
       "2018-10-11      14.430611      29.169828      -0.755470       4.839265   \n",
       "2018-10-12      12.522687      19.885821       3.953952       2.618308   \n",
       "2018-10-13      12.043008      18.803989       3.292032       2.564847   \n",
       "2018-10-14      11.626024      19.568788       2.056789       2.750113   \n",
       "\n",
       "            rAuxBase  ratioAuxBase_avg  slopeAuxBase   temp_avg  temp_stderr  \n",
       "2018-10-05  1.000000          1.845833     13.932668  27.935369     0.402234  \n",
       "2018-10-06  0.998972          1.542362      1.155258  27.292119     0.489868  \n",
       "2018-10-07  0.999999          1.501553      3.524757  25.873333     0.542709  \n",
       "2018-10-08  1.000000          1.519283      0.714021  25.297956     0.872296  \n",
       "2018-10-09  0.998091          1.517862      0.690610  25.173080     0.356385  \n",
       "2018-10-10  1.000000          1.445678     16.830774  25.192405     1.213371  \n",
       "2018-10-11  1.000000          2.759568      0.838548  26.106060     0.995498  \n",
       "2018-10-12  1.000000          2.705869      0.874458  26.636269     1.008486  \n",
       "2018-10-13  1.000000          1.547453      0.866028  26.828535     0.124468  \n",
       "2018-10-14  0.999611          1.503324      1.035657  26.457517     0.209756  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Average Delta between baseline and auxiliary electrode: 2.75298569831, and ratio: 1.78887865694\n",
      "Std Dev of Delta between baseline and auxiliary electrode: 0.531758791872, and ratio: 0.509185127775\n",
      "------------------------\n",
      "('GB_3W', 'GB_3A', 'PM_DALLAS_TEMP', 'EXT_HUM')\n",
      "------------------------------------------------------------------\n",
      "Calculation of \u001b[1mO3        \u001b[0m\n",
      "Data Range from 2018-10-05 00:00:00+02:00 to 2018-10-16 00:00:00+02:00 with 10 days\n",
      "------------------------------------------------------------------\n",
      "Using GB_3A as baseliner\n",
      "------------------------\n",
      " Meta Data\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_valid</th>\n",
       "      <th>deltaAuxBase_avg</th>\n",
       "      <th>hum_avg</th>\n",
       "      <th>hum_stderr</th>\n",
       "      <th>indexMax</th>\n",
       "      <th>pollutant_avg</th>\n",
       "      <th>pollutant_max</th>\n",
       "      <th>pollutant_min</th>\n",
       "      <th>pollutant_std</th>\n",
       "      <th>rAuxBase</th>\n",
       "      <th>ratioAuxBase_avg</th>\n",
       "      <th>slopeAuxBase</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-05</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.013970</td>\n",
       "      <td>43.893591</td>\n",
       "      <td>2.150597</td>\n",
       "      <td>0</td>\n",
       "      <td>18.085790</td>\n",
       "      <td>58.303768</td>\n",
       "      <td>-3710.474999</td>\n",
       "      <td>158.065174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>4.436747</td>\n",
       "      <td>27.935369</td>\n",
       "      <td>0.402234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-06</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.332508</td>\n",
       "      <td>43.828490</td>\n",
       "      <td>1.846713</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699907</td>\n",
       "      <td>42.111719</td>\n",
       "      <td>18.615334</td>\n",
       "      <td>3.635173</td>\n",
       "      <td>0.994933</td>\n",
       "      <td>0.886814</td>\n",
       "      <td>1.614471</td>\n",
       "      <td>27.292119</td>\n",
       "      <td>0.489868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-07</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.048906</td>\n",
       "      <td>40.230000</td>\n",
       "      <td>3.960770</td>\n",
       "      <td>0</td>\n",
       "      <td>32.440763</td>\n",
       "      <td>34.561384</td>\n",
       "      <td>31.155674</td>\n",
       "      <td>1.850216</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>0.864864</td>\n",
       "      <td>2.591236</td>\n",
       "      <td>25.873333</td>\n",
       "      <td>0.542709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-08</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.024694</td>\n",
       "      <td>39.828983</td>\n",
       "      <td>0.751778</td>\n",
       "      <td>0</td>\n",
       "      <td>30.927535</td>\n",
       "      <td>41.778200</td>\n",
       "      <td>7.483267</td>\n",
       "      <td>3.164010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859079</td>\n",
       "      <td>1.349726</td>\n",
       "      <td>25.297956</td>\n",
       "      <td>0.872296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-09</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.089547</td>\n",
       "      <td>42.891564</td>\n",
       "      <td>3.617668</td>\n",
       "      <td>0</td>\n",
       "      <td>29.609268</td>\n",
       "      <td>44.559486</td>\n",
       "      <td>15.817591</td>\n",
       "      <td>3.817267</td>\n",
       "      <td>0.989303</td>\n",
       "      <td>0.878408</td>\n",
       "      <td>1.131828</td>\n",
       "      <td>25.173080</td>\n",
       "      <td>0.356385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-10</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.195108</td>\n",
       "      <td>49.576892</td>\n",
       "      <td>2.497567</td>\n",
       "      <td>0</td>\n",
       "      <td>26.817153</td>\n",
       "      <td>61.022264</td>\n",
       "      <td>-430.979473</td>\n",
       "      <td>16.438676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814503</td>\n",
       "      <td>2.394126</td>\n",
       "      <td>25.192405</td>\n",
       "      <td>1.213371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.507549</td>\n",
       "      <td>50.241664</td>\n",
       "      <td>1.206205</td>\n",
       "      <td>0</td>\n",
       "      <td>28.957265</td>\n",
       "      <td>39.344202</td>\n",
       "      <td>17.205376</td>\n",
       "      <td>3.341204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.344055</td>\n",
       "      <td>26.106060</td>\n",
       "      <td>0.995498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-12</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.126246</td>\n",
       "      <td>50.348741</td>\n",
       "      <td>1.626011</td>\n",
       "      <td>0</td>\n",
       "      <td>29.158594</td>\n",
       "      <td>39.751629</td>\n",
       "      <td>20.458701</td>\n",
       "      <td>2.788311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.621317</td>\n",
       "      <td>1.409545</td>\n",
       "      <td>26.636269</td>\n",
       "      <td>1.008486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-13</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.637736</td>\n",
       "      <td>52.074436</td>\n",
       "      <td>0.961849</td>\n",
       "      <td>0</td>\n",
       "      <td>29.488815</td>\n",
       "      <td>41.015285</td>\n",
       "      <td>21.289054</td>\n",
       "      <td>2.737843</td>\n",
       "      <td>0.997192</td>\n",
       "      <td>0.900788</td>\n",
       "      <td>1.378973</td>\n",
       "      <td>26.828535</td>\n",
       "      <td>0.124468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.162422</td>\n",
       "      <td>52.568343</td>\n",
       "      <td>0.490381</td>\n",
       "      <td>0</td>\n",
       "      <td>29.345648</td>\n",
       "      <td>37.113804</td>\n",
       "      <td>20.568333</td>\n",
       "      <td>2.522735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841601</td>\n",
       "      <td>1.939672</td>\n",
       "      <td>26.457517</td>\n",
       "      <td>0.209756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_valid  deltaAuxBase_avg    hum_avg  hum_stderr  indexMax  \\\n",
       "2018-10-05     True         -1.013970  43.893591    2.150597         0   \n",
       "2018-10-06     True         -1.332508  43.828490    1.846713         0   \n",
       "2018-10-07     True         -1.048906  40.230000    3.960770         0   \n",
       "2018-10-08     True         -1.024694  39.828983    0.751778         0   \n",
       "2018-10-09     True         -1.089547  42.891564    3.617668         0   \n",
       "2018-10-10     True         -1.195108  49.576892    2.497567         0   \n",
       "2018-10-11     True         -0.507549  50.241664    1.206205         0   \n",
       "2018-10-12     True         -1.126246  50.348741    1.626011         0   \n",
       "2018-10-13     True         -0.637736  52.074436    0.961849         0   \n",
       "2018-10-14     True         -1.162422  52.568343    0.490381         0   \n",
       "\n",
       "            pollutant_avg  pollutant_max  pollutant_min  pollutant_std  \\\n",
       "2018-10-05      18.085790      58.303768   -3710.474999     158.065174   \n",
       "2018-10-06      29.699907      42.111719      18.615334       3.635173   \n",
       "2018-10-07      32.440763      34.561384      31.155674       1.850216   \n",
       "2018-10-08      30.927535      41.778200       7.483267       3.164010   \n",
       "2018-10-09      29.609268      44.559486      15.817591       3.817267   \n",
       "2018-10-10      26.817153      61.022264    -430.979473      16.438676   \n",
       "2018-10-11      28.957265      39.344202      17.205376       3.341204   \n",
       "2018-10-12      29.158594      39.751629      20.458701       2.788311   \n",
       "2018-10-13      29.488815      41.015285      21.289054       2.737843   \n",
       "2018-10-14      29.345648      37.113804      20.568333       2.522735   \n",
       "\n",
       "            rAuxBase  ratioAuxBase_avg  slopeAuxBase   temp_avg  temp_stderr  \n",
       "2018-10-05  1.000000          0.014438      4.436747  27.935369     0.402234  \n",
       "2018-10-06  0.994933          0.886814      1.614471  27.292119     0.489868  \n",
       "2018-10-07  0.999904          0.864864      2.591236  25.873333     0.542709  \n",
       "2018-10-08  1.000000          0.859079      1.349726  25.297956     0.872296  \n",
       "2018-10-09  0.989303          0.878408      1.131828  25.173080     0.356385  \n",
       "2018-10-10  1.000000          0.814503      2.394126  25.192405     1.213371  \n",
       "2018-10-11  1.000000               NaN      1.344055  26.106060     0.995498  \n",
       "2018-10-12  1.000000          0.621317      1.409545  26.636269     1.008486  \n",
       "2018-10-13  0.997192          0.900788      1.378973  26.828535     0.124468  \n",
       "2018-10-14  1.000000          0.841601      1.939672  26.457517     0.209756  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Average Delta between baseline and auxiliary electrode: -1.01386855322, and ratio: 0.742423655732\n",
      "Std Dev of Delta between baseline and auxiliary electrode: 0.252539028871, and ratio: 0.285631082484\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from pollutant_cal_utils import *\n",
    "%matplotlib inline\n",
    "min_delta = 30\n",
    "max_delta = 60\n",
    "ad_append = 'AD_BASE_' + str(min_delta) + '-' + str(max_delta)\n",
    "deltas_co = np.arange(10,10,1)\n",
    "deltas_no2 = np.arange(min_delta,min_delta+max_delta,1)\n",
    "deltas_ox = np.arange(min_delta,min_delta+max_delta,1)\n",
    "\n",
    "selectedTestsAD = tuple()\n",
    "def selectTestAD(x):\n",
    "    global selectedTestsAD\n",
    "    selectedTestsAD = list(x)\n",
    "    \n",
    "def calculateCorrectionAD(b):\n",
    "    clear_output()\n",
    "    for testAD in selectedTestsAD:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testAD]['devices']:\n",
    "            if 'is_reference' in readings[testAD]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testAD]['devices'][reading]['data']\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in readings[testAD]['devices']:\n",
    "            if 'alphasense' in readings[testAD]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testAD]['devices'][kit]['alphasense']\n",
    "                sensorID_CO = readings[testAD]['devices'][kit]['alphasense']['CO']\n",
    "                sensorID_NO2 = readings[testAD]['devices'][kit]['alphasense']['NO2']\n",
    "                sensorID_OX = readings[testAD]['devices'][kit]['alphasense']['O3']\n",
    "                sensorSlots = readings[testAD]['devices'][kit]['alphasense']['SLOTS']\n",
    "                              \n",
    "                sensorID = (['CO', sensorID_CO, 'classic', 'single_aux', sensorSlots.index('CO')+1, deltas_co], \n",
    "                            ['NO2', sensorID_NO2, 'baseline', 'single_aux', sensorSlots.index('NO2')+1, deltas_no2], \n",
    "                            ['O3', sensorID_OX, 'baseline', 'single_aux', sensorSlots.index('O3')+1, deltas_ox])\n",
    "                \n",
    "                # Calculate correction\n",
    "                readings[testAD]['devices'][kit]['alphasense']['model_stats'] = dict()\n",
    "                readings[testAD]['devices'][kit]['data'], readings[testAD]['devices'][kit]['alphasense']['model_stats'][ad_append] = calculatePollutantsAlpha(\n",
    "                        _dataframe = readings[testAD]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = ad_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = checkBoxDecomp.value,\n",
    "                        _plotsInter = checkBoxPlotsIn.value, \n",
    "                        _plotResult = checkBoxPlotsResult.value,\n",
    "                        _verbose = checkBoxVerb.value, \n",
    "                        _printStats = checkBoxStats.value)\n",
    "\n",
    "# Find out which tests have alphasense values\n",
    "testAlphaSense = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        if 'alphasense' in readings[test]['devices'][kit] and test not in testAlphaSense:\n",
    "            testAlphaSense.append(test)\n",
    "\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing alphasense to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestAD,\n",
    "         x = widgets.SelectMultiple(options=testAlphaSense, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsAD, \n",
    "                           layout=widgets.Layout(width='700px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionAD)\n",
    "\n",
    "# Synchronise Checkbox\n",
    "\n",
    "checkBoxDecomp = widgets.Checkbox(value=False, \n",
    "                                  description='Decomp')\n",
    "checkBoxPlotsIn = widgets.Checkbox(value=False, \n",
    "                                  description='Plots Inter')     \n",
    "checkBoxVerb = widgets.Checkbox(value=False, \n",
    "                                  description='Verbose') \n",
    "checkBoxPlotsResult = widgets.Checkbox(value=False, \n",
    "                                  description='Plots Results') \n",
    "checkBoxStats = widgets.Checkbox(value=True, \n",
    "                                  description='Print Stats') \n",
    "\n",
    "Box = widgets.HBox([calculateCorrection, checkBoxDecomp, checkBoxPlotsIn, checkBoxVerb, checkBoxPlotsResult, checkBoxStats])\n",
    "display(Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Explore results\n",
    "# test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "test = 'STATION'\n",
    "# device = 'STATION CHIMNEY'\n",
    "device = '4748'\n",
    "dataframe = readings[test]['devices'][device]['alphasense']['model_stats'][ad_append]\n",
    "dataframe['CO'] = dataframe['CO'].iloc[1:,:]\n",
    "dataframe['NO2'] = dataframe['NO2'].iloc[1:,:]\n",
    "\n",
    "\n",
    "fig, (ax, ax2)= plt.subplots(nrows = 2, figsize= (15,10))\n",
    "ax.bar(dataframe['CO'].index, dataframe['CO']['pollutant_avg'], label='CO')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=70 )\n",
    "ax2.bar(dataframe['NO2'].index,dataframe['NO2']['pollutant_avg'], label='NO2')\n",
    "#ax.plot(dataframe['O3'].index,dataframe['O3']['pollutant_avg'],'ro', label='O3')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=70 )\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.title('Average pollutant concentration')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Correction Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample For stats checks\n",
    "pollutant = 'NO2'\n",
    "display(CorrParams[pollutant])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax1.set_ylabel('Avg Temp-Hum / day')\n",
    "    ax1.grid(True)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax2.set_ylabel('Avg Temp / day')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig2, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax3.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_pollutant'], label = 'Avg Pollutant', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['deltaAuxBas_avg'], label = 'Delta', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['ratioAuxBas_avg'] , label = 'Ratio', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax3.legend(loc='best')\n",
    "    ax3.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax3.set_ylabel('Avg {} / day'.format(pollutant))\n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc='best')\n",
    "    ax4.set_xlabel('{} Average'.format(pollutant))\n",
    "    ax4.set_ylabel('Offset / Ratio Baseline vs Auxiliary')\n",
    "    ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MICS Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pollutant_cal_utils import *\n",
    "from test_utils import ref_append\n",
    "%matplotlib inline\n",
    "mics_append = 'MICS_BASE_CALC'\n",
    "\n",
    "selectedTestsMICS = tuple()\n",
    "def selectTestMICS(x):\n",
    "    global selectedTestsMICS\n",
    "    selectedTestsMICS = list(x)\n",
    "    \n",
    "def calculateCorrectionMICS(b):\n",
    "    clear_output()\n",
    "    for testMICS in selectedTestsMICS:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testMICS]['devices']:\n",
    "            # If there is reference, use it\n",
    "            if 'is_reference' in readings[testMICS]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data']\n",
    "                break\n",
    "            # If not, at least use alphasense data\n",
    "            elif 'alphasense' in readings[testMICS]['devices'][reading]:\n",
    "                refAvail = True\n",
    "                \n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data'].loc[:,['CO_' + ad_append, 'NO2_' + ad_append, 'O3_' + ad_append ]]\n",
    "                # Rename to be a reference\n",
    "                for name in dataframeRef.columns:\n",
    "                    namesub = re.sub(ad_append, ref_append, name)\n",
    "                    dataframeRef.rename(columns={name: namesub}, inplace=True)\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in deviceMICS:\n",
    "            if 'mics' in readings[testMICS]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testMICS]['devices'][kit]['mics']               \n",
    "                sensorID = (['CO', sensorID, 'baseline', 'single_temp'], \n",
    "                            ['NO2', sensorID, 'baseline', 'single_temp'])\n",
    "            \n",
    "            # Temporary until better understanding\n",
    "            else:\n",
    "                sensorID = (['CO', 1, 'baseline', 'single_temp'], \n",
    "                            ['NO2', 1, 'baseline', 'single_temp'])\n",
    "                \n",
    "            # Calculate correction\n",
    "            readings[testMICS]['devices'][kit]['data'], CorrParams = calculatePollutantsMICS(\n",
    "                        _dataframe = readings[testMICS]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = mics_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltasMICS,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have measured the mics\n",
    "testMICS = list()\n",
    "deviceMICS = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        columnsTest = readings[test]['devices'][kit]['data'].columns\n",
    "        if ('CO_MICS_RAW' in columnsTest or 'NO2_MICS_RAW' in columnsTest):\n",
    "            if test not in testMICS:\n",
    "                testMICS.append(test)\n",
    "            if kit not in deviceMICS:\n",
    "                deviceMICS.append(kit)\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing MICS to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestMICS,\n",
    "         x = widgets.SelectMultiple(options=testMICS, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsMICS, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionMICS)\n",
    "deltasMICS = np.arange(1,100,1)\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from test_utils import combine_data\n",
    "\n",
    "name_combined_data = 'COMBINED_DEVICES'\n",
    "test_model = 'STATION'\n",
    "\n",
    "try: \n",
    "    for reading in readings:\n",
    "        ## Since we don't know if there are more or less channels than last time\n",
    "        ## (and tbh, I don't feel like checking), remove the key\n",
    "        readings[reading]['devices'].pop('COMBINED_DEVICES', None)\n",
    "        ## And then add it again\n",
    "        dataframe = combine_data(readings[reading]['devices'], True)\n",
    "        readings[reading]['devices'][name_combined_data] = dict()\n",
    "        readings[reading]['devices'][name_combined_data]['data'] = dict()\n",
    "        readings[reading]['devices'][name_combined_data]['data'] = dataframe\n",
    "except:\n",
    "    print('Review data')\n",
    "else:\n",
    "    print('Data combined successfully')\n",
    "\n",
    "## Create dict for model comparison\n",
    "readings[test_model]['devices'][name_combined_data]['model'] = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Model Process\n",
    "\n",
    "Inspired by the example of \"Jakob Aungiers, Altum Intelligence ltd\" at https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     24
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Utils\n",
    "from linear_regression_utils import prep_data_OLS, fit_model_OLS, predict_OLS\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from numpy import concatenate\n",
    "\n",
    "# ML Utils\n",
    "from ml_utils import prep_dataframe_ML\n",
    "from ml_utils import fit_model_ML\n",
    "from ml_utils import predict_ML\n",
    "\n",
    "# Metrics \n",
    "from signal_utils import metrics\n",
    "import json\n",
    "\n",
    "## Input\n",
    "##--------------\n",
    "configs = json.load(open('./models/models.json', 'r'))\n",
    "device_name = '4748'\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-08-20 00:00:00'\n",
    "\n",
    "##--------------\n",
    "\n",
    "for config in configs.keys():\n",
    "    \n",
    "    # Retrieve models\n",
    "    model_name = config\n",
    "    print model_name\n",
    "\n",
    "    if model_name not in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "        model_type = configs[config]['type']\n",
    "        features = configs[config]['features']\n",
    "        \n",
    "        tuple_features = [(k, v) for k, v in features.iteritems()]\n",
    "    \n",
    "        list_features = list()\n",
    "        for feature in tuple_features: \n",
    "            if feature[0] == 'REF':\n",
    "                list_features.insert(0,feature[1] + '_' + device_name)\n",
    "                reference_name = feature[1] + '_' + device_name\n",
    "            else:\n",
    "                list_features.append(feature[1] + '_' + device_name)\n",
    "        \n",
    "        dataframeModel = readings[test_model]['devices'][name_combined_data]['data'].loc[:,list_features]\n",
    "        dataframeModel = dataframeModel.dropna()\n",
    "        \n",
    "        dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "        dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "        \n",
    "        print '\\t{}'. format(model_type) \n",
    "        if model_type == 'OLS':\n",
    "            \n",
    "            formula_expression = configs[config]['expression']\n",
    "            ratio_train = configs[config]['ratio_train']\n",
    "            alpha_filter = configs[config]['alpha_filter']\n",
    "            \n",
    "            ## Model Fit\n",
    "            dataTrain, dataTest, n_train_periods = prep_data_OLS(dataframeModel, tuple_features, min_date, max_date, ratio_train, alpha_filter, device_name)\n",
    "            model = fit_model_OLS(formula_expression, dataTrain, False)\n",
    "            \n",
    "            referenceTrain, referenceTest, predictionTrain, predictionTest = predict_OLS(model, dataTrain, dataTest, False)\n",
    "            \n",
    "            predictionTrain = predictionTrain.values\n",
    "            indexTrain = dataTrain['index']\n",
    "            indexTest = dataTest['index']\n",
    "            n_lags = 1\n",
    "\n",
    "        elif model_type == 'LSTM':\n",
    "            \n",
    "            epochs = configs[config]['epochs']\n",
    "            batch_size = configs[config]['batch_size']\n",
    "            verbose = configs[config]['verbose']\n",
    "            n_lags = configs[config]['n_lags']\n",
    "            loss = configs[config][\"loss\"]\n",
    "            optimizer = configs[config][\"optimizer\"]\n",
    "            layers = configs[config]['layers']\n",
    "    \n",
    "            ## Prep Dataframe\n",
    "            index, train_X, train_y, test_X, test_y, scalerX, scalery, n_train_periods = prep_dataframe_ML(dataframeModel, min_date, max_date, list_features, n_lags, ratio_train, alpha_filter, reference_name, False)\n",
    "            ## Model Fit\n",
    "            model = fit_model_ML(train_X, train_y, \n",
    "                                 test_X, test_y, \n",
    "                                 epochs = epochs, \n",
    "                                 batch_size = batch_size, \n",
    "                                 verbose = verbose, \n",
    "                                 plotResult = False, \n",
    "                                 loss = loss, \n",
    "                                 optimizer = optimizer,\n",
    "                                 layers = layers)\n",
    "            \n",
    "            # Get model prediction\n",
    "            \n",
    "            referenceTrain = get_inverse_transform(train_X, train_y, n_lags, scalery)\n",
    "            predictionTrain = predict_ML(model, train_X, n_lags, scalery)\n",
    "            \n",
    "            referenceTest = get_inverse_transform(test_X, test_y, n_lags, scalery)\n",
    "            predictionTest = predict_ML(model, test_X, n_lags, scalery)\n",
    "            \n",
    "            indexTrain = index[:n_train_periods]\n",
    "            indexTest = index[n_train_periods+n_lags:]\n",
    "            formula_expression = '-'\n",
    "        \n",
    "    \n",
    "        dataFrameTrain = pd.DataFrame(data = {'reference': referenceTrain, 'prediction': predictionTrain}, \n",
    "                                      index = indexTrain)\n",
    "        dataFrameTest = pd.DataFrame(data = {'reference': referenceTest, 'prediction': predictionTest}, \n",
    "                                      index = indexTest)\n",
    "        \n",
    "        dataFrameExport = dataFrameTrain.copy()\n",
    "        dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "            \n",
    "        # Get model metrics\n",
    "        metrics_model_train = metrics(referenceTrain, predictionTrain)\n",
    "        metrics_model_test = metrics(referenceTest, predictionTest)\n",
    "        \n",
    "        ## Put everything in the dict\n",
    "        dictModel = readings[test_model]['devices'][name_combined_data]\n",
    "        \n",
    "        # From https://hackmd.io/Y62wiJw0RaiBfU4Xhv8dQQ#\n",
    "        dictModel[model_name] = dict()\n",
    "        dictModel[model_name]['metrics'] = dict()\n",
    "        dictModel[model_name]['metrics']['train'] = metrics_model_train\n",
    "        dictModel[model_name]['metrics']['test'] = metrics_model_test\n",
    "        \n",
    "        # Model Parameters\n",
    "        dictModel[model_name]['parameters'] = dict()\n",
    "        dictModel[model_name]['parameters']['features'] = tuple_features\n",
    "        dictModel[model_name]['parameters']['formula'] = formula_expression\n",
    "        dictModel[model_name]['parameters']['ratio_train'] = n_train_periods\n",
    "        dictModel[model_name]['data'] = dict()\n",
    "        dictModel[model_name]['data']['train'] = dataFrameTrain\n",
    "        dictModel[model_name]['data']['test'] = dataFrameTest\n",
    "        dictModel[model_name]['model'] = model\n",
    "        \n",
    "        # Put it back in the readings dataframe\n",
    "        readings[test_model]['devices'][name_combined_data]['model'][model_name] = dictModel[model_name]\n",
    "        readings[test_model]['devices'][model_name] = dict()\n",
    "        readings[test_model]['devices'][model_name]['data'] = dataFrameExport\n",
    "        print '\\t Model Calculated'\n",
    "    else:\n",
    "        print '\\t Model already present, skipping'\n",
    "\n",
    "print 'All models calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Checks\n",
    "\n",
    "\n",
    "\n",
    "#### Data stationarity\n",
    "\n",
    "We will use the Dicker-fuller test (ADF) test to verify the data is stationary. We need to check data stationarity for certain type of models. \n",
    "\n",
    "If the process is stationary means it doesn’t change its statistical properties over time: mean and variance do not change over time (constancy of variance is also called homoscedasticity), also covariance function does not depend on the time (should only depend on the distance between observations) Source [here](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3). Visually:\n",
    "\n",
    "- Growing mean --> Non stationary\n",
    "![](https://cdn-images-1.medium.com/max/800/0*qrYiVksz8g3drl5Z.png)\n",
    "\n",
    "- Growing spread --> Non stationary\n",
    "![](https://cdn-images-1.medium.com/max/800/0*fEqQDq_TaEqa511n.png)\n",
    "\n",
    "- Varying time covariance --> Non stationary\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qJs3g2f77flIXr6mFsbPmw.png)\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary). Hence:\n",
    "\n",
    "- p-value > 0.05: fail to reject the null hypothesis (H0), the data has an unit root and is non-stationary.\n",
    "- p-value <= 0.05: reject the null hypothesis (H0), the data does not have an unit root and is stationary.\n",
    "\n",
    "#### Autocorrelation\n",
    "\n",
    "High levels of autocorrelation can indicate our time series is shows seasonality. We will use the ACF plot to identify possible autocorrelation and potentially include differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import tfullerplot\n",
    "\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "tuple_features = (['REF', 'CO_AD_BASE_60_FILTER'],\n",
    "                 ['A', 'CO_MICS_RAW'],\n",
    "                 ['B', 'TEMP'],\n",
    "                 ['C', 'HUM'],\n",
    "                 ['D', 'EXT_PM_25'])\n",
    "\n",
    "device_name = '4773'\n",
    "\n",
    "list_features = list()\n",
    "for item in tuple_features:\n",
    "    list_features.append(item[1] + '_' + device_name)\n",
    "    \n",
    "\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data'].loc[:,list_features]\n",
    "dataframeModel = dataframeModel.dropna()\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "dataframeModel = dataframeModel.groupby(pd.Grouper(freq='10Min')).aggregate(np.mean)\n",
    "\n",
    "# Fuller Plot for all\n",
    "for item in tuple_features:\n",
    "    if item[0] == 'REF': \n",
    "        reference = dataframeModel.loc[:,item[1] + '_' + device_name]\n",
    "        reference_name = dataframeModel.loc[:, item[1] + '_' + device_name].name\n",
    "    x = dataframeModel.loc[:,item[1] + '_' + device_name]\n",
    "    tfullerplot(x, name = item[1], lags=60, lags_diff = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Casuality Test (use with caution)\n",
    "\n",
    "This test is useful to determine the casuality of variables and determining whether one time series is useful in forecasting another. \n",
    "\n",
    "The Null hypothesis for granger causality tests is that the time series in the second column, x2, does NOT Granger cause the time series in the first column, x1. Grange causality means that past values of x2 have a statistically significant effect on the current value of x1, taking past values of x1 into account as regressors. We reject the null hypothesis that x2 does not Granger cause x1 if the pvalues are below a desired size of the test. Hence:\n",
    "\n",
    "- p-value < size: allows to reject the null hypothesis (H0) for x1 = f(x2)\n",
    "- p-value > size: we fail to reject the null hypothesis (H0) for x1 = f(x2)\n",
    "\n",
    "The null hypothesis for all four test is that the coefficients corresponding to past values of the second time series are zero.\n",
    "\n",
    "Reference [here](https://en.wikipedia.org/wiki/Granger_causality), [here](https://stats.stackexchange.com/questions/24753/interpreting-granger-causality-tests-results#24796) and [here](http://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.grangercausalitytests.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the channels first\n",
    "for channel in readings[test_model]['devices'][name_combined_data]['data'].columns:\n",
    "    print channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import prep_data_OLS, fit_model_OLS, predict_OLS, plotOLSCoeffs\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from numpy import concatenate\n",
    "\n",
    "## Select data\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "\n",
    "formula_expression = 'REF ~ np.log10(A) + B + D + np.power(D,2)'\n",
    "model_name_OLS = 'log(CO_MICS), T, Poly(PM) LONG'\n",
    "\n",
    "ratio_train = 3./4 # Important that this is a float, don't forget the .\n",
    "alpha_filter = 0.3\n",
    "\n",
    "# print ('--------------------------------------')\n",
    "# print ('Granger Causality Test')\n",
    "## Granger Causality Test (WIP)\n",
    "# for item in tuple_features:\n",
    "#     if item[0] != 'REF':\n",
    "#         print '\\nCausality for x1 = {} and x2 = {}'.format(reference_name, item[1])\n",
    "#         x = dataframeModel.loc[:,[reference_name, item[1]]].dropna()\n",
    "#         x = x.values\n",
    "#         granger_causality_tests = grangercausalitytests(x, 1)\n",
    "#         # print granger_causality_tests\n",
    "\n",
    "## Model Fit\n",
    "dataTrain, dataTest, n_train_periods = prep_data_OLS(dataframeModel, tuple_features, min_date, max_date, ratio_train, alpha_filter, device_name)\n",
    "model = fit_model_OLS(formula_expression, dataTrain, True)\n",
    "\n",
    "plotOLSCoeffs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from signal_utils import metrics\n",
    "\n",
    "referenceTrain, referenceTest, predictionTrain, predictionTest = predict_OLS(model, dataTrain, dataTest, True)\n",
    "\n",
    "dataFrameTrain = pd.DataFrame(data = {'reference': referenceTrain, 'prediction': predictionTrain.values}, \n",
    "                              index = dataTrain['index'])\n",
    "\n",
    "dataFrameTest = pd.DataFrame(data = {'reference': referenceTest, 'prediction': predictionTest}, \n",
    "                              index = dataTest['index'])\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "metrics_model_train = metrics(referenceTrain, predictionTrain)\n",
    "metrics_model_test = metrics(referenceTest, predictionTest)\n",
    "\n",
    "## Metrics Train\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))\n",
    "\n",
    "## Put everything into the dict\n",
    "dictModel = readings[test_model]['devices'][name_combined_data]\n",
    "\n",
    "dictModel[model_name_OLS] = dict()\n",
    "dictModel[model_name_OLS]['metrics'] = dict()\n",
    "dictModel[model_name_OLS]['metrics']['train'] = metrics_model_train\n",
    "dictModel[model_name_OLS]['metrics']['test'] = metrics_model_test\n",
    "\n",
    "# Model Parameters\n",
    "dictModel[model_name_OLS]['parameters'] = dict()\n",
    "dictModel[model_name_OLS]['parameters']['formula'] = formula_expression\n",
    "dictModel[model_name_OLS]['parameters']['features'] = tuple_features\n",
    "dictModel[model_name_OLS]['parameters']['ratio_train'] = ratio_train\n",
    "dictModel[model_name_OLS]['model'] = model\n",
    "dictModel[model_name_OLS]['data'] = dict()\n",
    "dictModel[model_name_OLS]['data']['train'] = dataFrameTrain\n",
    "dictModel[model_name_OLS]['data']['test'] = dataFrameTest\n",
    "\n",
    "# print dictModel[model_name].keys()\n",
    "# print dictModel[model_name]['metrics']\n",
    "# print dictModel[model_name]['parameters']\n",
    "# print dictModel[model_name]['model']\n",
    "\n",
    "# Put it back in the readings dataframe\n",
    "readings[test_model]['devices'][name_combined_data]['model'][model_name_OLS] = dictModel[model_name_OLS]\n",
    "readings[test_model]['devices'][model_name_OLS] = dict()\n",
    "readings[test_model]['devices'][model_name_OLS]['data'] = dataFrameExport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import modelRplots\n",
    "%matplotlib inline\n",
    "\n",
    "modelRplots(model, dataTrain, dataTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "modelDir_OLS = '/Users/macoscar/Documents/04_Projects/02_FabLab/02_SmartCitizen/04_iScape/99_DataAnalysis/smartcitizen-iscape-data/notebooks/models/export'\n",
    "filename_OLS = join(modelDir_OLS, model_name_OLS)\n",
    "\n",
    "# Save everything\n",
    "joblib.dump(dictModel[model_name_OLS]['metrics'], filename_OLS + '_metrics.sav')\n",
    "joblib.dump(dictModel[model_name_OLS]['parameters'], filename_OLS + '_parameters.sav')\n",
    "joblib.dump(dictModel[model_name_OLS]['model'], filename_OLS + '_model.sav')\n",
    "print(\"Model saved in: \" + modelDir_OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "modelDir_OLS = '/Users/macoscar/Documents/04_Projects/02_FabLab/02_SmartCitizen/04_iScape/03_Development/03_TestResults/TestStructure/models'\n",
    "filename_OLS = join(modelDir_OLS, model_name_OLS)\n",
    "\n",
    "# Load everything\n",
    "loaded_metrics = joblib.load(filename_OLS + '_metrics.sav')\n",
    "loaded_params = joblib.load(filename_OLS + '_parameters.sav')\n",
    "loaded_model = joblib.load(filename_OLS + '_model.sav')\n",
    "print(\"Loaded \" + model_name_OLS + \" from disk\")\n",
    "\n",
    "display(loaded_params)\n",
    "display(loaded_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for channel in readings[test_model]['devices'][name_combined_data]['data'].columns:\n",
    "    print channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and data training split\n",
    "\n",
    "The following code uses cross validation on rolling basis structure:\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine all data in one dataframe\n",
    "from ml_utils import prep_dataframe_ML\n",
    "from ml_utils import fit_model_ML\n",
    "\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "tuple_features = (['REF', 'CO_AD_BASE_30_FILTER'],\n",
    "                 ['A', 'CO_MICS_RAW'],\n",
    "                 ['B', 'TEMP'],\n",
    "                 ['C', 'HUM'],\n",
    "                 ['D', 'EXT_PM_25'])\n",
    "\n",
    "model_name_ML = 'LSTM CO - epochs = 100 - filter_alpha = 0.3 -  traintest = 0.8 - 3 lags'\n",
    "device_name = '4748'\n",
    "\n",
    "ratio_train = 4./5 # Important that this is a float, don't forget the .\n",
    "alpha_filter = 0.3 # 1 means no filtering\n",
    "\n",
    "# Number of lags for the model\n",
    "n_lags = 3\n",
    "\n",
    "## Prep Dataframe\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data']\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "dataframeModel = dataframeModel.groupby(pd.Grouper(freq='10Min')).aggregate(np.mean)\n",
    "\n",
    "list_features = list()\n",
    "for item in tuple_features: \n",
    "    if item[0] == 'REF':\n",
    "        list_features.insert(0,item[1] + '_' + device_name)\n",
    "        reference_name = item[1] + '_' + device_name\n",
    "    else:\n",
    "        list_features.append(item[1] + '_' + device_name)\n",
    "\n",
    "index, train_X, train_y, test_X, test_y, scalerX, scalery, n_train_periods = prep_dataframe_ML(dataframeModel, min_date, max_date, list_features, n_lags, ratio_train, alpha_filter, reference_name)\n",
    "# Model Fit\n",
    "model = fit_model_ML(train_X, train_y, \n",
    "                      test_X, test_y, \n",
    "                      epochs = 100, batch_size = 72, \n",
    "                      verbose = 2, plotResult = True, \n",
    "                      loss = 'mse', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ml_utils import predict_ML, get_inverse_transform_ML\n",
    "from signal_utils import metrics\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Get model prediction\n",
    "inv_y_train = get_inverse_transform_ML(train_y, n_lags, scalery)\n",
    "inv_yhat_train = predict_ML(model, train_X, n_lags, scalery)\n",
    "\n",
    "inv_y_test = get_inverse_transform_ML(test_y, n_lags, scalery)\n",
    "inv_yhat_test = predict_ML(model, test_X, n_lags, scalery)\n",
    "\n",
    "dataFrameTrain = pd.DataFrame(data = {'reference': inv_y_train, 'prediction': inv_yhat_train}, \n",
    "                              index = index[:n_train_periods])\n",
    "dataFrameTest = pd.DataFrame(data = {'reference': inv_y_test, 'prediction': inv_yhat_test}, \n",
    "                              index = index[n_train_periods+n_lags:])\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model_train = metrics(inv_y_train, inv_yhat_train)\n",
    "metrics_model_test = metrics(inv_y_test, inv_yhat_test)\n",
    "\n",
    "## Print Metrics\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))\n",
    "\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "# Actual data\n",
    "plot.plot(index[:n_train_periods], inv_y_train,'r', label = 'Reference Train', alpha = 0.3)\n",
    "plot.plot(index[n_train_periods+n_lags:], inv_y_test, 'b', label = 'Reference Test', alpha = 0.3)\n",
    "# Fitted Values for Training\n",
    "plot.plot(index[:n_train_periods], inv_yhat_train, 'r', label = 'Prediction Train')\n",
    "\n",
    "# Fitted Values for Test\n",
    "plot.plot(index[n_train_periods+n_lags:], inv_yhat_test, 'b', label = 'Prediction Test')\n",
    "\n",
    "plot.title('LSTM Regression Results' + model_name_ML)\n",
    "plot.ylabel('Reference/Prediction (-)')\n",
    "plot.xlabel('Date (-)')\n",
    "plot.legend(loc='best')\n",
    "plot.show()\n",
    "\n",
    "## Put everything in the dict\n",
    "dictModel = readings[test_model]['devices'][name_combined_data]\n",
    "\n",
    "# From https://hackmd.io/Y62wiJw0RaiBfU4Xhv8dQQ#\n",
    "dictModel[model_name_ML] = dict()\n",
    "dictModel[model_name_ML]['metrics'] = dict()\n",
    "dictModel[model_name_ML]['metrics']['train'] = metrics_model_train\n",
    "dictModel[model_name_ML]['metrics']['test'] = metrics_model_test\n",
    "\n",
    "# Model Parameters\n",
    "dictModel[model_name_ML]['parameters'] = dict()\n",
    "dictModel[model_name_ML]['parameters']['features'] = tuple_features\n",
    "dictModel[model_name_ML]['parameters']['ratio_train'] = n_train_periods\n",
    "dictModel[model_name_ML]['parameters']['scalerX'] = scalerX\n",
    "dictModel[model_name_ML]['parameters']['scalery'] = scalery\n",
    "dictModel[model_name_ML]['parameters']['n_lags'] = n_lags\n",
    "dictModel[model_name_ML]['parameters']['alpha_filter'] = alpha_filter\n",
    "\n",
    "dictModel[model_name_ML]['data'] = dict()\n",
    "dictModel[model_name_ML]['data']['train'] = dataFrameTrain\n",
    "dictModel[model_name_ML]['data']['test'] = dataFrameTest\n",
    "dictModel[model_name_ML]['model'] = model\n",
    "\n",
    "# Put it back in the readings dataframe\n",
    "readings[test_model]['devices'][name_combined_data]['model'][model_name_ML] = dictModel[model_name_ML]\n",
    "readings[test_model]['devices'][model_name_ML] = dict()\n",
    "readings[test_model]['devices'][model_name_ML]['data'] = dataFrameExport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "modelDirectory = join(rootDirectory, '/smartcitizen-iscape-models/')\n",
    "modelType = 'MICS' # MICS, ALPHASENSE OR PMS\n",
    "modelDirML = join(modelDirectory, 'Models/', modelType)\n",
    "summaryDir = join(modelDirectory, 'Models/summary.json')\n",
    "filenameML = join(modelDirML, model_name_ML)\n",
    "\n",
    "# Save everything\n",
    "joblib.dump(dictModel[model_name_ML]['metrics'], filenameML + '_metrics.sav')\n",
    "joblib.dump(dictModel[model_name_ML]['parameters'], filenameML + '_parameters.sav')\n",
    "model_json = model.to_json()\n",
    "with open(filenameML + \"_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(filenameML + \"_model.h5\")\n",
    "print(\"Model: \\n\\t\" + model_name_ML + \"\\nSaved in:\\n\\t\" + modelDirML)\n",
    "\n",
    "summary = json.load(open(summaryDir, 'r'))\n",
    "summary[modelType][model_name_ML] = dict()\n",
    "summary[modelType][model_name_ML] = ''\n",
    "\n",
    "with open(summaryDir, 'w') as json_file:\n",
    "    json_file.write(json.dumps(summary))\n",
    "    json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeSeries Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.tools as tls\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "referencePlotted = False\n",
    "\n",
    "for model in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "    data = readings[test_model]['devices'][name_combined_data]['model'][model]['data']\n",
    "    if (not referencePlotted):\n",
    "        plt.plot(data['train'].index, data['train']['referenceTrain'], 'b', label = 'Reference', alpha = 0.3)\n",
    "        plt.plot(data['test'].index, data['test']['referenceTest'], 'b', label = 'Reference', alpha = 0.3)\n",
    "        referencePlotted = True\n",
    "        \n",
    "    plt.plot(data['train'].index, data['train']['predictionTrain'], label = 'Prediction Train ' + model)\n",
    "    plt.plot(data['test'].index, data['test']['predictionTest'], label = 'Prediction Test ' + model)\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylabel('CO (ppm)')\n",
    "plt.xlabel('Date (-)')\n",
    "plt.title('Model Comparison for ' + dict(readings[test_model]['devices'][name_combined_data]['model'][model]['parameters']['features'])[\"REF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     12,
     18
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "for model in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "    print '-----------------------------------------------------'\n",
    "    print '\\nModel Name: {}'.format(model)\n",
    "    print '\\n\\t\\t Train \\t\\t Test'\n",
    "    metrics_model = readings[test_model]['devices'][name_combined_data]['model'][model]['metrics']\n",
    "    for item in metrics_model['train']:\n",
    "        print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model['train'][item], metrics_model['test'][item]))\n",
    "\n",
    "def minRtarget(targetR):\n",
    "    return sqrt(1+ np.power(targetR,2)-2*np.power(targetR,2))\n",
    "\n",
    "_plot_train = True\n",
    "_dataframe = readings[test_model]['devices'][name_combined_data]\n",
    "\n",
    "def targetDiagram(dataframe, plot_train):\n",
    "\n",
    "    targetR20 = 0.5\n",
    "    targetR0 = sqrt(targetR20)\n",
    "    MR0 = minRtarget(targetR0)\n",
    "    targetR21 = 0.7\n",
    "    targetR1 = sqrt(targetR21)\n",
    "    MR1 = minRtarget(targetR1)\n",
    "    targetR22 = 0.9\n",
    "    targetR2 = sqrt(targetR22)\n",
    "    MR2 = minRtarget(targetR2)\n",
    "\n",
    "\n",
    "    fig  = plot.figure(figsize=(13,13))\n",
    "    for model in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "        metrics_model = dataframe[model]['metrics']\n",
    "    \n",
    "        if plot_train == True:\n",
    "            plot.scatter(metrics_model['train']['sign_sigma']*metrics_model['train']['RMSD_norm_unb'], metrics_model['train']['normalised_bias'], label = 'Train ' + model)\n",
    "        plot.scatter(metrics_model['test']['sign_sigma']*metrics_model['test']['RMSD_norm_unb'], metrics_model['test']['normalised_bias'], label = 'Test ' + model)\n",
    "    \n",
    "    ## Add circles\n",
    "    ax = plot.gca()\n",
    "    circle1 = plot.Circle((0, 0), 1, linewidth = 0.8, color='k', fill =False)\n",
    "    circleMR0 = plot.Circle((0, 0), MR0, linewidth = 0.8, color='r', fill=False)\n",
    "    circleMR1 = plot.Circle((0, 0), MR1, linewidth = 0.8, color='y', fill=False)\n",
    "    circleMR2 = plot.Circle((0, 0), MR2, linewidth = 0.8, color='g', fill=False)\n",
    "    \n",
    "    circle3 = plot.Circle((0, 0), 0.01, color='k', fill=True)\n",
    "    \n",
    "    ## Add annotations\n",
    "    ax.add_artist(circle1)\n",
    "    ax.annotate('R2 < 0',\n",
    "                xy=(1, 0), xycoords='data',\n",
    "                xytext=(-35, 10), textcoords='offset points')\n",
    "    \n",
    "    ax.add_artist(circleMR0)\n",
    "    ax.annotate('R2 < ' + str(targetR20),\n",
    "                xy=(MR0, 0), xycoords='data',\n",
    "                xytext=(-35, 10), textcoords='offset points', color = 'r')\n",
    "    \n",
    "    ax.add_artist(circleMR1)\n",
    "    ax.annotate('R2 < ' + str(targetR21),\n",
    "                xy=(MR1, 0), xycoords='data',\n",
    "                xytext=(-45, 10), textcoords='offset points', color = 'y')\n",
    "    \n",
    "    \n",
    "    ax.add_artist(circleMR2)\n",
    "    ax.annotate('R2 < ' + str(targetR22),\n",
    "                xy=(MR2, 0), xycoords='data',\n",
    "                xytext=(-45, 10), textcoords='offset points', color = 'g')\n",
    "    ax.add_artist(circle3)\n",
    "    \n",
    "    ## Display and others\n",
    "    plt.axhline(0, color='black', linewidth = 0.5)\n",
    "    plt.axvline(0, color='black', linewidth = 0.5)\n",
    "    plot.legend(loc='best')\n",
    "    plot.xlim([-1.1,1.1])\n",
    "    plot.ylim([-1.1,1.1])\n",
    "    plot.title('Target Diagram')\n",
    "    plot.ylabel('Normalised Bias (-)')\n",
    "    plot.xlabel(\"RMSD*'\")\n",
    "    plot.show()\n",
    "    \n",
    "\n",
    "targetDiagram(_dataframe, _plot_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "modelDirML = join(rootDirectory, 'smartcitizen-iscape-models/Models/')\n",
    "\n",
    "dict_models = dict()\n",
    "with open(join(modelDirML, 'summary.json'), 'r') as summary_file:\n",
    "    dict_models = json.load(summary_file)\n",
    "    \n",
    "selectedModels = tuple()\n",
    "def selectModels(Source):\n",
    "    global selectedModels\n",
    "    selectedModels = list(Source)\n",
    "    \n",
    "def loadModel(b):\n",
    "    filenameML = join(modelDirML, type_drop.value, selectedModels[0])\n",
    "    \n",
    "    global loaded_model\n",
    "    global loaded_params\n",
    "    global loaded_metrics\n",
    "    # Load Model and weights\n",
    "    json_file = open(filenameML + \"_model.json\", \"r\")\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    \n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(filenameML + \"_model.h5\")\n",
    "    # Load params and metrics\n",
    "    loaded_params = joblib.load(filenameML + '_parameters.sav')\n",
    "    loaded_metrics = joblib.load(filenameML + '_metrics.sav')\n",
    "    \n",
    "    print(\"Loaded \" + selectedModels[0] + \" from disk\")\n",
    "    \n",
    "def show_models(Source):\n",
    "    list_models = list()\n",
    "    for item in dict_models[Source]:\n",
    "        list_models.append(item)\n",
    "    models.options = list(list_models)\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Import Local Models</h4>'))\n",
    "\n",
    "type_drop = widgets.Dropdown(options = ['ALPHASENSE', 'MICS', 'PMS'],\n",
    "                                  value = 'MICS',\n",
    "                                  description = 'Model Target',\n",
    "                                  layout = widgets.Layout(width='300px'))\n",
    "\n",
    "model_type_drop = widgets.interactive(show_models, \n",
    "                                Source=type_drop, \n",
    "                                layout=widgets.Layout(width='700px'))\n",
    "\n",
    "models = widgets.SelectMultiple(selected_labels = selectedModels, \n",
    "                           layout=widgets.Layout(width='700px'))\n",
    "\n",
    "models_interact = widgets.interactive(selectModels,\n",
    "                                     Source = models,\n",
    "                                     layout = widgets.Layout(width='700px'))\n",
    "display(model_type_drop)\n",
    "display(models)\n",
    "\n",
    "loadB = widgets.Button(description='Load Model')\n",
    "loadB.on_click(loadModel)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB])\n",
    "display(buttonBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ml_utils import prep_prediction_ML\n",
    "from ml_utils import predict_ML\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# ---\n",
    "\n",
    "# Input\n",
    "test_name = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "device_name = 'STATION CHIMNEY'\n",
    "prediction_name = 'CO_MICS_ppm'\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "# ---\n",
    "\n",
    "model_predict = loaded_model\n",
    "tuple_feat_predict = loaded_params['features'] \n",
    "scalerX_predict = loaded_params['scalerX']\n",
    "scalery_predict = loaded_params['scalery']\n",
    "n_lags = loaded_params['n_lags']\n",
    "alpha_filter = loaded_params['alpha_filter']\n",
    "n_train_periods = loaded_params['ratio_train']\n",
    "\n",
    "## Prep Dataframe\n",
    "dataframeModel = readings[test_name]['devices'][device_name]['data']\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "list_features_predict = list()\n",
    "for item in tuple_feat_predict: \n",
    "    if item[0] != 'REF':\n",
    "        list_features_predict.append(item[1])\n",
    "\n",
    "test_X, index_pred, n_obs = prep_prediction_ML(dataframeModel, list_features_predict, n_lags, alpha_filter, scalerX_predict, verbose = True)\n",
    "prediction = predict_ML(model_predict, test_X, n_lags, scalery_predict)\n",
    "dataframe = pd.DataFrame(prediction, columns = [prediction_name]).set_index(index_pred)\n",
    "readings[test_name]['devices'][device_name]['data'][prediction_name] = dataframe.loc[:,prediction_name]\n",
    "\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "# Actual data\n",
    "try:\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['reference'],'r', label = 'Reference Train', alpha = 0.3)\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['reference'], 'b', label = 'Reference Test', alpha = 0.3)\n",
    "    # Fitted Values for Training\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['prediction'], 'r', label = 'Prediction Train')\n",
    "    # Fitted Values for Test\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['prediction'], 'b', label = 'Prediction Test')\n",
    "except:\n",
    "    print 'No data available for reference'\n",
    "    \n",
    "# Fitted values\n",
    "plot.plot(dataframe.index, dataframe.loc[:, prediction_name], 'g')\n",
    "plot.ylim([0,1.5])\n",
    "plot.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ARIMAX\n",
    "model = sm.tsa.SARIMAX(endog = train_y,\n",
    "                       exog = train_X,\n",
    "                       order= (7,0,7),\n",
    "                       enforce_invertibility=False,\n",
    "                       enforce_stationarity=False,\n",
    "                       trend='c')\n",
    "                \n",
    "results = model.fit(disp=0)\n",
    "                \n",
    "\n",
    "inv_y = test_y\n",
    "prediction = results.get_prediction(full_results=True,\n",
    "                                alpha=0.05)\n",
    "\n",
    "forecast = results.get_forecast(test_y.shape[0], \n",
    "                                exog=test_X)\n",
    "\n",
    "inv_yhat_prediction = prediction.predicted_mean\n",
    "inv_yhat = forecast.predicted_mean\n",
    "\n",
    "# Get your prediction intervals by alpha parameter. alpha=0.05 implies 95% CI\n",
    "inv_yhat_cis_prediction = prediction.conf_int(alpha=0.05)\n",
    "inv_yhat_cis = forecast.conf_int(alpha=0.05)\n",
    "print(results.summary())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "48px",
    "left": "552px",
    "top": "705.497px",
    "width": "441px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
