{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search with Cross Validation \n",
    "\n",
    "We perform here cross validated random search of the model hyperparameters, to later on retrieve the best parameters with a grid search around the best found results of the CV.\n",
    "\n",
    "Using **k-fold cross validation** below:\n",
    "\n",
    "![](https://i.imgur.com/HLbgMSS.png)\n",
    "\n",
    "Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from src.models.ml_utils import predict_ML\n",
    "\n",
    "if model_type == 'RF':\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    ## Evaluate the default model\n",
    "    base_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    random_model = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "elif model_type == 'SVR':\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    random_grid = {\"C\": [1e0, 1e1, 1e2, 1e3], \n",
    "                   \"gamma\": np.logspace(-2, 2, 5),\n",
    "                   \"kernel\": ['rbf', 'sigmoid'],\n",
    "                  \"shrinking\": [True, False]}\n",
    "    \n",
    "    ## Create the default model\n",
    "    base_model = SVR(kernel='rbf', gamma=0.1)\n",
    "\n",
    "    ## Create randomized Search\n",
    "    random_model = RandomizedSearchCV(estimator = SVR(), cv=5, \n",
    "                             n_iter = 100, scoring = 'neg_mean_absolute_error',\n",
    "                             param_distributions=random_grid,  verbose=2, random_state=42, n_jobs=-1)\n",
    "    \n",
    "# Fit the base model\n",
    "base_model.fit(train_X, train_y)\n",
    "## Get base model prediction\n",
    "dataFrameTrain_base = predict_ML(base_model, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest_base = predict_ML(base_model, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])\n",
    "\n",
    "# Fit the random search model\n",
    "random_model.fit(train_X, train_y)\n",
    "random_model.best_params_\n",
    "best_random = random_model.best_estimator_\n",
    "## Evaluate the best model\n",
    "dataFrameTrain_best = predict_ML(best_random, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest_best = predict_ML(best_random, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if model_type == 'RF':\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'min_samples_split': [2, 3],\n",
    "        'n_estimators': [200, 300, 400, 1000]\n",
    "    }\n",
    "        \n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, \n",
    "                               scoring = 'neg_mean_absolute_error', cv = 3, \n",
    "                               n_jobs = -1, verbose = 2)\n",
    "elif model_type == 'SVR':\n",
    "\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\"C\": [1e0, 1e1, 1e2, 1e3], \n",
    "                   \"gamma\": np.logspace(-2, 2, 5),\n",
    "                  \"shrinking\": [True, False]\n",
    "    }\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = SVR(), param_grid = param_grid, \n",
    "                               scoring = 'neg_mean_absolute_error', cv = 3, \n",
    "                               n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X, train_y)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy with the best predictions of the grid search, put them in the dataframe for plotting and archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (grid_search.best_params_)\n",
    "best_grid = grid_search.best_estimator_\n",
    "print (best_grid)\n",
    "dataFrameTrain_best_grid = predict_ML(best_grid, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest_best_grid = predict_ML(best_grid, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy with the best predictions of the grid search, put them in the dataframe for plotting and archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameExport = dataFrameTrain_best_grid.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest_best_grid)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model = dict()\n",
    "metrics_model['train'] = metrics(dataFrameTrain_best_grid['reference'], dataFrameTrain_best_grid['prediction'])\n",
    "metrics_model['test'] = metrics(dataFrameTest_best_grid['reference'], dataFrameTest_best_grid['prediction'])\n",
    "\n",
    "records.archive_model(test_model, model_full_name + '_best_grid_search', \n",
    "                      metrics_model, \n",
    "                      dataFrameExport, best_grid, model_type, \n",
    "                      model_target, ratio_train)\n",
    "\n",
    "print ('Metrics Summary:')\n",
    "print (\"{:<23} {:<7} {:<5}\".format('Metric','Train','Test'))\n",
    "for metric in metrics_model['train'].keys():\n",
    "    print (\"{:<20}\".format(metric) +\"\\t\" +\"{:0.3f}\".format(metrics_model['train'][metric]) +\"\\t\"+ \"{:0.3f}\".format(metrics_model['test'][metric]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
